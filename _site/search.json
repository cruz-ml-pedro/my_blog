[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Saudações a todos!",
    "section": "",
    "text": "Saudações a todos!\nMe chamo Pedro Lima, sou geofísico de formação e mestre em ciências, é um prazer dar as boas-vindas a vocês em meu blog. Neste espaço pretendo compartilhar minha jornada de exploração e aprendizado contínuo sobre uma variedade de conteúdos relaionados a análise e modelagem de dados.\nPretendo compartilhar tutoriais detalhados, insights sobre minhas próprias experiências e, é claro, códigos práticos em R, e espero que em breve, também conteúdos em Python.\nCaso tenha interesse em entrar em contato, vocês encontrarão links para minhas redes sociais na parte superior e inferior da página, onde estou sempre aberto a discussões, perguntas e compartilhamento de ideias. Além disso, vocês terão a oportunidade de acessar e baixar meu currículo diretamente aqui no blog, para que possam conhecer mais sobre minha jornada e experiência.\nCom entusiasmo,\nPedro Lima"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Olá!\nMe chamo Pedro Lima e este é o meu blog sobre ciência de dados. Possuo bacharelado em geofísica e mestrado em ciências e tecnologias espaciais, formações que sempre me levaram a buscar novas técnicas de análise e modelagem de dados.\nDevido meu background possuo um interesse mais acentuado em temas relacionados a séries temporais. Entretanto, neste espaço irei compartilhar meus principais aprendizados sobre os mais diversos temas deste vasto mundo da ciência de dados. Desde seus conceitos mais fundamentais até tópicos mais avançados, como a criação e implantação de aplicações e modelos utilizando, por exemplo, o Docker.\nCaso tenha interesse em entrar em contato comigo ou saber mais sobre o meu trabalho, basta clicar nos links do GitHub e/ou LinkedIn presentes nesta página."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Cotação do ouro- EDA",
    "section": "",
    "text": "Essa é a minha primeira postagem e fará parte de uma série de posts relacionados aos dados do valor do ouro, obtidos do Yahoo Finanças. Nessa série, iremos realizar a análise exploratória desses dados e modelá-los utilizando métodos clássicos e de aprendizado de máquina. Além disso, em um futuro próximo, buscaremos possíveis regressores que nos ajudem a aprimorar nossas previsões sobre o valor do ouro.\nSem mais delongas, vamos à EDA (Exploratory Data Analysis) desses dados."
  },
  {
    "objectID": "posts/EDA-ouro/index.html",
    "href": "posts/EDA-ouro/index.html",
    "title": "Cotação do ouro- EDA",
    "section": "",
    "text": "Essa é a primeira postagem de uma série de posts relacionados a cotação ouro, obtidos do Yahoo Finanças. Nessa série, irei realizar a análise exploratória desses dados e modelá-los utilizando métodos clássicos e de aprendizado de máquina. Além disso, em um futuro próximo, buscaremos possíveis regressores que nos ajudem a aprimorar nossas previsões sobre o valor do ouro.\nA Análise Exploratória de Séries Temporais é um processo essencial na compreensão e interpretação de conjuntos de dados que evoluem ao longo do tempo. Por meio dessa abordagem, busca-se desvendar padrões, tendências e características intrínsecas presentes em dados sequenciais, permitindo uma visão aprofundada das variações temporais. Ao explorar visualizações, estatísticas descritivas, análise de autocorrelação e decomposição, entre outras técnicas, os analistas podem identificar relações de dependência, sazonalidades e eventos atípicos, contribuindo para uma compreensão mais sólida do comportamento temporal dos dados. Através dessa exploração minuciosa, a Análise Exploratória de Séries Temporais desempenha um papel fundamental no embasamento de decisões informadas e na construção de modelos de previsão mais precisos.\n\n\nNeste projeto, utilizaremos uma seleção de pacotes para análise avançada de séries temporais financeiras. Para facilitar o carregamento dos pacotes vamos usar a função p_load() oriundo do pacote pacman, que permite carregar as bibliotecas de modo unificado\n\ntidyquant: Este pacote agiliza a coleta e manipulação de dados financeiros, integrando-se ao “tidyverse” e permitindo aquisição de dados de diversas fontes, como Yahoo Finanças.\ntsibble: O “tsibble” fornece uma estrutura de dados eficiente para manipulação de séries temporais, facilitando o tratamento de datas, horários e valores.\nfabletools: Parte da família “fable”, o “fabletools” oferece ferramentas para análise e previsão de séries temporais, tornando mais acessíveis técnicas avançadas.\ntimetk: Com foco na preparação de dados de séries temporais, o “timetk” oferece métodos simplificados para transformações e limpeza de informações temporais.\ntibbletime: Este pacote amplia as funcionalidades do “tibble” para dados temporais, permitindo manipulações intuitivas e agregações eficientes.\nfeasts: O “feasts” é voltado para modelagem e previsão de séries temporais, com suporte para decomposição, ajuste de modelos e geração de previsões.\ntidyverse: Uma coleção de pacotes interligados para manipulação e visualização de dados. Com suas funcionalidades, ele é uma base sólida para análise.\ntseries: Focado em econometria e séries temporais, o “tseries” oferece métodos estatísticos e ferramentas para análise temporal avançada.\nWaveletComp: Este pacote é utilizado para análise de séries temporais por meio da transformada wavelet, permitindo identificar padrões em diferentes escalas.\n\n\npacman::p_load(tidyquant, tsibble,fabletools,fabletools,timetk,fpp3,\n               tibbletime,feasts, tidyverse, tseries, WaveletComp,\n               tsoutliers, DT, plotly)"
  },
  {
    "objectID": "posts/EDA-ouro/index.html#baixando-os-dados",
    "href": "posts/EDA-ouro/index.html#baixando-os-dados",
    "title": "Cotação do ouro- EDA",
    "section": "Baixando os dados",
    "text": "Baixando os dados\n\npacman::p_load(tidyquant, quantmod, tidyverse, tseries, WaveletComp, fable.prophet, fpp3)\n\n# Obter dados da ação de ouro (código GLD)\ngold_data &lt;- tq_get(\"GLD\")\n\nVamos começar a realizar uma análise exploratória (EDA), dando uma olhada na cara da série temporal.\n\ngold_data %&gt;% \n  ggplot(aes(x=date,y=close))+\n  geom_line()+\n  labs(title = \"Valor Ouro 2013-2023\")+\n  theme_minimal()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')\n\n\n\n\nPrimeiro, vamos manipular os dados para preencher as lacunas existentes e colocá-los em um formato adequado para as análises que vamos realizar.\nEsses dados vêm da bolsa de valores, que não funciona nos fins de semana e feriados… Não sei se esse é o método mais indicado, mas é o mais simples =).\n\ntdf_ouro &lt;- gold_data %&gt;%\n  tsibble::tsibble() %&gt;% \n  tsibble::fill_gaps() %&gt;% \n  tidyr::fill(close, .direction = \"down\") %&gt;% \n  select(date, close)\n\nUsing `date` as index variable.\n\n\n\ngold_acf &lt;- gold_data %&gt;%\n  tsibble::tsibble() %&gt;% \n  tsibble::fill_gaps() %&gt;% \n  tidyr::fill(close, .direction = \"down\") %&gt;% \n   rename(\n      index = date,\n      value = close\n   ) %&gt;% \n   select(index,value)\n\nUsing `date` as index variable.\n\nfit &lt;- gold_acf %&gt;%\n  fabletools::model(\n  feasts::STL(value)\n    ) %&gt;% \n  fabletools::components()\n\n\nts_acf &lt;- fit %&gt;% \n   mutate(\n    value = value - trend\n  ) %&gt;% \n  select(index,value) %&gt;% \n  timetk::tk_tbl() %&gt;%\n  tibbletime::as_tbl_time(index = index)\n\nWarning in tk_tbl.data.frame(.): Warning: No index to preserve. Object\notherwise converted to tibble successfully.\n\n\nvamos ver a autocorrelação.\nfunção personalizada para gerar os valores de ACF\n\ntidy_acf &lt;- function(data, value, lags = 0:20) {\n    \n    #value_expr &lt;- enquo(value)\n    \n    acf_values &lt;- data %&gt;%\n        acf(lag.max = tail(lags, 1), plot = FALSE) %&gt;%\n        .$acf %&gt;%\n        .[,,1]\n    \n    ret &lt;- tibble(acf = acf_values) %&gt;%\n        rowid_to_column(var = \"lag\") %&gt;%\n        mutate(lag = lag - 1) %&gt;%\n        filter(lag %in% lags)\n    \n    return(ret)\n}\n\n\nmax_lag &lt;- 3853\n\ntidy_acf(ts_acf$value, lags = 0:max_lag) \n\n# A tibble: 3,854 × 2\n     lag   acf\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     0 1    \n 2     1 0.981\n 3     2 0.962\n 4     3 0.945\n 5     4 0.926\n 6     5 0.908\n 7     6 0.890\n 8     7 0.872\n 9     8 0.855\n10     9 0.837\n# ℹ 3,844 more rows\n\n\n\nconfidence &lt;- 1.96 / sqrt(nrow(ts_acf))\n\n\ntidy_acf(ts_acf$value, lags = 0:max_lag) %&gt;%\n    ggplot(aes(lag, acf)) +\n    geom_segment(aes(xend = lag, yend = 0), color = palette_light()[[1]]) +\n    geom_vline(xintercept = 575, linewidth = 3, color = palette_light()[[2]]) +\n    geom_hline(yintercept = confidence, color = \"red\",\n               linetype = \"dotted\",linewidth = 1.2) +\n    geom_hline(yintercept = -confidence, color = \"red\",\n               linetype = \"dotted\",linewidth = 1.2) +\n    annotate(\"text\", label = \"1.6 Year Mark\", x = 600, y = 0.8, \n             color = palette_light()[[2]], size = 6, hjust = 0) +\n    theme_tq() +\n    labs(title = \"ACF: \")\n\n\n\n\n\ntidy_acf(ts_acf$value, lags = 170:210) %&gt;%\n    ggplot(aes(lag, acf)) +\n    geom_vline(xintercept = 200, size = 3, color = palette_light()[[2]]) +\n    geom_segment(aes(xend = lag, yend = 0), color = palette_light()[[1]]) +\n    geom_hline(yintercept = confidence, color = \"red\",\n               linetype = \"dotted\",linewidth = 1.2) +\n    geom_hline(yintercept = -confidence, color = \"red\",\n               linetype = \"dotted\",linewidth = 1.2) +\n    geom_point(color = palette_light()[[1]], size = 2) +\n    geom_label(aes(label = acf %&gt;% round(2)), vjust = -1,\n              color = palette_light()[[1]]) +\n    annotate(\"text\", label = \"~1.6 Year Mark\", x = 200, y = 0.8, \n             color = palette_light()[[2]], size = 5, hjust = 0) +\n    theme_tq() +\n    labs(title = \"ACF: Sunspots\",\n         subtitle = \"Zoomed in on Lags 500 to 520\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\ntdf_ouro %&gt;% \n    feasts::ACF(lag_max = 730) %&gt;% \n    autoplot()\n\nResponse variable not specified, automatically selected `var = close`\n\n\n\n\n\nOww…Oww… os valores de ouro são altamente correlacionados! Existe um aparente trend nesse resultado, que é, digamos maios ou menos corroborado pelo gráfico inicial. (A cara desse gráfico é de dados não estacionários)\n\ntdf_ouro %&gt;% \nfeasts::PACF() %&gt;% \n  autoplot()\n\nResponse variable not specified, automatically selected `var = close`\n\n\n\n\n\nNa análise de autocorrelação parcial, observa-se que apenas o lag 0 apresenta valores significativos. Ao examinar os valores restantes, é possível identificar um aparente padrão cíclico, porém todos eles estão abaixo da faixa de significância estabelecida.\nEsses resultados indicam que há uma dependência linear entre as observações passadas e futuras, em que as observações mais próximas no tempo têm uma influência significativa sobre as observações subsequentes. No entanto, uma vez que os valores de autocorrelação parcial se tornam insignificantes após o lag 0, isso sugere que a dependência nas observações futuras pode ser explicada principalmente pela observação imediatamente anterior, sem a influência direta de defasagens adicionais.\nIsso pode ser interpretado como um padrão de comportamento em que os valores atuais da série temporal são diretamente influenciados pelos valores imediatamente anteriores, mas não são afetados por defasagens adicionais. Esse tipo de modelo é comumente chamado de “processo autorregressivo de ordem 1” (AR(1)).\nVamos agora verificar a estacionaridade e explosividade dos dados.\n\nadf.test(ts(tdf_ouro$close),alternative =\"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(tdf_ouro$close)\nDickey-Fuller = -3.4304, Lag order = 15, p-value = 0.04896\nalternative hypothesis: stationary\n\n\nComo vemos existe uma evidência estatística fraca contra a hipótese nula, de que a ST é não estacionária. No entanto, esse valor ainda está próximo do limite de significância comumente utilizado de 0,05.\nEm termos práticos, um p-value de 0.04626 sugere que há uma probabilidade de aproximadamente 4,6% de obter os resultados observados ou mais extremos sob a hipótese nula de não-estacionariedade ser verdadeira. Portanto, embora não seja uma rejeição clara da hipótese nula, sugere-se que há uma tendência de que a série seja estacionária.\nPois é… na hora da modelagem teremos que olhar isso com mais calma.\n\nadf.test(ts(tdf_ouro$close),alternative = \"explosive\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(tdf_ouro$close)\nDickey-Fuller = -3.4304, Lag order = 15, p-value = 0.951\nalternative hypothesis: explosive\n\n\nO alto valor do p-value sugere que os dados não fornecem suporte para a presença de explosividade na série.\nEm termos práticos, um p-value de 0.9537 indica que há uma probabilidade de aproximadamente 95,37% de obter os resultados observados ou mais extremos sob a hipótese nula de não explosividade ser verdadeira. Portanto, não há indícios significativos de explosividade na série temporal com base nos resultados do teste.\nBom, a série é aparentemente estacionária… sem a presença de mudanças bruscas. O que em um primeiro momento indicaria o uso de um modelo da família ARMA e não GARCH. Entretanto vamos continuar explorando os dados.\nMesmo os testes indicando que a série é estacionaria, o grafico de ACF indica o contrário. Vamos diferenciar essa série e ver como ela se comporta.\n\ntdf_ouro %&gt;%\n features(close, unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\nusando esse teste acima vemos que é indicado um diferenciação nos dados.\n\ntdf_ouro %&gt;% \nfeasts::ACF(difference(close)) %&gt;% \n  autoplot()\n\n\n\n\n\ntdf_ouro %&gt;% \nmutate(diff_close = difference(close)) %&gt;% \n  features(diff_close, ljung_box, lag = 3)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    12.0   0.00751\n\n\nApós diferenciar a série, apesar de baixos valores, vemos que existem lags que apresentam valores significativos!\nNão vou exibir aquiu, mas o resultado de PACF diferenciando os dados é o mesmo da ACF com os dados diferenciados.\nvamos refazer o teste de estacionaridade\n\ntdf_ouro %&gt;% \nmutate(diff_close = difference(close)) %&gt;% \n  features(diff_close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.344         0.1\n\n\n\nts &lt;- tdf_ouro %&gt;% \n  mutate(\n    diff_data = difference(close)\n  ) %&gt;% \n  as_tibble() %&gt;% \n  select(diff_data) %&gt;% \ntidyr::drop_na()\n \nadf.test(ts(ts),alternative =\"stationary\")\n\nWarning in adf.test(ts(ts), alternative = \"stationary\"): p-value smaller than\nprinted p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(ts)\nDickey-Fuller = -16.437, Lag order = 15, p-value = 0.01\nalternative hypothesis: stationary\n\n\nParece que agora a série é estacionária de forma mais consistente.\nvamos ver a cara dos dados diferenciados.\n\ntdf_ouro %&gt;% \n  mutate(\n    diff_close=difference(close)\n  ) %&gt;% \n  autoplot(diff_close)\n\nWarning: Removed 1 row containing missing values (`geom_line()`)."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#carregando-os-pacotes-para-a-análise-exploratória",
    "href": "posts/EDA-ouro/index.html#carregando-os-pacotes-para-a-análise-exploratória",
    "title": "Cotação do ouro- EDA",
    "section": "Carregando os pacotes para a análise exploratória",
    "text": "Carregando os pacotes para a análise exploratória\n\npacman::p_load(tidyverse, tidyquant, tseries, WaveletComp, fpp3)\n\nAntes de começar vamos a uma breve descrição das bibliotecas utilizadas\ntidyverse - O tidyverse é um conjunto de pacotes R desenvolvidos para facilitar a análise de dados e a ciência de dados em geral, seguindo os princípios da filosofia “tidy data”.\nEsse conjunto de pacotes foi criado para oferecer uma abordagem consistente, intuitiva e eficiente para a manipulação, visualização e modelagem de dados. Os principais pacotes que compõem o tidyverse incluem: dplyr, ggplot2, tidyr, readr, purrr, tibble, stringr e forcats.\ntidyquant - O pacote “tidyquant” é uma biblioteca projetada para facilitar a análise de dados financeiros e a realização de tarefas relacionadas ao mercado financeiro.\nO pacote foi desenvolvido com base nos princípios da filosofia “tidyverse”. O tidyquant combina várias outras bibliotecas populares, como “quantmod”, “TTR” e “xts”, para fornecer uma abordagem “tidy”.\nPrincipais características do pacote tidyquant são: integração com “tidyverse”, acesso a dados financeiros, análise técnica, estratégias de negociação, Visualização Interativa.\ntseries - O pacote “tseries” é um conjunto de ferramentas dedicado à análise de séries temporais. Esse pacote oferece uma ampla variedade de funções e métodos para lidar com dados temporais, realizar diagnósticos, modelagem e previsão.\nAlgumas das principais funcionalidades do pacote “tseries” incluem: testes de estacionariedade, transformações, modelos de média móvel, decomposição de séries temporais, autocorrelação, previsão.\nfpp3 - O pacote “fpp3” (Forecasting Principles and Practice, 3rd edition) é uma biblioteca desenvolvida para auxiliar na análise e previsão de séries temporais.\nEsse pacote é baseado no livro “Forecasting: Principles and Practice” (3ª edição), escrito por Rob J Hyndman e George Athanasopoulos.\nWaveletComp - O WaveletComp é um pacote destinado à análise wavelet de séries temporais univariadas e bivariadas. Com o WaveletComp, as funções wavelet são implementadas de forma a oferecer fácil acesso a uma ampla gama de resultados intermediários e finais.\nUma das principais funcionalidades do pacote é testar a hipótese nula de que não há (co)periodicidade na série, utilizando valores-p obtidos por meio de simulação. O usuário tem a liberdade de escolher entre diversas opções de modelos para realizar as simulações.\nAlém disso, o WaveletComp permite a reconstrução e filtragem de uma série específica a partir de sua decomposição wavelet, sujeita a diversas possibilidades de restrições."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#carregando-os-dados",
    "href": "posts/EDA-ouro/index.html#carregando-os-dados",
    "title": "Cotação do ouro- EDA",
    "section": "carregando os dados",
    "text": "carregando os dados\nPara obter os dados que serão utilizados nessa série de posts vamos utilizar a função tq_get do pacote tidyquant. Essa função tem como padrão a opção \"stock.prices\", que retorna os valores de ‘open’, ‘high’, ‘low’, ‘close’, ‘volume’ e ‘adjusted’ do Yahoo Finaças. Todavia é possível obter outras opções como dividendos ou ‘split’ de ações, além de dados de outras fontes.\n\n# Obter dados da ação de ouro (código GLD)\ngold_data &lt;- tq_get(\"GLD\")\n\nAo utilizarmos o padrão da função, informando apenas o simbolo da ação que desejamos obter, será retornado todo o período de tempo disponível. Vamos iniciar uma análise exploratória (EDA), examinando a estrutura da série temporal."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#visualizando-os-dados",
    "href": "posts/EDA-ouro/index.html#visualizando-os-dados",
    "title": "Cotação do ouro- EDA",
    "section": "visualizando os dados",
    "text": "visualizando os dados\nVamos iniciar nossa análise exploratória (EDA), examinando a estrutura da série temporal.\n\nhead(gold_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 GLD    2013-01-02  163.  164.  163.  163. 10431800     163.\n2 GLD    2013-01-03  162.  163.  161.  161. 16117500     161.\n3 GLD    2013-01-04  160.  161.  159.  160. 19179800     160.\n4 GLD    2013-01-07  159.  160.  159.  159.  9361800     159.\n5 GLD    2013-01-08  160.  161.  160.  161.  7694800     161.\n6 GLD    2013-01-09  161.  161.  160.  160.  8334200     160.\n\nsummary(gold_data)\n\n    symbol               date                 open            high      \n Length:2661        Min.   :2013-01-02   Min.   :100.9   Min.   :101.0  \n Class :character   1st Qu.:2015-08-24   1st Qu.:119.2   1st Qu.:119.7  \n Mode  :character   Median :2018-04-16   Median :126.7   Median :127.2  \n                    Mean   :2018-04-14   Mean   :138.6   Mean   :139.2  \n                    3rd Qu.:2020-12-03   3rd Qu.:163.7   3rd Qu.:164.2  \n                    Max.   :2023-07-28   Max.   :193.7   Max.   :194.4  \n      low            close           volume            adjusted    \n Min.   :100.2   Min.   :100.5   Min.   : 1436500   Min.   :100.5  \n 1st Qu.:118.7   1st Qu.:119.2   1st Qu.: 5505200   1st Qu.:119.2  \n Median :126.2   Median :126.7   Median : 7505400   Median :126.7  \n Mean   :138.0   Mean   :138.6   Mean   : 8642517   Mean   :138.6  \n 3rd Qu.:163.0   3rd Qu.:163.7   3rd Qu.:10342300   3rd Qu.:163.7  \n Max.   :192.5   Max.   :193.9   Max.   :93804200   Max.   :193.9  \n\n\nBom… olhando para as primeiras linhas do conjunto de dados podemos notar que as datas não são regularmente espaçadas, não paresentando valores para finais de semana e feriados. O que pode nos atrapalhar durante o processo de análise dos dados. A seguir vamos corrigir isso.\n\ngold_data %&gt;% \n  ggplot(aes(x=date,y=close))+\n  geom_line()+\n  labs(title = \"Valor Ouro 2013-2023\")+\n  theme_minimal()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')\n\n\n\n\n\n\n\n\nNessa primeira visualização gráfica do conjunto de dados não é possível identificar nada muito evidente em relação a informações que nos ajudem a criar um bom modelo, apenas mudanças em seu trend ao logo do tempo e que aparentemente não é estacionária.\nAntes de começar a fazer alguns testes vamos corrigir a irregularidade amostral dos dados."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#verificando-a-estacionariedade",
    "href": "posts/EDA-ouro/index.html#verificando-a-estacionariedade",
    "title": "Cotação do ouro- EDA",
    "section": "Verificando a estacionariedade",
    "text": "Verificando a estacionariedade\nComo mencionado anteriormente a série temporal é aparentemente não estacionária. Vamos realizar alguns testes para verificar.\n\ntdf_ouro %&gt;% \n  feasts::ACF() %&gt;% \n  autoplot()\n\nResponse variable not specified, automatically selected `var = value`\n\n\n\n\n\n\n\n\n\n\ntdf_ouro %&gt;% \n  feasts::PACF() %&gt;% \n  autoplot()\n\nResponse variable not specified, automatically selected `var = value`\n\n\n\n\n\n\n\n\n\nOs valores altos e smilares para os divérsos lags nos resultados de correlação cruzada e a presença de valores significativos apenas no lag 0 para os resultados de correlação parcial condizem com a análise visual, indicando que os dados são não estacionários, além de conter uma componente de tendência.\nvamos utilizar o teste Aumentado Dickey-Fuller (adf.test()) para verificar, de forma menos subjetiva se a série temporal é estacionária. E vamos aproveitar para verificar se ela é explosiva, informação que tembém será útil na etapa de modelagem.\n\ntseries::adf.test(ts(tdf_ouro$value),alternative =\"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(tdf_ouro$value)\nDickey-Fuller = -3.4463, Lag order = 15, p-value = 0.04743\nalternative hypothesis: stationary\n\n\nComo vemos existe uma evidência estatística fraca contra a hipótese nula, de que a ST é não estacionária. No entanto, esse valor ainda está próximo do limite de significância comumente utilizado de 0,05.\nEm termos práticos, um p-value de 0.04626 sugere que há uma probabilidade de aproximadamente 4,6% de obter os resultados observados ou mais extremos sob a hipótese nula de não-estacionariedade ser verdadeira. Portanto, embora não seja uma rejeição clara da hipótese nula, sugere-se que há uma tendência de que a série seja estacionária.\nPara melhorar o entendimento podemos utilizar outro teste para verificar a estacionariedade da série temporal.\n\ntdf_ouro %&gt;%\n  features(value, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      25.7        0.01\n\n\nO teste kpss (Kwiatkowski-Phillips-Schmidt-Shin) apresenta evidência para rejeitar a hipótese nula, que nesse caso é de estacionariedade.\nVamos verificar a explosividade da série.\n\ntseries::adf.test(ts(tdf_ouro$value),alternative = \"explosive\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(tdf_ouro$value)\nDickey-Fuller = -3.4463, Lag order = 15, p-value = 0.9526\nalternative hypothesis: explosive\n\n\nO alto valor do p-value sugere que os dados não fornecem suporte para a presença de explosividade na série.\nEm termos práticos, um p-value de 0.9537 indica que há uma probabilidade de aproximadamente 95,37% de obter os resultados observados ou mais extremos sob a hipótese nula de não explosividade ser verdadeira. Portanto, não há indícios significativos de explosividade na série temporal com base nos resultados do teste.\nAssim, em conjunto com a análise visual e dos gráficos de ACF e PACF temos indicios sufcientes para crêr que a série e não estacionária e também não explosiva.\nAté agora, as principais informações que podem nos ajudar na etapa de modelagem são:\n\nausência de processos explosivos, não sendo assim indicanda a necessidade de modelos da failia GARCH\nsérie não estacionária, indicando a necessidade de algum tipo de transformação nos dados. Como box-cox, sqrt, differenciação, remoção do trend, entre outras.\n\nVamos verificar os resultados de algumas dessas técnicas."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#interpolando-os-dados",
    "href": "posts/EDA-ouro/index.html#interpolando-os-dados",
    "title": "Cotação do ouro- EDA",
    "section": "Interpolando os dados",
    "text": "Interpolando os dados\nEu não sei exatamente o que é mais usual dentro do mundo de finanças, interpolar ou utilizar os dados como se fossem regularmnte espaçados, ignorando os finais de semana e feriados quando a bolsa não abre. Aqui eu vou interpolar os dados de modo simples. Utilizando a combinação das funções fill_gaps do pacote tsibble e fill do pacote tidyr.\n\ntdf_ouro &lt;- gold_data %&gt;%\n  tsibble::tsibble() %&gt;% \n  tsibble::fill_gaps() %&gt;% \n  tidyr::fill(close, .direction = \"down\") %&gt;% \n   rename(\n    index = date,\n    value = close\n  ) %&gt;% \n  select(index,value)\n\nUsing `date` as index variable.\n\nhead(tdf_ouro)\n\n# A tsibble: 6 x 2 [1D]\n  index      value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2013-01-02  163.\n2 2013-01-03  161.\n3 2013-01-04  160.\n4 2013-01-05  160.\n5 2013-01-06  160.\n6 2013-01-07  159.\n\n\nAgora com os dados interpolados, e selecionado apenas a variável que vamos trabalhar, podemos análisar melhor a sua estrutura dos dados."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#transformação-por-diferença",
    "href": "posts/EDA-ouro/index.html#transformação-por-diferença",
    "title": "Cotação do ouro- EDA",
    "section": "Transformação por diferença",
    "text": "Transformação por diferença\nComeçando com o método de diferenciação podemos verificar a quantidade de diferenças necessárias para tornar a série estacionária. Em seguida vamos testar sua estacionariedade utilizando o teste KPSS.\n\ntdf_ouro %&gt;%\n  features(value,  unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\n\ntdf_ouro %&gt;%\n   mutate(diff_close = difference(value)) %&gt;% \n  features(diff_close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.336         0.1\n\n\nAgora o tete KPSS não apresenta evidência para rejeitar a hipótese nula de estacionariedade.\nVamos visualizar a Série temporal após a aplicação de uma diferença.\n\ntdf_ouro %&gt;% \n  mutate(diff_close = difference(value)) %&gt;% \n  ggplot(aes(index,diff_close))+\n  geom_line()\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nVamos verificar a função de autocorrelção novamente\n\ntdf_ouro %&gt;% \n  mutate(diff_close = difference(value)) %&gt;% \n  feasts::ACF(diff_close,lag_max = 365) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\ntdf_ouro %&gt;% \n  mutate(diff_close = difference(value)) %&gt;% \n  features(diff_close, ljung_box, lag = 365)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    385.     0.227\n\n\nAtravés do gráfico e do teste Ljung-Box encontramos evidências de que após esse procedimento os dados não apresentam autocorrelação entre os atrasos da série."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#transformação-box-cox",
    "href": "posts/EDA-ouro/index.html#transformação-box-cox",
    "title": "Cotação do ouro- EDA",
    "section": "transformação Box Cox",
    "text": "transformação Box Cox\nNo caso do box-cox precisamos determinar o valor de alfa. Podemos obter esse valor utilizando a função forecast::BoxCox.lambda do pacote forecast.\n\nround(forecast::BoxCox.lambda(tdf_ouro$value), digits = 2)\n\n[1] 0.12\n\n\n\ntdf_ouro %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=0.12)) %&gt;% \n  features(box_cox_close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      25.4        0.01\n\n\nMesmo após a transformação continuamos encontrando evidências para rejeitar a estacionariedade.\nVisualizando os dados após a tranformação\n\ntdf_ouro %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=0.12)) %&gt;% \n  ggplot(aes(x=index, y=box_cox_close))+\n  geom_line()\n\n\n\n\n\n\n\n\nVamos verificar a função de autocorrelção novamente\n\ntdf_ouro %&gt;% \n  mutate(box_cox_close = fabletools::box_cox(value, lambda=0.12)) %&gt;% \n  feasts::ACF(box_cox_close,lag_max = 365) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\ntdf_ouro %&gt;% \n  mutate(box_cox_close = fabletools::box_cox(value, lambda=0.12)) %&gt;% \n  features(box_cox_close, ljung_box, lag = 365)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1 960012.         0\n\n\nexiste evidência para rejeitar a hiopótese nula de que não há autocorrelação."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#removendo-o-trend-da-série",
    "href": "posts/EDA-ouro/index.html#removendo-o-trend-da-série",
    "title": "Cotação do ouro- EDA",
    "section": "Removendo o trend da série",
    "text": "Removendo o trend da série\nOutro método que podemos tentar para tornar os dados estacionários é a remoção da tendência. Um dos modos de remover a tendência de uma série é à decompondo através de um modelo STL (Seasonal and Trend decomposition using Loess). Uma técnica de decomposição de séries temporais que visa separar os padrões sazonais, de tendência e de erro de uma série temporal. É amplamente utilizado para analisar e visualizar séries temporais com componentes sazonais e de tendência.\nA decomposição STL utiliza um algoritmo baseado em suavização local (Loess) para estimar as componentes da série temporal. O processo de decomposição envolve três etapas principais:\nSeasonal (Sazonal): O modelo STL estima a componente sazonal da série temporal. A componente sazonal representa as flutuações regulares e previsíveis que ocorrem em períodos fixos, como sazonalidade anual, trimestral, mensal ou semanal. A suavização Loess é usada para estimar os valores sazonais.\nTrend (Tendência): Em seguida, o modelo STL estima a componente de tendência da série temporal. A componente de tendência representa a direção geral dos dados, mostrando a evolução ou mudanças de longo prazo na série ao longo do tempo.\nRemainder (Erro): Por fim, o modelo STL estima a componente de erro, também chamada de resíduo. Essa componente captura as variações aleatórias ou não sistemáticas que não podem ser explicadas pelas componentes sazonal e de tendência.\nA decomposição STL é útil para entender melhor a estrutura das séries temporais, identificar padrões sazonais, avaliar tendências e isolar o ruído ou variações aleatórias que não podem ser explicadas pelas outras componentes.\n\nfit &lt;- tdf_ouro %&gt;%\n  fabletools::model(\n    feasts::STL(value)\n  ) %&gt;% \n  fabletools::components() \n\n\nfit %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\ndetrend_ts &lt;- fit %&gt;% \n  mutate(\n    value = value - trend\n  ) %&gt;% \n  select(index,value) \n\n\ndetrend_ts %&gt;%\n  features(value, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.154         0.1\n\n\nEncontramos evidencia de que a série é estacionária\nVamos visualiza-lá\n\ndetrend_ts %&gt;%\n  ggplot(aes(index,value))+\n  geom_line()\n\n\n\n\n\n\n\n\nVamos verificar a função de autocorrelção novamente\n\ndetrend_ts %&gt;% \n  feasts::ACF(value,lag_max = 100) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\ntdf_ouro %&gt;% \n  mutate(box_cox_close = fabletools::box_cox(value, lambda=0.12)) %&gt;% \n  features(box_cox_close, ljung_box, lag = 365)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1 960012.         0\n\n\nExistem evidências para rejeitar a hipótese nula de que não há autocorrelação. Assim como nos dados originais e na transformação box_cox, os dados apresentam correlação com seus atrasos de modo significativo. Porém a caracteristica da curva de correlção com a remoção do trend é bem diferente, sujerindo a existência de um padrão ciclico nos dados. Isso vai de encontro as componetes encontradas pelo modelo STL, que indica a existência de componetes sazonais nos dados. Contudo, os lags com valore de até 0.5 vão até o lag 35."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#análise-espectral-dos-dados",
    "href": "posts/EDA-ouro/index.html#análise-espectral-dos-dados",
    "title": "Cotação do ouro- EDA",
    "section": "Análise espectral dos dados",
    "text": "Análise espectral dos dados\nPara entender melhor essas estruturas existentes nos dados podemos recorrer a análises espectrais. Vamos analisar os nossos dados livre da tendência e também as componentes sazonais encontradas pelo modelo STL.\nVamos começar calculando os espectros de potência e escalogramas de wavelet das componentes sazonais indentificadas durante a decomposição dos dados.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.ad &lt;- analyze.wavelet(detrend_ts,\n                             \"value\",\n                             date.format=\"%Y-%m-%d\",\n                             loess.span = 0,\n                             verbose=FALSE)#análise wavelet stock\n\n\n# Calculando a transformada wavelet anual\nw.ouro.y &lt;- analyze.wavelet(fit,\n                            \"season_year\",\n                            date.format=\"%Y-%m-%d\",\n                            verbose=FALSE)#análise wavelet stock\n\n\n# Calculando a transformada wavelet semanal\nw.ouro.w &lt;- analyze.wavelet(fit,\n                            \"season_week\",\n                            date.format=\"%Y-%m-%d\",\n                            verbose=FALSE)#análise wavelet stock\n\nEspectro dos dados season adjusted\n\n#Plot do gráfico de espectro de potência ad\n\nwt.avg(w.ouro.ad)#,\n\n\n\n\n\n\n\n#maximum.level = maximum.level.ad,periodlab = \"period(days)\")#,\n#spec.period.axis = list(at = c(2,8,32,90,182,365)))\n\nescalograma dos dados season adjusted\n\n#Plot do escalograma de wavelet\nwt.image(w.ouro.ad, color.key = \"interval\",\n         legend.params = list(lab = \"wavelet power levels\"),\n         show.date = TRUE)#,\n\nWarning in wt.image(w.ouro.ad, color.key = \"interval\", legend.params = list(lab = \"wavelet power levels\"), : \nPlease check your calendar dates, format and time zone: dates may not be in an unambiguous format or chronological. The default numerical axis was used instead.\n\n\n\n\n\n\n\n\n#spec.period.axis = list(at = c(2,8,32,90,182,730,1095)))\n\nespectro dos dados season year fornecidos pela função decompose\n\n#Plot do gráfico de espectro de potência y\n\nwt.avg(w.ouro.y)#,\n\n\n\n\n\n\n\n#maximum.level = maximum.level.y,periodlab = \"period(days)\")#,\n#spec.period.axis = list(at = c(2,8,32,90,182,365)))\n\nescalograma dos dados season year\n\n#Plot do escalograma de wavelet\nwt.image(w.ouro.y, color.key = \"interval\",\n         legend.params = list(lab = \"wavelet power levels\"),\n         show.date = TRUE,\n         spec.period.axis = list(at = c(2,8,32,90,182,365,730)))\n\nWarning in wt.image(w.ouro.y, color.key = \"interval\", legend.params = list(lab = \"wavelet power levels\"), : \nPlease check your calendar dates, format and time zone: dates may not be in an unambiguous format or chronological. The default numerical axis was used instead.\n\n\n\n\n\n\n\n\n\nespectro dos dados season week\n\n#Plot do gráfico de espectro de potência w\nwt.avg(w.ouro.w, spec.period.axis = list(at = c(2,8,32,128,512)))\n\n\n\n\n\n\n\n\nescalograma dos dados season week\n\n#Plot do escalograma de wavelet\nwt.image(w.ouro.w, color.key = \"interval\",\n         legend.params = list(lab = \"wavelet power levels\"),\n         show.date = TRUE,\n         spec.period.axis = list(at = c(2,8,32,90,182,365)))\n\nWarning in wt.image(w.ouro.w, color.key = \"interval\", legend.params = list(lab = \"wavelet power levels\"), : \nPlease check your calendar dates, format and time zone: dates may not be in an unambiguous format or chronological. The default numerical axis was used instead."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nModelagem Elasticidade Preço-Demanda\n\n\nModelos Aditivos Generalizados (GAMs)\n\n\n\n\nModelos Estatísticos\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nPedro Lima\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nCotação do ouro - Parte 3\n\n\nSuavização Exponencial\n\n\n\n\nModelos Estatísticos\n\n\nSérie-Temporal\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nPedro Lima\n\n\n18 min\n\n\n\n\n\n\n  \n\n\n\n\nCotação do ouro - Parte 2\n\n\nAnálise Exploratória - Parte 2\n\n\n\n\nEDA\n\n\nSérie-Temporal\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nPedro Lima\n\n\n24 min\n\n\n\n\n\n\n  \n\n\n\n\nCotação do ouro - Parte 1\n\n\nAnálise Exploratória - Parte 1\n\n\n\n\nEDA\n\n\nSérie-Temporal\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nPedro Lima\n\n\n25 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/EDA-ouro/index.html#análise-preliminar",
    "href": "posts/EDA-ouro/index.html#análise-preliminar",
    "title": "Cotação do ouro- EDA",
    "section": "Análise preliminar",
    "text": "Análise preliminar\nUma vez que os dados foram carregados, podemos começar nossa análise. Vamos utilizar as funções head e summary para visualizar as primeiras linhas dos nossos dados e algumas estatísticas básicas.\n\nhead(gold_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 GLD    2013-01-02  163.  164.  163.  163. 10431800     163.\n2 GLD    2013-01-03  162.  163.  161.  161. 16117500     161.\n3 GLD    2013-01-04  160.  161.  159.  160. 19179800     160.\n4 GLD    2013-01-07  159.  160.  159.  159.  9361800     159.\n5 GLD    2013-01-08  160.  161.  160.  161.  7694800     161.\n6 GLD    2013-01-09  161.  161.  160.  160.  8334200     160.\n\nsummary(gold_data)\n\n    symbol               date                 open            high      \n Length:2664        Min.   :2013-01-02   Min.   :100.9   Min.   :101.0  \n Class :character   1st Qu.:2015-08-24   1st Qu.:119.2   1st Qu.:119.7  \n Mode  :character   Median :2018-04-17   Median :126.7   Median :127.2  \n                    Mean   :2018-04-16   Mean   :138.7   Mean   :139.3  \n                    3rd Qu.:2020-12-07   3rd Qu.:163.9   3rd Qu.:164.4  \n                    Max.   :2023-08-02   Max.   :193.7   Max.   :194.4  \n      low            close           volume            adjusted    \n Min.   :100.2   Min.   :100.5   Min.   : 1436500   Min.   :100.5  \n 1st Qu.:118.7   1st Qu.:119.2   1st Qu.: 5504400   1st Qu.:119.2  \n Median :126.2   Median :126.7   Median : 7501050   Median :126.7  \n Mean   :138.1   Mean   :138.7   Mean   : 8639569   Mean   :138.7  \n 3rd Qu.:163.0   3rd Qu.:163.8   3rd Qu.:10341400   3rd Qu.:163.8  \n Max.   :192.5   Max.   :193.9   Max.   :93804200   Max.   :193.9  \n\n\nAnalisando a saída dessas funções, talvez o ponto mais relevante seja a irregularidade amostral, uma vez que os dados não possuem registros para os finais de semana e feriados. Isso pode gerar efeitos indesejáveis e até inviabilizar algumas análises.\nDentre as diversas abordagens para lidar com essa situação, optaremos pelo método do “resampling”, onde converteremos nossos dados que possuem uma frequência amostral diária em mensal.\nApesar dessa técnica levar a uma perda de resolução, ela pode proporcionar uma simplificação do processamento e análise, além da diminuição do ruido nos dados. Pontos que serão muito positivos levando em conta o propósito demonstrativo deste post. É importante ressaltar que, dependendo do propósito da análise, a melhor opção seria utilizar o máximo de informação possível e optar pelo “descarte” de parte das informações apenas após uma avaliação mais criteriosa."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#criando-as-médias-mensais",
    "href": "posts/EDA-ouro/index.html#criando-as-médias-mensais",
    "title": "Cotação do ouro- EDA",
    "section": "Criando as médias mensais",
    "text": "Criando as médias mensais\nPara criar as médias mensais e deixar o conjunto de dados no formato desejado, são necessários alguns passos, os quais são explicados nos comentários do código abaixo. Em suma, a principal função utilizada foi a summarise do pacote dplyr.\n\ngold_data &lt;- gold_data %&gt;% \n  # criando variáveis ano e mês para cálculo das médias mensais\n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n    )%&gt;% \n  #agrupando os dados pelas variáveis ano e mês\n  group_by(year,month) %&gt;% \n  #calculando as médias mensais\n  summarise(\n    month_mean = mean(close)\n    ) %&gt;% \n  #desagrupando os dados\n  ungroup() %&gt;% \n  #criando a variável que será usada com index temporal\n   mutate(\n     index = make_yearmonth(year,month)\n     ) %&gt;% \n  #renomeando a coluna dos valores\n  rename(\n    value = month_mean\n    ) %&gt;%\n  #selecionando apenas as colunas de interesse\n  select(index,value) %&gt;% \n  # transformando os dados em tsibble, formato adequado para os pacotes utilizados\n  tsibble::tsibble() \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\nUsing `index` as index variable.\n\nhead(gold_data)\n\n# A tsibble: 6 x 2 [1M]\n     index value\n     &lt;mth&gt; &lt;dbl&gt;\n1 2013 jan  162.\n2 2013 fev  158.\n3 2013 mar  154.\n4 2013 abr  144.\n5 2013 mai  137.\n6 2013 jun  130."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#criando-um-gréfico-da-série-temporal",
    "href": "posts/EDA-ouro/index.html#criando-um-gréfico-da-série-temporal",
    "title": "Cotação do ouro- EDA",
    "section": "criando um gréfico da série temporal",
    "text": "criando um gréfico da série temporal\nAgora que os dados estão no formato desejádo vamos criar um gráfico para uma primeira avaliação visual.\n\ngold_data %&gt;% \n  ggplot(aes(x=index,y=value))+\n  geom_line()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')+\n  labs(title = \"Valor Ouro 2013-2023\")+\n  xlab(\"data\")+\n  ylab(\"\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnalisando o gráfico do conjunto de dados, é possível identificar apenas mudanças em sua tendência ao longo do tempo e que, aparentemente, a série não é estacionária e não possui valores extremos."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#decompondo-a-série-temporal",
    "href": "posts/EDA-ouro/index.html#decompondo-a-série-temporal",
    "title": "Cotação do ouro- EDA",
    "section": "Decompondo a série temporal",
    "text": "Decompondo a série temporal\nUma técnica comumente utilizada para extrair mais informações dos dados é a decomposição em diferentes componentes por meio de um modelo STL (Seasonal and Trend decomposition using Loess). O STL é uma técnica de decomposição de séries temporais que visa separar os padrões sazonais, de tendência e de erro de uma série temporal. Essa abordagem é amplamente utilizada para analisar e visualizar séries temporais que possuem componentes sazonais e de tendência. O método de decomposição STL utiliza um algoritmo baseado em suavização local (Loess) para estimar as componentes da série temporal.\n\nfit &lt;- gold_data %&gt;%\n  fabletools::model(\n    feasts::STL(value)\n  ) %&gt;% \n  fabletools::components() \n\nVamso visualizar as componentes obtidas pelo modelo STL.\n\nfit %&gt;% autoplot()\n\n\n\n\n\n\n\n\nO modelo indica a presença de componentes de tendência, ruído e um componente sazonal. Além disso, são observadas estruturas aparentes dentro da componente de ruído. Antes de prosseguirmos e investigarmos a possível presença de componentes periódicos nos dados, vamos verificar se a componente “remainder” pode ser considerada ruído branco. Para isso, utilizaremos o teste de Ljung-Box, que verifica a existência de correlação significativa nos dados.\n\nfit %&gt;% \n  features(remainder, ljung_box, lag = 48)\n\n# A tibble: 1 × 3\n  .model             lb_stat lb_pvalue\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 feasts::STL(value)    244.         0\n\n\nO resultado do teste rejeita a hipótese nula de que não há autocorrelação nos resíduos. Assim, não podemos considerar que a componente não possui informações relevantes."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#análises-espectrais",
    "href": "posts/EDA-ouro/index.html#análises-espectrais",
    "title": "Cotação do ouro- EDA",
    "section": "Análises espectrais",
    "text": "Análises espectrais\nPara investigar mais a fundo a presença de componentes periódicas nos dados à seguinte abordagem será adotada. Através do cálculo do espectro de potência e do escalogrma de wavelets, vamos verificar a presença e a evolução ao longo do registro das componentes espectrias nos dados originais e nas componentes (season, remainder) resultantes do modelo STL.\n\nRemoção da tendência e normalização dos dados\nUma etapa necessária antes de realizar a análise espectral de um conjunto de dados é a remoção da tendência, caso exista, e a normalização dos dados. Para isso, utilizaremos as componentes criadas pelo modelo STL, que foram salvas no objeto chamado “fit”, em conjunto com a função base do R chamada scale. Embora a maioria das funções especializadas em análise espectral possuam essas etapas embutidas em sua execução, realizaremos separadamente para ilustrar o processo.\n\ndetrend_ts &lt;- fit %&gt;% \n  mutate(\n    value = value - trend,\n    value = scale(value)\n  ) %&gt;% \n  select(index,value) \n\nhead(detrend_ts)\n\n# A tsibble: 6 x 2 [1M]\n     index value[,1]\n     &lt;mth&gt;     &lt;dbl&gt;\n1 2013 jan    1.94  \n2 2013 fev    1.69  \n3 2013 mar    1.54  \n4 2013 abr    0.0551\n5 2013 mai   -0.730 \n6 2013 jun   -1.65  \n\n\nPronto, nossos dados estão prontos para as análises espectrais!\n\n\nCálculo da transformada wavelet\nPara realizar essa análise, vamos utilizar o pacote WaveletComp. O código usado para calcular os valores e criar os gráficos será o mesmo tanto para os dados originais como para as saídas do modelo. Os códigos serão apresentados apenas na primeira aplicação e omitidos nas demais para evitar repetições desnecessárias.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- WaveletComp::analyze.wavelet(detrend_ts,\n                             \"value\",\n                             date.format=\"%Y-%b\",\n                             loess.span = 0,\n                             verbose=FALSE)\n\nUma vez calculada a transformada wavelet através da função analyze.wavelet, podemos criar o gráfico do espectro de potência dos dados sem tendência.\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\n\n\n\n\nO espectro de potência dos dados originais apresenta dois picos estatisticamente significativos, em aproximadamente 11 meses e 16 meses. Embora a presença desses dois picos seja evidente, os resultados não possuem uma boa resolução.\nVamos verificar o comportamento dessas componentes ao longo da variável tempo. O código abaixo cria o gráfico do escalograma de wavelets, utilizando também a saída da função analyze.wavelet.\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(w.ouro.dt, color.key = \"interval\",\n                       legend.params = list(lab = \"wavelet power levels\"),\n                       date.format=\"%Y-%b\",\n                       show.date = TRUE)\n\n\n\n\n\n\n\n\n\n# tibble(w.ouro.dt$Period, w.ouro.dt$Power.avg) %&gt;% \n#      arrange(-`w.ouro.dt$Power.avg`) %&gt;% \n#      filter(`w.ouro.dt$Power.avg` &gt; 0.35)\n\nAssim como nos resultados anteriores, não existe uma boa resolução das compoentes espectrais. Apesar de presentes ao longo de todo o registro, os valores de atribuidos aos períodos detectados são baixos, atingindo seus valores mais altos na parte final do registro. Outro ponto a ser destacado é que as componentes sofrem peqenas mudanças em suas faixas ao longo do registro, podendo ser efeito de interações complexas entre as componentes como modulação.\nVamos prosseguir com a análise e verificar os resultados para a componente season gerada pelo modelo STL.\n\n\n\n\n\n\n\n\n\nDiferente dos resultados anteriores, as componentes aqui presentes são bem definidas e estão centradas em 12 meses e 6 meses. Outros pontos relevantes a serem destacados são: a ausência da componente de aproximadamente 16 meses, que estava presente nos resultados anteriores. Além disso, a componente de 6 meses apresenta significância estatística, o que difere dos resultados anteriores.\n\n\n\n\n\n\n\n\n\nAs componentes detectadas nos resultados anteriores aparecem de forma contínua e bem definida ao longo de todo o registro. A componente anual apresenta comportamento semelhante ao longo de todo o registro. No entanto, a componente de 6 meses possui altos valores de amplitude atribuídos a ela apenas até a metade do tempo do registro, praticamente não apresentando energia na parte final.\nVamos agora verificar a componente “remainder” criada pelo modelo STL\n\n\n\n\n\n\n\n\n\nApesar de ser tratada como resíduo pelo modelo STL, os resultados do espectro de potência são bastante similares aos dos dados originais, apresentando picos espectrais mais definidos. As componentes detectadas estão em torno de ~16 meses e 10 meses. Há também uma componente de ~6 meses, mas esta está fora da margem de significância estatística.\nVamos verificar o escalograma de wavelets.\n\n\n\n\n\n\n\n\n\nÉ fácil notar a semelhança entre os resultados da componente “remainder” e os dados originais. Dessa forma, podemos concluir que o modelo STL não foi capaz de capturar a componente de ~16 meses ou a considerou como ruído vermelho. Isso é surpreendente, pois esse método é teoricamente capaz de identificar tanto padrões cíclicos como semicíclicos."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#reamostrando-os-dados",
    "href": "posts/EDA-ouro/index.html#reamostrando-os-dados",
    "title": "Cotação do ouro- EDA",
    "section": "Reamostrando os dados",
    "text": "Reamostrando os dados\nPara criar as médias mensais e deixar o conjunto de dados no formato desejado, são necessários alguns passos, os quais são explicados nos comentários do código abaixo. Em suma, a principal função utilizada foi a summarise do pacote dplyr.\n\ngold_data &lt;-\n  gold_data %&gt;% \n  # criando variáveis ano e mês para cálculo das médias mensais\n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n    )%&gt;% \n  #agrupando os dados pelas variáveis ano e mês\n  group_by(year,month) %&gt;% \n  #calculando as médias mensais\n  summarise(\n    month_mean = mean(close)\n    ) %&gt;% \n  #desagrupando os dados\n  ungroup() %&gt;% \n  #criando a variável que será usada com index temporal\n   mutate(\n     index = make_yearmonth(year,month)\n     ) %&gt;% \n  #renomeando a coluna dos valores\n  rename(\n    value = month_mean\n    ) %&gt;%\n  #selecionando apenas as colunas de interesse\n  select(index,value) %&gt;% \n  # transformando os dados em tsibble, formato adequado para os pacotes utilizados\n  tsibble::tsibble() \n\n\nhead(gold_data)\n\n# A tsibble: 6 x 2 [1M]\n     index value\n     &lt;mth&gt; &lt;dbl&gt;\n1 2013 jan  162.\n2 2013 fev  158.\n3 2013 mar  154.\n4 2013 abr  144.\n5 2013 mai  137.\n6 2013 jun  130."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#gráfico-da-série-temporal",
    "href": "posts/EDA-ouro/index.html#gráfico-da-série-temporal",
    "title": "Cotação do ouro- EDA",
    "section": "Gráfico da série temporal",
    "text": "Gráfico da série temporal\nAgora que os dados estão no formato desejádo vamos criar um gráfico para uma primeira avaliação visual.\n\ngold_data %&gt;% \n  ggplot(aes(x=index,y=value))+\n  geom_line()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')+\n  labs(title = \"Valor Ouro 2013-2023\")+\n  xlab(\"data\")+\n  ylab(\"\")+\n  theme_minimal()\n\n\n\n\nAnalisando o gráfico do conjunto de dados, é possível identificar apenas mudanças em sua tendência ao longo do tempo e que, aparentemente, a série não é estacionária e não possui valores extremos."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#remoção-da-tendência-e-normalização-dos-dados",
    "href": "posts/EDA-ouro/index.html#remoção-da-tendência-e-normalização-dos-dados",
    "title": "Cotação do ouro- EDA",
    "section": "Remoção da tendência e normalização dos dados",
    "text": "Remoção da tendência e normalização dos dados\nUma etapa necessária antes de realizar a análise espectral de um conjunto de dados é a remoção da tendência, caso exista, e a normalização dos dados. Para isso, utilizaremos as componentes criadas pelo modelo STL, que foram salvas no objeto chamado “fit”, em conjunto com a função base do R chamada scale. Embora a maioria das funções especializadas em análise espectral possuam essas etapas embutidas em sua execução, realizaremos separadamente para ilustrar o processo.\n\ndetrend_ts &lt;- \n  fit %&gt;%\n  mutate(\n    value = value - trend,\n    value = scale(value)\n    ) %&gt;% \n  select(index,value) \n\nhead(detrend_ts)\n\n# A tsibble: 6 x 2 [1M]\n     index value[,1]\n     &lt;mth&gt;     &lt;dbl&gt;\n1 2013 jan    1.94  \n2 2013 fev    1.68  \n3 2013 mar    1.54  \n4 2013 abr    0.0542\n5 2013 mai   -0.728 \n6 2013 jun   -1.65  \n\n\nPronto, nossos dados estão prontos para as análises espectrais!"
  },
  {
    "objectID": "posts/EDA-ouro/index.html#cálculo-da-transformada-wavelet",
    "href": "posts/EDA-ouro/index.html#cálculo-da-transformada-wavelet",
    "title": "Cotação do ouro- EDA",
    "section": "Cálculo da transformada wavelet",
    "text": "Cálculo da transformada wavelet\nPara realizar essa análise, vamos utilizar o pacote WaveletComp. O código usado para calcular os valores e criar os gráficos será o mesmo tanto para os dados originais como para as saídas do modelo. Os códigos serão apresentados apenas na primeira aplicação e omitidos nas demais para evitar repetições desnecessárias.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    date.format=\"%Y-%M\",\n    loess.span = 0,\n    verbose=FALSE\n    )\n\nUma vez calculada a transformada wavelet através da função analyze.wavelet, podemos criar o gráfico do espectro de potência dos dados sem tendência.\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\nO espectro de potência dos dados originais apresenta dois picos estatisticamente significativos, em aproximadamente 11 meses e 16 meses. Embora a presença desses dois picos seja evidente, os resultados não possuem uma boa resolução.\nVamos verificar o comportamento dessas componentes ao longo da variável tempo. O código abaixo cria o gráfico do escalograma de wavelets, utilizando também a saída da função analyze.wavelet.\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  date.format=\"%Y-%m-%d\",\n  show.date = TRUE\n  )\n\n\n\n\nAssim como nos resultados anteriores, não existe uma boa resolução das compoentes espectrais. Apesar de presentes ao longo de todo o registro, os valores de atribuidos aos períodos detectados são baixos, atingindo seus valores mais altos na parte final do registro. Outro ponto a ser destacado é que as componentes sofrem peqenas mudanças em suas faixas ao longo do registro, podendo ser efeito de interações complexas entre as componentes como modulação.\nVamos prosseguir com a análise e verificar os resultados para a componente season gerada pelo modelo STL.\n\n\n\n\n\nDiferente dos resultados anteriores, as componentes aqui presentes são bem definidas e estão centradas em 12 meses e 6 meses. Outros pontos relevantes a serem destacados são: a ausência da componente de aproximadamente 16 meses, que estava presente nos resultados anteriores. Além disso, a componente de 6 meses apresenta significância estatística, o que difere dos resultados anteriores.\n\n\n\n\n\nAs componentes detectadas nos resultados anteriores aparecem de forma contínua e bem definida ao longo de todo o registro. A componente anual apresenta comportamento semelhante ao longo de todo o registro. No entanto, a componente de 6 meses possui altos valores de amplitude atribuídos a ela apenas até a metade do tempo do registro, praticamente não apresentando energia na parte final.\nVamos agora verificar a componente “remainder” criada pelo modelo STL\n\n\n\n\n\nApesar de ser tratada como resíduo pelo modelo STL, os resultados do espectro de potência são bastante similares aos dos dados originais, apresentando picos espectrais mais definidos. As componentes detectadas estão em torno de ~16 meses e 10 meses. Há também uma componente de ~6 meses, mas esta está fora da margem de significância estatística.\nVamos verificar o escalograma de wavelets.\n\n\n\n\n\nÉ fácil notar a semelhança entre os resultados da componente “remainder” e os dados originais. Dessa forma, podemos concluir que o modelo STL não foi capaz de capturar a componente de ~16 meses ou a considerou como ruído vermelho. Isso é surpreendente, pois esse método é teoricamente capaz de identificar tanto padrões cíclicos como semicíclicos."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#pontos-mais-relevantes",
    "href": "posts/EDA-ouro/index.html#pontos-mais-relevantes",
    "title": "Cotação do ouro- EDA",
    "section": "pontos mais relevantes",
    "text": "pontos mais relevantes\nApós analisarmos os dados originais (sem tendência) e as componentes geradas pelo modelo STL, encontramos evidências que nos levam a crer na presença de componentes periódicas em nossos dados. Caso essa informação seja confirmada, teremos uma boa capacidade de predição, o que resultará em modelos mais eficientes e robustos."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#pontos-mais-relevantes-1",
    "href": "posts/EDA-ouro/index.html#pontos-mais-relevantes-1",
    "title": "Cotação do ouro- EDA",
    "section": "pontos mais relevantes",
    "text": "pontos mais relevantes\nO resultado da análise de autocorrelação reforça os resultados encontrados pelas análises espctrais. Apesar dos resultados de correlação apresentarem valores acima da faixa de significância para lags além de 18 meses, os valores são todos inferiores a 0.5. Além disso nenhuma periodicidade muito maior que 18 meses aparece nos testes espectrais.\nConforme evidenciado pelo gráfico da série temporal, esses resultados apontam para a não estacionariedade dos dados."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#verificando-a-explosividade-da-série.",
    "href": "posts/EDA-ouro/index.html#verificando-a-explosividade-da-série.",
    "title": "Cotação do ouro- EDA",
    "section": "Verificando a explosividade da série.",
    "text": "Verificando a explosividade da série.\nVerificar a explosividade de uma série temporal é de extrema importância em diversas áreas, especialmente em finanças e economia. A explosividade refere-se à presença de mudanças rápidas e extremas nos valores da série ao longo do tempo. Esses movimentos abruptos podem indicar eventos impactantes, mudanças estruturais.\nNas análises realizadas até o momento não existe nenhum indício da presença de moviemntos extremos em nossos dados. Contudo, vamos realizar um teste para determinar essa possibilidade.\n\ntseries::adf.test(ts(gold_data$value),alternative = \"explosive\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(gold_data$value)\nDickey-Fuller = -2.4131, Lag order = 5, p-value = 0.5952\nalternative hypothesis: explosive\n\n\nO p-value elevado sugere que os dados não oferecem respaldo para a presença de explosividade na série.\nEm termos práticos, um p-value de 0.5859 indica que há aproximadamente 58,59% de probabilidade de obter os resultados observados ou mais extremos sob a hipótese nula de não explosividade. Portanto, com base nos resultados do teste, não há indícios significativos de explosividade na série temporal.\nDesse modo, em conjunto com a análise visual e a observação do gráfico de ACF, temos elementos suficientes para concluir que a série não é estacionária e também não apresenta características explosivas."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#pontos-mais-relevantes-2",
    "href": "posts/EDA-ouro/index.html#pontos-mais-relevantes-2",
    "title": "Cotação do ouro- EDA",
    "section": "Pontos mais relevantes",
    "text": "Pontos mais relevantes\nAté o momento, as informações-chave que podem orientar a etapa de modelagem são as seguintes:\nA série não é estacionária, o que aponta para a necessidade de aplicar alguma forma de transformação nos dados. Opções incluem a transformação Box-Cox, raiz quadrada, diferenciação, remoção de tendência, entre outras."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#box-cox",
    "href": "posts/EDA-ouro/index.html#box-cox",
    "title": "Cotação do ouro- EDA",
    "section": "Box Cox",
    "text": "Box Cox\nA transformação de Box-Cox é uma técnica estatística usada para estabilizar a variância e tornar uma distribuição mais próxima da normalidade. Ela é frequentemente aplicada em séries temporais ou outras análises estatísticas quando os dados exibem heteroscedasticidade (variação não constante) ou não seguem uma distribuição normal. A transformação é especialmente útil quando você deseja aplicar métodos estatísticos que assumem uma distribuição normal dos dados.\n\\[y(\\lambda) = \\begin{cases}\n\\frac{y^\\lambda - 1}{\\lambda}, & \\text{se } \\lambda \\neq 0 \\\\\n\\log(y), & \\text{se } \\lambda = 0\n\\end{cases}\\]\nNesta fórmula,\\(y\\) é o valor original da série temporal e \\(λ\\) é o parâmetro de transformação. O parâmetro \\(λ\\) pode assumir qualquer valor real, e diferentes valores de \\(λ\\) resultam em diferentes transformações. A escolha do valor ideal de \\(λ\\) geralmente é feita de maneira a maximizar a normalidade ou estabilizar a variância dos dados transformados.\nPara escolher o valor ideal de \\(λ\\), é comum testar vários valores em um intervalo, aplicar a transformação a cada valor da série e analisar a normalidade e a homogeneidade da variância dos dados transformados. Isso pode ser feito visualmente ou por meio de testes estatísticos. Para essa tarefa vamos utilizando a função forecast::BoxCox.lambda do pacote forecast.\n\nlambda &lt;- round(forecast::BoxCox.lambda(gold_data$value), digits = 2)\nlambda\n\n[1] 0.45\n\n\n\ngold_data %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   features(box_cox_close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      1.82        0.01\n\n\nMesmo após a transformação continuamos encontrando evidências para rejeitar a estacionariedade. Não será mostrado, mas outras transformações foram tentadas (log, sqrt) e também não foram eficazes.\n\nVisualizando os dados após a tranformação\n\ngold_data %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n  ggplot(aes(x=index, y=box_cox_close))+\n  geom_line()\n\n\n\n\nComo essa abordagem não se mostrou efetiva não vamos processeguir avaliando seus resultados."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#diferenciação",
    "href": "posts/EDA-ouro/index.html#diferenciação",
    "title": "Cotação do ouro- EDA",
    "section": "Diferenciação",
    "text": "Diferenciação\nA diferenciação em séries temporais é uma técnica fundamental para transformar dados não estacionários em um formato mais adequado para análise e modelagem. Ela envolve a subtração de valores consecutivos da série, visando remover tendências e padrões de sazonalidade. Ao aplicar diferenciação, a série é transformada em uma nova série de diferenças, que é esperançosamente estacionária. Essa abordagem permite a utilização de modelos estatísticos, como o ARIMA (AutoRegressive Integrated Moving Average), que pressupõem a estacionariedade dos dados.\nPara se determinar o número de diferenças necessárias para tornar os dados estacionários usaremos a função unitroot_ndiffs. O termo “unit root” refere-se à raiz unitária, que é uma característica de uma série temporal não estacionária. A presença de uma raiz unitária indica que a série não reverte rapidamente a perturbações ou choques, o que pode tornar a análise e a modelagem mais desafiadoras.\n\ngold_data %&gt;%\n  features(value,  unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\nO teste indica que uma diferenciação é necessária para tornar a série estacionária.\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.388      0.0825\n\n\nO tete KPSS apresenta evidência para aceitar a hipótese nula de estacionariedade. Vamos visualizar a Série temporal após a aplicação de uma diferença.\n\n\n\n\n\nApós a transformação os dados, aparentemente, não existe nenhuma estrutura remanecente. Vamos utilizar o teste Ljung-Box para verificar se os valores observados das autocorrelações são consistentes com o que seria esperado em uma série de dados aleatórios sem autocorrelação. Valores significativos no teste podem indicar que há autocorrelações nas defasagens testadas, o que sugere que um modelo de série temporal pode ser necessário para capturar essas correlações.\n\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    17.5     0.131\n\n\nVamos verificar a função de autocorrelção novamente\n\n\n\n\n\nAtravés do gráfico e do teste Ljung-Box encontramos evidências de que após esse procedimento os dados não apresentam autocorrelação entre os atrasos da série.\n\npontos relevantes\nApós a modelagem, é necessário reverter a diferenciação para realizar previsões na escala original da série temporal. Isso envolve somar as diferenças previstas aos valores anteriores da série (ou ao último valor conhecido da série original). Ao realizar esse processo, estamos extrapolando as mudanças previstas para os próximos períodos e adicionando-as aos valores anteriores para obter as previsões finais. Isso pressupõe que as mudanças esperadas no período futuro sejam semelhantes às mudanças observadas no período de treinamento do modelo.\nAqui está uma fórmula geral para ilustrar o processo de reversão da diferenciação:\nSeja \\(y_t\\) o valor original na época \\(t\\) e \\(y'_t\\) a série temporal diferenciada na época \\(t\\). Seja \\(y'_t+1\\) a previsão diferenciada para o período \\(t+1\\). Então, a previsão final \\(y'_{t+1}\\) na escala original é calculada da seguinte forma:\n\\[y_{t+1} = y_t + y'_{t+1}\\]"
  },
  {
    "objectID": "posts/EDA-ouro/index.html#remoção-da-tendência",
    "href": "posts/EDA-ouro/index.html#remoção-da-tendência",
    "title": "Cotação do ouro- EDA",
    "section": "Remoção da tendência",
    "text": "Remoção da tendência\nApós a decomposição dos dados podemos obter os dados sem o trend de modo bem simples, apenas subtraindo o trend, encontrado pelo modelo, dos dados originais.\nVamos utilizar novamente o test KPSS para verificar a estacionariedade dos dados.\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1    0.0283         0.1\n\n\nEncontramos evidencia de que a série é estacionária\nVisualizando os dados após a remoção do trend.\n\n\n\n\n\nVamos verificar a função de autocorrelção novamente\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    146.         0\n\n\nExistem evidências para rejeitar a hipótese nula de que não há autocorrelação. Assim como nos dados originais e na transformação box_cox, os dados apresentam correlação com seus atrasos de modo significativo. Porém a caracteristica da curva de correlção com a remoção do trend é bem diferente, sugerindo a existência de um padrão ciclico nos dados. Isso vai de encontro as componetes encontradas pelo modelo STL, que indica a existência de componetes sazonais nos dados. Contudo, os lags com valore de até 0.5 vão até o lag 35."
  },
  {
    "objectID": "posts/EDA-ouro/index.html#carregando-os-pacotes-utilizados",
    "href": "posts/EDA-ouro/index.html#carregando-os-pacotes-utilizados",
    "title": "Cotação do ouro- EDA",
    "section": "",
    "text": "Neste projeto, utilizaremos uma seleção de pacotes para análise avançada de séries temporais financeiras. Para facilitar o carregamento dos pacotes vamos usar a função p_load() oriundo do pacote pacman, que permite carregar as bibliotecas de modo unificado\n\ntidyquant: Este pacote agiliza a coleta e manipulação de dados financeiros, integrando-se ao “tidyverse” e permitindo aquisição de dados de diversas fontes, como Yahoo Finanças.\ntsibble: O “tsibble” fornece uma estrutura de dados eficiente para manipulação de séries temporais, facilitando o tratamento de datas, horários e valores.\nfabletools: Parte da família “fable”, o “fabletools” oferece ferramentas para análise e previsão de séries temporais, tornando mais acessíveis técnicas avançadas.\ntimetk: Com foco na preparação de dados de séries temporais, o “timetk” oferece métodos simplificados para transformações e limpeza de informações temporais.\ntibbletime: Este pacote amplia as funcionalidades do “tibble” para dados temporais, permitindo manipulações intuitivas e agregações eficientes.\nfeasts: O “feasts” é voltado para modelagem e previsão de séries temporais, com suporte para decomposição, ajuste de modelos e geração de previsões.\ntidyverse: Uma coleção de pacotes interligados para manipulação e visualização de dados. Com suas funcionalidades, ele é uma base sólida para análise.\ntseries: Focado em econometria e séries temporais, o “tseries” oferece métodos estatísticos e ferramentas para análise temporal avançada.\nWaveletComp: Este pacote é utilizado para análise de séries temporais por meio da transformada wavelet, permitindo identificar padrões em diferentes escalas.\n\n\npacman::p_load(tidyquant, tsibble,fabletools,fabletools,timetk,fpp3,\n               tibbletime,feasts, tidyverse, tseries, WaveletComp,\n               tsoutliers, DT, plotly)"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download CV"
  },
  {
    "objectID": "posts/EDA-ouro/index.html#identificando-outliers",
    "href": "posts/EDA-ouro/index.html#identificando-outliers",
    "title": "Cotação do ouro- EDA",
    "section": "Identificando Outliers",
    "text": "Identificando Outliers\nEmbora o gráfico aparentemente não revele valores atípicos, iremos realizar uma verificação mais precisa utilizando o pacote tsoutliers. Esse pacote oferece uma opção automatizada para essa tarefa. Considerando que esses dados provêm de plataformas que disponibilizam valores oficiais do mercado de ações, a eventual presença de outliers nesses dados provavelmente estará ligada a eventos notáveis, e não a qualquer tipo de erro.\n\ntsoutliers::tso(ts(gold_data$value))\n\nSeries:  \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.2746\ns.e.  0.0832\n\nsigma^2 = 18.93:  log likelihood = -366.47\nAIC=736.94   AICc=737.04   BIC=742.63\n\nNo outliers were detected.\n\n\nNenhum outlier foi detectado, fato esperado já que os dados são médias mensais e portanto “suavizados”."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html",
    "href": "posts/serie_ouro/EDA/index.html",
    "title": "Cotação do ouro - Parte 1",
    "section": "",
    "text": "Este é o primeiro post de uma série dedicada ao desenvolvimento, de maneira simplificada, de um projeto de ciência de dados. Dividido em várias postagens e com ênfase variada nos tópicos, abordaremos os seguintes temas:\n\nDefinição do Problema\nAquisição de Dados\nLimpeza de Dados\nAnálise Exploratória\nPré-processamento de Dados\nModelagem\nAvaliação dos modelos\nImplantação dos modelos\nMonitoramento e Manutenção\nComunicação de Resultados\n\nNos dois primeiros artigos, abordarei os quatro primeiros itens, dando um enfoque na análise exploratória. As etapas de 5 a 7 serão discutidas em postagens subsequentes, nas quais uma ampla variedade de modelos estatísticos e de IA serão utilizados. O modelo que apresentar o melhor desempenho será implementado em produção e monitorado (tópicos 8, 9 e 10).\n\n\n\nConforme o título deste post sugere, esta série estará relacionada à cotação do ouro, mais especificamente na criação de um modelo preditivo para o SPDR Gold Shares (GLD).\n\n\n\nNeste projeto, farei uso de uma ampla variedade de pacotes e, consequentemente, suas respectivas funções para realizar as tarefas necessárias. No contexto do R, é comum carregar os pacotes próximos às linhas onde suas funções serão utilizadas. No entanto, isso nem sempre é viável, o que pode tornar a identificação da origem das funções confusa. Isso não apenas dificulta a leitura do código por terceiros, mas também pode complicar a revisão do próprio autor.\nPara tornar mais fácil identificar a origem das funções em uso, o R oferece uma opção útil: nome_do_pacote::nome_da_função(). Esse padrão é considerado uma boa prática no R para indicar a origem das funções e será adotado em todas as próximas postagens, sempre que necessário.\nPara simplificar o processo de carregamento dos pacotes, utilizaremos a função p_load(), que faz parte do pacote pacman. Essa função permite carregar as bibliotecas de forma centralizada e eficiente.\n\npacman::p_load(tidyquant, tsibble,fabletools,fabletools,timetk,fpp3,\n               tibbletime,feasts, tidyverse, tseries, WaveletComp,\n               tsoutliers, DT, plotly)\n\n\n\n\nOs dados utilizados nesta série são provenientes do Yahoo Finanças. Para acessá-los, utilizaremos a função tq_get do pacote tidyquant. Por padrão, essa função utiliza a opção stock.prices, que retorna os valores de ‘open’, ‘high’, ‘low’, ‘close’, ‘volume’ e ‘adjusted’ do Yahoo Finanças. No entanto, é possível obter outras opções, como dividendos ou informações sobre ‘split’ de ações, além de dados de outras fontes.\nQuando utilizamos o padrão da função e informamos apenas o símbolo da ação desejada, todos os dados disponíveis para o período de tempo completo são retornados.\n\n# Obter dados da ação de ouro (código GLD)\ngold_data &lt;- tidyquant::tq_get(\"GLD\")\n\n\n\n\nA etapa de limpeza de dados é um processo fundamental na análise de dados que envolve a identificação, correção e eliminação de erros, inconsistências e valores ausentes em um conjunto de dados. Isso inclui remover duplicatas, preencher lacunas, padronizar formatos e eliminar outliers. A limpeza de dados é crucial para garantir a qualidade e confiabilidade dos dados, pois dados sujos ou corrompidos podem levar a análises imprecisas e conclusões equivocadas. Portanto, essa etapa prepara os dados para análise posterior, assegurando que estejam prontos para serem explorados e modelados de forma eficaz.\nDevido a fonte dos dados, Yahoo Finanças, amplamente reconhecida e utilizada tanto na indústria quanto por investidores individuais, podemos antecipar que encontraremos poucos problemas.\nSem mais delongas vamos iniciar a análise dos dados.\n\n\n\nUma vez que os dados foram carregados, podemos começar nossa análise.\nVou começar criando uma tabela para visualizar os dados e utilizando as funções summary e str para obter informações da estrutura dos dados dentro do R e infomrações estatísticas básicas.\n\nDT::datatable(\n  gold_data,\n  options = list(\n    width = \"100%\",       # Definir a largura da tabela\n    columnDefs = list(\n      list(width = \"30%\", targets = 2),  # Ajustar a largura da coluna \"Nome\"\n      list(orderable = FALSE, targets = \"_all\")  # Desabilitar a ordenação em todas as colunas\n    ),\n    paging = TRUE,        # Ativar paginação\n    searching = FALSE,     # Ativar busca\n    style = \"responsive\"  # Estilo responsivo\n    )\n  ) %&gt;% \n  formatRound(\n  columns = colnames(gold_data)[sapply(gold_data, is.numeric)],\n  digits = 2\n)\n\n\n\n\n\n\n\n\n#base::summary(gold_data)\n#utils::str(gold_data)\n\nO aspecto mais significativo desses resultados é a irregularidade amostral. Apesar dos resultados da função summary não indicarem a presença de NA’s (valores ausentes), é importante observar que as datas na coluna “date” contêm registros para fins de semana e feriados, que são omitidos pela fonte dos dados. Isso pode resultar em efeitos indesejados e até mesmo tornar inviáveis algumas análises.\nDentre as diversas abordagens para lidar com essa situação, eu vou optar pelo método da reamostragem, onde converteremos nossos dados que possuem uma frequência amostral diária em mensal.\nApesar dessa técnica levar a uma perda de resolução, ela pode proporcionar uma simplificação do processamento e análise, além da diminuição do ruido nos dados. Pontos que serão muito positivos levando em conta o propósito demonstrativo deste post. É importante ressaltar que, dependendo do propósito da análise, a melhor opção seria utilizar o máximo de informação possível e optar pelo “descarte” de parte das informações apenas após uma avaliação mais criteriosa.\n\n\n\nA realização da reamostragem mencionada implica na criação de médias mensais, o que transformará o conjunto de dados no formato desejado. Os passos necessários para a execução desse procedimento estão explicados nos comentários do código a seguir. A maioria das funções utilizadas a seguir faz parte do pacote dplyr.\n\ngold_data &lt;-\n  gold_data %&gt;% \n  # criando variáveis ano e mês para cálculo das médias mensais\n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n    )%&gt;% \n  #agrupando os dados pelas variáveis ano e mês\n  group_by(year,month) %&gt;% \n  #calculando as médias mensais\n  summarise(\n    month_mean = mean(close)\n    ) %&gt;% \n  #desagrupando os dados\n  ungroup() %&gt;% \n  #criando a variável que será usada como index temporal\n   mutate(\n     index = tsibble::make_yearmonth(year,month)\n     ) %&gt;% \n  #renomeando a coluna dos valores\n  rename(\n    value = month_mean\n    ) %&gt;%\n  #selecionando apenas as colunas de interesse\n  select(index,value) %&gt;% \n  # transformando os dados em tsibble, formato adequado para os pacotes utilizados\n  tsibble::tsibble() \n\n\nhead(gold_data)\n\n# A tsibble: 6 x 2 [1M]\n     index value\n     &lt;mth&gt; &lt;dbl&gt;\n1 2013 jan  162.\n2 2013 fev  158.\n3 2013 mar  154.\n4 2013 abr  144.\n5 2013 mai  137.\n6 2013 jun  130.\n\n#class(gold_data)\n\nPronto, agora os dados estão no formato desejado.\n\n\n\nCom os dados preparados para análise, vou criar a primeira representação gráfica dos mesmos.\n\ngold_data %&gt;%\n  ggplot2::ggplot(aes(x=index,y=value))+\n  geom_line()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')+\n  labs(title = \"Cotação do Ouro entre 2013-2023\", )+\n  xlab(\"\")+\n  ylab(\"USD por Ação\")+\n  theme_minimal()\n\n\n\n\nAo analisar o gráfico do conjunto de dados, é possível identificar mudanças em sua tendência ao longo do tempo, e aparentemente, a série não é estacionária e não apresenta valores extremos. No entanto, é importante ressaltar que essas observações são baseadas apenas em uma primeira análise visual. A seguir, vou realizar alguns testes para confirmar ou refutar essas primeiras impressões.\n\n\n\nEmbora o gráfico aparentemente não revele valores atípicos, irei realizar uma verificação mais precisa utilizando o pacote tsoutliers. Esse pacote oferece uma opção automatizada para essa tarefa. Considerando que esses dados provêm de plataformas que disponibilizam valores oficiais do mercado de ações, a eventual presença de outliers nesses dados provavelmente estará ligada a eventos notáveis, e não a qualquer tipo de erro.\n\ntsoutliers::tso(ts(gold_data$value))\n\nSeries:  \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.2729\ns.e.  0.0826\n\nsigma^2 = 18.77:  log likelihood = -368.83\nAIC=741.66   AICc=741.76   BIC=747.36\n\nNo outliers were detected.\n\n\nBom…Nenhum outlier foi detectado, fato esperado já que os dados são médias mensais e portanto suavizados."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#carregando-os-pacotes-utilizados",
    "href": "posts/serie_ouro/EDA/index.html#carregando-os-pacotes-utilizados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Carregando os Pacotes Utilizados",
    "text": "Carregando os Pacotes Utilizados\n\nNeste projeto, farei uso de uma ampla variedade de pacotes e, consequentemente, suas respectivas funções para realizar as tarefas necessárias. No contexto do R, é comum carregar os pacotes próximos às linhas onde suas funções serão utilizadas. No entanto, isso nem sempre é viável, o que pode tornar a identificação da origem das funções confusa. Isso não apenas dificulta a leitura do código por terceiros, mas também pode complicar a revisão do próprio autor.\n\n\nPara tornar mais fácil identificar a origem das funções em uso, o R oferece uma opção útil: nome_do_pacote::nome_da_função(). Esse padrão é considerado uma boa prática no R para indicar a origem das funções e será adotado em todas as próximas postagens, sempre que necessário.\nPara simplificar o processo de carregamento dos pacotes, utilizaremos a função p_load(), que faz parte do pacote pacman. Essa função permite carregar as bibliotecas de forma centralizada e eficiente.\n\n\npacman::p_load(tidyquant, tsibble,fabletools,fabletools,timetk,fpp3,\n               tibbletime,feasts, tidyverse, tseries, WaveletComp,\n               tsoutliers, DT, plotly, kableExtra)\n\nAntes de continuar, vou criar uma função personalizada usando o pacote kableExtra para formatar as tabelas que serão apresentadas neste e nos próximos posts. Os argumentos desta função são os dados a serem tabulados e o título da tabela.\n\nset_tab &lt;- function(dados_tab, cap_tab){\n    kableExtra::kable( x = dados_tab,\n        booktabs = TRUE,\n          escape   = FALSE,\n          digits   = 4,\n          caption  = cap_tab) |&gt; \n  kableExtra::kable_styling(latex_options =\n                c(\"striped\", \"hold_position\"),\n                position      = \"center\",\n                full_width    = F,\n                bootstrap_options =\n                   c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n}"
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#reamostrando-os-dados",
    "href": "posts/serie_ouro/EDA/index.html#reamostrando-os-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Reamostrando os Dados",
    "text": "Reamostrando os Dados\n\nA realização da reamostragem mencionada implica na criação de médias mensais, o que transformará o conjunto de dados no formato desejado. Os passos necessários para a execução desse procedimento estão explicados nos comentários do código a seguir. A maioria das funções utilizadas a seguir faz parte do pacote dplyr.\n\n\ngold_data &lt;-\n  gold_data %&gt;% \n  # criando variáveis ano e mês para cálculo das médias mensais\n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n    )%&gt;% \n  #agrupando os dados pelas variáveis ano e mês\n  group_by(year,month) %&gt;% \n  #calculando as médias mensais\n  summarise(\n    month_mean = mean(close)\n    ) %&gt;% \n  #desagrupando os dados\n  ungroup() %&gt;% \n  #criando a variável que será usada como index temporal\n   mutate(\n     index = tsibble::make_yearmonth(year,month)\n     ) %&gt;% \n  #renomeando a coluna dos valores\n  rename(\n    value = month_mean\n    ) %&gt;%\n  #selecionando apenas as colunas de interesse\n  select(index,value) %&gt;% \n  # transformando os dados em tsibble, formato adequado para os pacotes utilizados\n  tsibble::tsibble() \n\n\n#head(gold_data)\n#class(gold_data)\n\n\ngold_data %&gt;% \n  slice_head(n=5) %&gt;% \n  set_tab(cap_tab = \"Médias mensais\")\n\n\n\nMédias mensais\n\n\nindex\nvalue\n\n\n\n\n2013 jan\n161.6733\n\n\n2013 fev\n157.6026\n\n\n2013 mar\n154.1335\n\n\n2013 abr\n143.6223\n\n\n2013 mai\n136.9909\n\n\n\n\n\n\n\n\nPronto, agora os dados estão no formato desejado."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#gráfico-da-série-temporal",
    "href": "posts/serie_ouro/EDA/index.html#gráfico-da-série-temporal",
    "title": "Cotação do ouro - Parte 1",
    "section": "Gráfico da Série Temporal",
    "text": "Gráfico da Série Temporal\nCom os dados preparados para análise, vou criar a primeira representação gráfica dos mesmos.\n\ngold_data %&gt;%\n  ggplot2::ggplot(aes(x=index,y=value))+\n  geom_line()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')+\n  labs(title = \"Cotação do Ouro entre 2013-2023\", )+\n  xlab(\"\")+\n  ylab(\"USD por Ação\")+\n  theme_minimal()\n\n\n\n\n\nAo analisar o gráfico do conjunto de dados, é possível identificar mudanças em sua tendência ao longo do tempo, e aparentemente, a série não é estacionária e não apresenta valores extremos. No entanto, é importante ressaltar que essas observações são baseadas apenas em uma primeira análise visual. A seguir, vou realizar alguns testes para confirmar ou refutar essas primeiras impressões."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#identificando-outliers",
    "href": "posts/serie_ouro/EDA/index.html#identificando-outliers",
    "title": "Cotação do ouro - Parte 1",
    "section": "Identificando Outliers",
    "text": "Identificando Outliers\n\nOutliers, ou valores atípicos, são observações em uma série temporal que se desviam significativamente do padrão ou comportamento esperado dos dados em um determinado período de tempo. Eles podem ser causados por uma variedade de razões, incluindo erros de medição, eventos incomuns, mudanças no processo subjacente ou simplesmente flutuações aleatórias extremas. Identificar e lidar com outliers é importante na análise de séries temporais, pois podem afetar negativamente a precisão dos modelos e análises. Considerando que esses dados provêm de plataformas que disponibilizam valores oficiais do mercado de ações, a eventual presença de outliers nesses dados provavelmente estará ligada a eventos notáveis, e não a qualquer tipo de erro.\nHá várias abordagens para identificar e tratar outliers em séries temporais. Optei por utilizar o pacote tsoutliers, que oferece uma opção automatizada para identificação desses valores atípicos.\n\n\ntsoutliers::tso(ts(gold_data$value)) \n\nSeries:  \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.2729\ns.e.  0.0826\n\nsigma^2 = 18.77:  log likelihood = -368.83\nAIC=741.66   AICc=741.76   BIC=747.36\n\nNo outliers were detected.\n\n\n\nBem, não foi identificado nenhum outlier, o que era esperado, considerando a fonte dos dados e o fato de que eles representam médias mensais, o que implica um certo grau de suavização."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#remoção-da-tendência-e-normalização-dos-dados",
    "href": "posts/serie_ouro/EDA/index.html#remoção-da-tendência-e-normalização-dos-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Remoção da Tendência e Normalização dos Dados",
    "text": "Remoção da Tendência e Normalização dos Dados\nAntes de avançarmos com a análise espectral, é imprescindível realizar duas etapas cruciais: a remoção da tendência, caso esta esteja presente, e a normalização dos dados. Para atingir esses objetivos, utilizaremos a componente ‘trend’, gerada pelo modelo STL, a qual será subtraída dos dados originais. Posteriormente, empregaremos a função ‘scale’ para centralizar e normalizar os dados, assegurando que estejam preparados de forma adequada para a análise subsequente.\nLembrando que esses procedimentos serão aplicados apenas para os dados originais. As componentes ‘season’ e ‘remainder’, provenientes do modelos STL, já estão livres de tendência e centrados.\n\ndetrend_ts &lt;- \n  fit %&gt;%\n  mutate(\n    value = value - trend,\n    value = scale(value)\n    ) %&gt;% \n  select(index,value) \n\nhead(detrend_ts)\n\n# A tsibble: 6 x 2 [1M]\n     index value[,1]\n     &lt;mth&gt;     &lt;dbl&gt;\n1 2013 jan    1.94  \n2 2013 fev    1.68  \n3 2013 mar    1.54  \n4 2013 abr    0.0542\n5 2013 mai   -0.728 \n6 2013 jun   -1.65  \n\n\nPronto, os dados estão prontos para as análises espectrais!"
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#cálculo-da-transformada-wavelet",
    "href": "posts/serie_ouro/EDA/index.html#cálculo-da-transformada-wavelet",
    "title": "Cotação do ouro - Parte 1",
    "section": "Cálculo da Transformada Wavelet",
    "text": "Cálculo da Transformada Wavelet\nPara realizar essa análise, vamos utilizar o pacote WaveletComp. O código usado para calcular os valores e criar os gráficos será o mesmo tanto para os dados originais como para as saídas do modelo. Os códigos serão apresentados apenas na primeira aplicação e omitidos nas demais para evitar repetições desnecessárias.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    date.format=\"%Y-%M\",\n    loess.span = 0,\n    verbose=FALSE\n    )\n\nUma vez calculada a transformada wavelet através da função analyze.wavelet, podemos criar o gráfico do espectro de potência dos dados sem tendência.\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\nO espectro de potência dos dados originais apresenta dois picos estatisticamente significativos, em aproximadamente 11 meses e 16 meses. Embora a presença desses dois picos seja evidente, os resultados não possuem uma boa resolução.\nVamos verificar o comportamento dessas componentes ao longo da variável tempo. O código abaixo cria o gráfico do escalograma de wavelets, utilizando também a saída da função analyze.wavelet.\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  date.format=\"%Y-%m-%d\",\n  show.date = TRUE\n  )\n\n\n\n\nAssim como nos resultados anteriores, não existe uma boa resolução das compoentes espectrais. Apesar de presentes ao longo de todo o registro, os valores de atribuidos aos períodos detectados são baixos, atingindo seus valores mais altos na parte final do registro. Outro ponto a ser destacado é que as componentes sofrem peqenas mudanças em suas faixas ao longo do registro, podendo ser efeito de interações complexas entre as componentes como modulação.\nVamos prosseguir com a análise e verificar os resultados para a componente season gerada pelo modelo STL.\n\n\n\n\n\nDiferente dos resultados anteriores, as componentes aqui presentes são bem definidas e estão centradas em 12 meses e 6 meses. Outros pontos relevantes a serem destacados são: a ausência da componente de aproximadamente 16 meses, que estava presente nos resultados anteriores. Além disso, a componente de 6 meses apresenta significância estatística, o que difere dos resultados anteriores.\n\n\n\n\n\nAs componentes detectadas nos resultados anteriores aparecem de forma contínua e bem definida ao longo de todo o registro. A componente anual apresenta comportamento semelhante ao longo de todo o registro. No entanto, a componente de 6 meses possui altos valores de amplitude atribuídos a ela apenas até a metade do tempo do registro, praticamente não apresentando energia na parte final.\nVamos agora verificar a componente “remainder” criada pelo modelo STL\n\n\n\n\n\nApesar de ser tratada como resíduo pelo modelo STL, os resultados do espectro de potência são bastante similares aos dos dados originais, apresentando picos espectrais mais definidos. As componentes detectadas estão em torno de ~16 meses e 10 meses. Há também uma componente de ~6 meses, mas esta está fora da margem de significância estatística.\nVamos verificar o escalograma de wavelets.\n\n\n\n\n\nÉ fácil notar a semelhança entre os resultados da componente “remainder” e os dados originais. Dessa forma, podemos concluir que o modelo STL não foi capaz de capturar a componente de ~16 meses ou a considerou como ruído vermelho. Isso é surpreendente, pois esse método é teoricamente capaz de identificar tanto padrões cíclicos como semicíclicos.\n\nPontos Mais Relevantes\nApós analisarmos os dados originais (sem tendência) e as componentes geradas pelo modelo STL, encontramos evidências que nos levam a crer na presença de componentes periódicas em nossos dados. Caso essa informação seja confirmada, teremos uma boa capacidade de predição, o que resultará em modelos mais eficientes e robustos."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#pontos-mais-relevantes",
    "href": "posts/serie_ouro/EDA/index.html#pontos-mais-relevantes",
    "title": "Cotação do ouro - Parte 1",
    "section": "pontos mais relevantes",
    "text": "pontos mais relevantes\nApós analisarmos os dados originais (sem tendência) e as componentes geradas pelo modelo STL, encontramos evidências que nos levam a crer na presença de componentes periódicas em nossos dados. Caso essa informação seja confirmada, teremos uma boa capacidade de predição, o que resultará em modelos mais eficientes e robustos."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#pontos-mais-relevantes-1",
    "href": "posts/serie_ouro/EDA/index.html#pontos-mais-relevantes-1",
    "title": "Cotação do ouro - Parte 1",
    "section": "Pontos Mais Relevantes",
    "text": "Pontos Mais Relevantes\nAté o momento, as informações-chave que podem orientar a etapa de modelagem são as seguintes:\nA série não é estacionária, o que aponta para a necessidade de aplicar alguma forma de transformação nos dados. Opções incluem a transformação Box-Cox, raiz quadrada, diferenciação, remoção de tendência, entre outras."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#verificando-a-explosividade-da-série.",
    "href": "posts/serie_ouro/EDA/index.html#verificando-a-explosividade-da-série.",
    "title": "Cotação do ouro - Parte 1",
    "section": "Verificando a Explosividade da Série.",
    "text": "Verificando a Explosividade da Série.\nVerificar a explosividade de uma série temporal é de extrema importância em diversas áreas, especialmente em finanças e economia. A explosividade refere-se à presença de mudanças rápidas e extremas nos valores da série ao longo do tempo. Esses movimentos abruptos podem indicar eventos impactantes, mudanças estruturais.\nNas análises realizadas até o momento não existe nenhum indício da presença de moviemntos extremos em nossos dados. Contudo, vamos realizar um teste para determinar essa possibilidade.\n\ntseries::adf.test(ts(gold_data$value),alternative = \"explosive\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(gold_data$value)\nDickey-Fuller = -2.4131, Lag order = 5, p-value = 0.5952\nalternative hypothesis: explosive\n\n\nO p-value elevado sugere que os dados não oferecem respaldo para a presença de explosividade na série.\nEm termos práticos, um p-value de 0.5859 indica que há aproximadamente 58,59% de probabilidade de obter os resultados observados ou mais extremos sob a hipótese nula de não explosividade. Portanto, com base nos resultados do teste, não há indícios significativos de explosividade na série temporal.\nDesse modo, em conjunto com a análise visual e a observação do gráfico de ACF, temos elementos suficientes para concluir que a série não é estacionária e também não apresenta características explosivas."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#pontos-mais-relevantes-2",
    "href": "posts/serie_ouro/EDA/index.html#pontos-mais-relevantes-2",
    "title": "Cotação do ouro - Parte 1",
    "section": "Pontos Mais Relevantes",
    "text": "Pontos Mais Relevantes\nAté o momento, as informações-chave que podem orientar a etapa de modelagem são as seguintes:\nA série não é estacionária, o que aponta para a necessidade de aplicar alguma forma de transformação nos dados. Opções incluem a transformação Box-Cox, raiz quadrada, diferenciação, remoção de tendência, entre outras."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#box-cox",
    "href": "posts/serie_ouro/EDA/index.html#box-cox",
    "title": "Cotação do ouro - Parte 1",
    "section": "Box Cox",
    "text": "Box Cox\nA transformação de Box-Cox é uma técnica estatística usada para estabilizar a variância e tornar uma distribuição mais próxima da normalidade. Ela é frequentemente aplicada em séries temporais ou outras análises estatísticas quando os dados exibem heteroscedasticidade (variação não constante) ou não seguem uma distribuição normal. A transformação é especialmente útil quando você deseja aplicar métodos estatísticos que assumem uma distribuição normal dos dados.\n\\[y(\\lambda) = \\begin{cases}\n\\frac{y^\\lambda - 1}{\\lambda}, & \\text{se } \\lambda \\neq 0 \\\\\n\\log(y), & \\text{se } \\lambda = 0\n\\end{cases}\\]\nNesta fórmula,\\(y\\) é o valor original da série temporal e \\(λ\\) é o parâmetro de transformação. O parâmetro \\(λ\\) pode assumir qualquer valor real, e diferentes valores de \\(λ\\) resultam em diferentes transformações. A escolha do valor ideal de \\(λ\\) geralmente é feita de maneira a maximizar a normalidade ou estabilizar a variância dos dados transformados.\nPara escolher o valor ideal de \\(λ\\), é comum testar vários valores em um intervalo, aplicar a transformação a cada valor da série e analisar a normalidade e a homogeneidade da variância dos dados transformados. Isso pode ser feito visualmente ou por meio de testes estatísticos. Para essa tarefa vamos utilizando a função forecast::BoxCox.lambda do pacote forecast.\n\nlambda &lt;- round(forecast::BoxCox.lambda(gold_data$value), digits = 2)\nlambda\n\n[1] 0.45\n\n\n\ngold_data %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   features(box_cox_close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      1.82        0.01\n\n\nMesmo após a transformação continuamos encontrando evidências para rejeitar a estacionariedade. Não será mostrado, mas outras transformações foram tentadas (log, sqrt) e também não foram eficazes.\n\nVisualizando os dados após a tranformação\n\ngold_data %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n  ggplot(aes(x=index, y=box_cox_close))+\n  geom_line()\n\n\n\n\nComo essa abordagem não se mostrou efetiva não vamos processeguir avaliando seus resultados."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#diferenciação",
    "href": "posts/serie_ouro/EDA/index.html#diferenciação",
    "title": "Cotação do ouro - Parte 1",
    "section": "Diferenciação",
    "text": "Diferenciação\nA diferenciação em séries temporais é uma técnica fundamental para transformar dados não estacionários em um formato mais adequado para análise e modelagem. Ela envolve a subtração de valores consecutivos da série, visando remover tendências e padrões de sazonalidade. Ao aplicar diferenciação, a série é transformada em uma nova série de diferenças, que é esperançosamente estacionária. Essa abordagem permite a utilização de modelos estatísticos, como o ARIMA (AutoRegressive Integrated Moving Average), que pressupõem a estacionariedade dos dados.\nPara se determinar o número de diferenças necessárias para tornar os dados estacionários usaremos a função unitroot_ndiffs. O termo “unit root” refere-se à raiz unitária, que é uma característica de uma série temporal não estacionária. A presença de uma raiz unitária indica que a série não reverte rapidamente a perturbações ou choques, o que pode tornar a análise e a modelagem mais desafiadoras.\n\ngold_data %&gt;%\n  features(value,  unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\nO teste indica que uma diferenciação é necessária para tornar a série estacionária.\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.388      0.0825\n\n\nO tete KPSS apresenta evidência para aceitar a hipótese nula de estacionariedade. Vamos visualizar a Série temporal após a aplicação de uma diferença.\n\n\n\n\n\nApós a transformação os dados, aparentemente, não existe nenhuma estrutura remanecente. Vamos utilizar o teste Ljung-Box para verificar se os valores observados das autocorrelações são consistentes com o que seria esperado em uma série de dados aleatórios sem autocorrelação. Valores significativos no teste podem indicar que há autocorrelações nas defasagens testadas, o que sugere que um modelo de série temporal pode ser necessário para capturar essas correlações.\n\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    17.5     0.131\n\n\nVamos verificar a função de autocorrelção novamente\n\n\n\n\n\nAtravés do gráfico e do teste Ljung-Box encontramos evidências de que após esse procedimento os dados não apresentam autocorrelação entre os atrasos da série.\n\npontos relevantes\nApós a modelagem, é necessário reverter a diferenciação para realizar previsões na escala original da série temporal. Isso envolve somar as diferenças previstas aos valores anteriores da série (ou ao último valor conhecido da série original). Ao realizar esse processo, estamos extrapolando as mudanças previstas para os próximos períodos e adicionando-as aos valores anteriores para obter as previsões finais. Isso pressupõe que as mudanças esperadas no período futuro sejam semelhantes às mudanças observadas no período de treinamento do modelo.\nAqui está uma fórmula geral para ilustrar o processo de reversão da diferenciação:\nSeja \\(y_t\\) o valor original na época \\(t\\) e \\(y'_t\\) a série temporal diferenciada na época \\(t\\). Seja \\(y'_t+1\\) a previsão diferenciada para o período \\(t+1\\). Então, a previsão final \\(y'_{t+1}\\) na escala original é calculada da seguinte forma:\n\\[y_{t+1} = y_t + y'_{t+1}\\]"
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#remoção-da-tendência",
    "href": "posts/serie_ouro/EDA/index.html#remoção-da-tendência",
    "title": "Cotação do ouro - Parte 1",
    "section": "Remoção da tendência",
    "text": "Remoção da tendência\nApós a decomposição dos dados podemos obter os dados sem o trend de modo bem simples, apenas subtraindo o trend, encontrado pelo modelo, dos dados originais.\nVamos utilizar novamente o test KPSS para verificar a estacionariedade dos dados.\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1    0.0283         0.1\n\n\nEncontramos evidencia de que a série é estacionária\nVisualizando os dados após a remoção do trend.\n\n\n\n\n\nVamos verificar a função de autocorrelção novamente\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    146.         0\n\n\nExistem evidências para rejeitar a hipótese nula de que não há autocorrelação. Assim como nos dados originais e na transformação box_cox, os dados apresentam correlação com seus atrasos de modo significativo. Porém a caracteristica da curva de correlção com a remoção do trend é bem diferente, sugerindo a existência de um padrão ciclico nos dados. Isso vai de encontro as componetes encontradas pelo modelo STL, que indica a existência de componetes sazonais nos dados. Contudo, os lags com valore de até 0.5 vão até o lag 35."
  },
  {
    "objectID": "posts/serie_ouro/model/index.html",
    "href": "posts/serie_ouro/model/index.html",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "Modelos ARIMA (Autoregressive Integrated Moving Average) e SARIMA (Seasonal ARIMA) são poderosas ferramentas de análise e previsão de séries temporais amplamente usadas em estatísticas e ciência de dados.\nOs modelos ARIMA combinam três componentes principais: o componente autoregressivo (AR), que leva em consideração as dependências lineares entre observações passadas; o componente de média móvel (MA), que modela o ruído da série; e a diferenciação integrada (I), que lida com tendências e sazonalidades não estacionárias. Juntos, esses componentes permitem modelar e prever uma ampla variedade de padrões em séries temporais univariadas.\nOs modelos SARIMA, por sua vez, estendem os ARIMA para lidar com sazonalidades sazonais. Eles incluem componentes adicionais para modelar sazonalidades anuais, trimestrais ou de qualquer período desejado. Isso os torna especialmente úteis para prever séries temporais com padrões sazonais evidentes.\nAmbos os modelos ARIMA e SARIMA exigem a escolha adequada das ordens dos componentes (p, d, q) e (P, D, Q, S), respectivamente, o que pode ser realizado por meio de análise exploratória dos dados, como a função de autocorrelação (ACF) e a função de autocorrelação parcial (PACF). Uma vez ajustados, esses modelos podem gerar previsões úteis e precisas para uma ampla gama de aplicações, desde previsões financeiras até demanda de produtos.\n\n\n\n\n\n\nsplits &lt;- \ntime_series_split(\n  gold_mean,\n  assess = \"1 year\",\n  cumulative = TRUE\n)\n\n\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media)\n\n\n\n\n\n\n\n\nA função auto_arima() é uma parte fundamental do pacote modeltime, que é uma extensão do pacote forecast e é projetado para facilitar a modelagem e previsão de séries temporais no R. auto_arima() é uma ferramenta poderosa que automatiza o processo de seleção do melhor modelo ARIMA (Autoregressive Integrated Moving Average) para uma série temporal específica.\nAqui estão alguns pontos-chave sobre auto_arima() no modeltime:\n\nSeleção Automática de Modelos: O principal objetivo do auto_arima() é determinar automaticamente os valores ideais dos parâmetros (p, d, q) para o modelo ARIMA que melhor se ajusta à sua série temporal. Isso é feito através de uma busca inteligente por meio de várias combinações possíveis desses parâmetros.\nSeleção de Sazonalidade: Além dos parâmetros ARIMA, auto_arima() também pode lidar com modelos SARIMA (Seasonal ARIMA) e selecionar automaticamente os parâmetros sazonais (P, D, Q, S) quando a série apresenta sazonalidade.\nCritérios de Avaliação: A função utiliza critérios de avaliação estatística, como o Critério de Informação de Akaike (AIC) e o Critério de Informação Bayesiano (BIC), para avaliar e comparar o ajuste de diferentes modelos. Ela seleciona o modelo com o menor valor desses critérios, o que geralmente indica um ajuste melhor à série.\nFacilidade de Uso: auto_arima() é projetado para ser fácil de usar. Basta fornecer a série temporal como entrada, e ele cuidará de todo o processo de seleção do modelo. Você não precisa especificar manualmente os parâmetros, a menos que deseje restringir as opções.\nFlexibilidade: Apesar da automação, auto_arima() também permite que você insira algumas restrições ou diretrizes para o processo de seleção, como limitar os valores máximos de p, d e q, ou forçar a inclusão de sazonalidade.\nResultados Interpretais: Uma vez que o auto_arima() encontra o melhor modelo, ele fornece um objeto de modelo que pode ser usado para fazer previsões e também inclui informações sobre os parâmetros selecionados, facilitando a interpretação do modelo final.\n\n\nauto_arima &lt;- \n  arima_reg() %&gt;% \n  set_engine(\"auto_arima\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\n\n\nJá foi descrito no ínicio. Os parâmetros foram determinados durante a análise exploratória.\n\nmodel_arima &lt;- \n  arima_reg(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n        non_seasonal_differences = 1,\n        non_seasonal_ma          = 0,\n        seasonal_ar              = 1,\n        seasonal_differences     = 1,\n        seasonal_ma              = 0\n    ) %&gt;% \n  set_engine(\"arima\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\n\n\nÉ uma extensão do modelo ARIMA tradicional que permite a incorporação de múltiplas sazonalidades em uma série temporal. Este modelo é particularmente útil quando seus dados exibem padrões sazonais em diferentes escalas de tempo.\n\narima_m_season &lt;- \n  seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n  ) %&gt;% \n  set_engine(\"stlm_arima\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\n\n\nA combinação de modelos ARIMA (Autoregressive Integrated Moving Average) com algoritmos de machine learning, como o XGBoost (Extreme Gradient Boosting), é uma abordagem útil para melhorar a previsão de séries temporais, especialmente quando os dados exibem padrões complexos e não lineares. A ideia principal é aproveitar a capacidade de ambos os modelos para capturar diferentes aspectos da série temporal.\n\nmodel_fit_arima_boosted &lt;- \n  arima_boost(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n    non_seasonal_differences = 1,\n    non_seasonal_ma = 0,\n    seasonal_ar     = 1,\n    seasonal_differences = 1,\n    seasonal_ma     = 0,\n\n    # XGBoost Args\n    tree_depth = 6,\n    learn_rate = 0.015,\n    min_n = 2) %&gt;% \n  set_engine(\"arima_xgboost\") %&gt;% \n  fit(media ~ mes + as.numeric(mes) + factor(month(mes, label = TRUE), ordered = F),\n        data = training(splits))\n\n\n\n\n\nmodel_table &lt;- \nmodeltime_table(\n  auto_arima,\n  model_arima,\n  arima_m_season,\n  model_fit_arima_boosted\n)\n\n\n\n\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))\n\n\n\n\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )\n\n\n\n\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 4 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         1 ARIMA(1,1,0)              Test  21.7  12.1   4.63 13.1   23.6 0.655 \n2         2 ARIMA(1,1,0)(1,1,0)[18]   Test   9.88  5.54  2.11  5.73  11.4 0.359 \n3         3 SEASONAL DECOMP: ARIMA(1… Test  20.1  11.2   4.28 12.0   22.0 0.0669\n4         4 ARIMA(1,1,0)(1,1,0)[18] … Test   9.87  5.53  2.11  5.72  11.4 0.364 \n\n\n\n\n\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()\n\n\n\n\n\n\n\n\n\ncalib_arima &lt;- \n  model_arima %&gt;% \n  modeltime_calibrate(testing(splits))\n\nfuture_forecast_tbl &lt;- \n  calib_arima %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#cross-validação",
    "href": "posts/serie_ouro/model/index.html#cross-validação",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "splits &lt;- \ntime_series_split(\n  gold_mean,\n  assess = \"1 year\",\n  cumulative = TRUE\n)\n\n\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media)"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#auto-arima",
    "href": "posts/serie_ouro/model/index.html#auto-arima",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "A função auto_arima() é uma parte fundamental do pacote modeltime, que é uma extensão do pacote forecast e é projetado para facilitar a modelagem e previsão de séries temporais no R. auto_arima() é uma ferramenta poderosa que automatiza o processo de seleção do melhor modelo ARIMA (Autoregressive Integrated Moving Average) para uma série temporal específica.\nAqui estão alguns pontos-chave sobre auto_arima() no modeltime:\n\nSeleção Automática de Modelos: O principal objetivo do auto_arima() é determinar automaticamente os valores ideais dos parâmetros (p, d, q) para o modelo ARIMA que melhor se ajusta à sua série temporal. Isso é feito através de uma busca inteligente por meio de várias combinações possíveis desses parâmetros.\nSeleção de Sazonalidade: Além dos parâmetros ARIMA, auto_arima() também pode lidar com modelos SARIMA (Seasonal ARIMA) e selecionar automaticamente os parâmetros sazonais (P, D, Q, S) quando a série apresenta sazonalidade.\nCritérios de Avaliação: A função utiliza critérios de avaliação estatística, como o Critério de Informação de Akaike (AIC) e o Critério de Informação Bayesiano (BIC), para avaliar e comparar o ajuste de diferentes modelos. Ela seleciona o modelo com o menor valor desses critérios, o que geralmente indica um ajuste melhor à série.\nFacilidade de Uso: auto_arima() é projetado para ser fácil de usar. Basta fornecer a série temporal como entrada, e ele cuidará de todo o processo de seleção do modelo. Você não precisa especificar manualmente os parâmetros, a menos que deseje restringir as opções.\nFlexibilidade: Apesar da automação, auto_arima() também permite que você insira algumas restrições ou diretrizes para o processo de seleção, como limitar os valores máximos de p, d e q, ou forçar a inclusão de sazonalidade.\nResultados Interpretais: Uma vez que o auto_arima() encontra o melhor modelo, ele fornece um objeto de modelo que pode ser usado para fazer previsões e também inclui informações sobre os parâmetros selecionados, facilitando a interpretação do modelo final.\n\n\nauto_arima &lt;- \n  arima_reg() %&gt;% \n  set_engine(\"auto_arima\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#arima-model",
    "href": "posts/serie_ouro/model/index.html#arima-model",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "Já foi descrito no ínicio. Os parâmetros foram determinados durante a análise exploratória.\n\nmodel_arima &lt;- \n  arima_reg(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n        non_seasonal_differences = 1,\n        non_seasonal_ma          = 0,\n        seasonal_ar              = 1,\n        seasonal_differences     = 1,\n        seasonal_ma              = 0\n    ) %&gt;% \n  set_engine(\"arima\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#modelo-arima-sazonal-múltiplo",
    "href": "posts/serie_ouro/model/index.html#modelo-arima-sazonal-múltiplo",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "É uma extensão do modelo ARIMA tradicional que permite a incorporação de múltiplas sazonalidades em uma série temporal. Este modelo é particularmente útil quando seus dados exibem padrões sazonais em diferentes escalas de tempo.\n\narima_m_season &lt;- \n  seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n  ) %&gt;% \n  set_engine(\"stlm_arima\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#arima-gxboost-dos-erros",
    "href": "posts/serie_ouro/model/index.html#arima-gxboost-dos-erros",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "A combinação de modelos ARIMA (Autoregressive Integrated Moving Average) com algoritmos de machine learning, como o XGBoost (Extreme Gradient Boosting), é uma abordagem útil para melhorar a previsão de séries temporais, especialmente quando os dados exibem padrões complexos e não lineares. A ideia principal é aproveitar a capacidade de ambos os modelos para capturar diferentes aspectos da série temporal.\n\nmodel_fit_arima_boosted &lt;- \n  arima_boost(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n    non_seasonal_differences = 1,\n    non_seasonal_ma = 0,\n    seasonal_ar     = 1,\n    seasonal_differences = 1,\n    seasonal_ma     = 0,\n\n    # XGBoost Args\n    tree_depth = 6,\n    learn_rate = 0.015,\n    min_n = 2) %&gt;% \n  set_engine(\"arima_xgboost\") %&gt;% \n  fit(media ~ mes + as.numeric(mes) + factor(month(mes, label = TRUE), ordered = F),\n        data = training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/model/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "model_table &lt;- \nmodeltime_table(\n  auto_arima,\n  model_arima,\n  arima_m_season,\n  model_fit_arima_boosted\n)"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#calibrando",
    "href": "posts/serie_ouro/model/index.html#calibrando",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#residual",
    "href": "posts/serie_ouro/model/index.html#residual",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#accuracy",
    "href": "posts/serie_ouro/model/index.html#accuracy",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 4 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         1 ARIMA(1,1,0)              Test  21.7  12.1   4.63 13.1   23.6 0.655 \n2         2 ARIMA(1,1,0)(1,1,0)[18]   Test   9.88  5.54  2.11  5.73  11.4 0.359 \n3         3 SEASONAL DECOMP: ARIMA(1… Test  20.1  11.2   4.28 12.0   22.0 0.0669\n4         4 ARIMA(1,1,0)(1,1,0)[18] … Test   9.87  5.53  2.11  5.72  11.4 0.364"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#test-set-visualization",
    "href": "posts/serie_ouro/model/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/model/index.html#forecast-future",
    "href": "posts/serie_ouro/model/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_arima &lt;- \n  model_arima %&gt;% \n  modeltime_calibrate(testing(splits))\n\nfuture_forecast_tbl &lt;- \n  calib_arima %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/lstm/modelo_sltm.html",
    "href": "posts/serie_ouro/lstm/modelo_sltm.html",
    "title": "Cotação do ouro - Parte 5",
    "section": "",
    "text": "Desenvolvendo uma estratégia de Backtesting\n# Realizar o backtesting com a função rolling_origin()\nperiods_train &lt;- 12 * 4\nperiods_test  &lt;- 12 * 1\nskip_span     &lt;- 12 * 2\n\n\nrolling_origin_resamples &lt;- rsample::rolling_origin(\n    gold_mean,\n    initial    = periods_train,\n    assess     = periods_test,\n    cumulative = FALSE,\n    skip       = skip_span\n)\nrolling_origin_resamples\n\n# Rolling origin forecast resampling \n# A tibble: 3 × 2\n  splits          id    \n  &lt;list&gt;          &lt;chr&gt; \n1 &lt;split [48/12]&gt; Slice1\n2 &lt;split [48/12]&gt; Slice2\n3 &lt;split [48/12]&gt; Slice3\nCriando uma função para visualizar os blocos de Backtest\n# Plotting function for a single split\nplot_split &lt;- function(split, expand_y_axis = TRUE, alpha = 1, size = 1, base_size = 14) {\n    \n    # Manipulate data\n    train_tbl &lt;- training(split) %&gt;%\n        add_column(key = \"training\") \n    \n    test_tbl  &lt;- testing(split) %&gt;%\n        add_column(key = \"testing\") \n    \n    data_manipulated &lt;- bind_rows(train_tbl, test_tbl) %&gt;%\n        as_tbl_time(index = index) %&gt;%\n        mutate(key = fct_relevel(key, \"training\", \"testing\"))\n        \n    # Collect attributes\n    train_time_summary &lt;- train_tbl %&gt;%\n        tk_index() %&gt;%\n        tk_get_timeseries_summary()\n    \n    test_time_summary &lt;- test_tbl %&gt;%\n        tk_index() %&gt;%\n        tk_get_timeseries_summary()\n    \n    # Visualize\n    g &lt;- data_manipulated %&gt;%\n        ggplot(aes(x = index, y = value, color = key)) +\n        geom_line(linewidth = size, alpha = alpha) +\n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        labs(\n            title    = glue(\"Split: {split$id}\"),\n            subtitle = glue(\"{train_time_summary$start} to {test_time_summary$end}\"),\n            y = \"\", x = \"\"\n        ) +\n        theme(legend.position = \"none\") \n    \n    if (expand_y_axis) {\n        \n        gold_ts_time_summary &lt;- gold_mean %&gt;% \n            tk_index() %&gt;% \n            tk_get_timeseries_summary()\n        \n        g &lt;- g +\n            scale_x_date(limits = c(gold_ts_time_summary$start, \n                                    gold_ts_time_summary$end))\n    }\n    \n    return(g)\n}\nVerificando um dos blocos de Backtest\nrolling_origin_resamples$splits[[1]] %&gt;%\n    plot_split(expand_y_axis = TRUE) +\n    theme(legend.position = \"bottom\")\nFunção para colocar os gráficos na escala de tempo do recorte dos dados\n# Plotting function that scales to all splits \nplot_sampling_plan &lt;- function(sampling_tbl, expand_y_axis = TRUE, \n                               ncol = 3, alpha = 1, size = 1, base_size = 14, \n                               title = \"Sampling Plan\") {\n    \n    # Map plot_split() to sampling_tbl\n    sampling_tbl_with_plots &lt;- sampling_tbl %&gt;%\n        mutate(gg_plots = map(splits, plot_split, \n                              expand_y_axis = expand_y_axis,\n                              alpha = alpha, base_size = base_size))\n    \n    # Make plots with cowplot\n    plot_list &lt;- sampling_tbl_with_plots$gg_plots \n    \n    p_temp &lt;- plot_list[[1]] + theme(legend.position = \"bottom\")\n    legend &lt;- get_legend(p_temp)\n    \n    p_body  &lt;- plot_grid(plotlist = plot_list, ncol = ncol)\n    \n    p_title &lt;- ggdraw() + \n        draw_label(title, size = 18, fontface = \"bold\", colour = palette_light()[[1]])\n    \n    g &lt;- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n    \n    return(g)\n    \n}\nPlotando todos os blocos de teste\nrolling_origin_resamples %&gt;%\n    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Rolling Origin Sampling Plan\")\nPlotando os gráficos em uma escala mais apropriada “zoom”\nrolling_origin_resamples %&gt;%\n    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Zoomed In\")"
  },
  {
    "objectID": "posts/serie_ouro/lstm/modelo_sltm.html#modelagem-the-keras-stateful-lstm-model",
    "href": "posts/serie_ouro/lstm/modelo_sltm.html#modelagem-the-keras-stateful-lstm-model",
    "title": "Cotação do ouro - Parte 5",
    "section": "Modelagem The Keras Stateful LSTM Model",
    "text": "Modelagem The Keras Stateful LSTM Model"
  },
  {
    "objectID": "posts/serie_ouro/lstm/modelo_sltm.html#single-lstm",
    "href": "posts/serie_ouro/lstm/modelo_sltm.html#single-lstm",
    "title": "Cotação do ouro - Parte 5",
    "section": "Single LSTM",
    "text": "Single LSTM\n\nsplit    &lt;- rolling_origin_resamples$splits[[1]]\nsplit_id &lt;- rolling_origin_resamples$id[[1]]\n\n\nplot_split(split, expand_y_axis = FALSE, size = 0.5) +\n    theme(legend.position = \"bottom\") +\n    ggtitle(glue(\"Split: {split_id}\"))\n\n\n\n\nCriando um index para treino e teste e combinando em um df\n\ndf_trn &lt;- training(split)\ndf_tst &lt;- testing(split)\n\ndf &lt;- bind_rows(\n    df_trn %&gt;% add_column(key = \"training\"),\n    df_tst %&gt;% add_column(key = \"testing\")\n) %&gt;% \n    as_tbl_time(index = index)\n\ndf\n\n# A time tibble: 60 × 3\n# Index:         index\n   index      value key     \n   &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;   \n 1 2013-01-01  162. training\n 2 2013-02-01  158. training\n 3 2013-03-01  154. training\n 4 2013-04-01  144. training\n 5 2013-05-01  137. training\n 6 2013-06-01  130. training\n 7 2013-07-01  124. training\n 8 2013-08-01  131. training\n 9 2013-09-01  130. training\n10 2013-10-01  127. training\n# ℹ 50 more rows"
  },
  {
    "objectID": "posts/serie_ouro/lstm/modelo_sltm.html#preprocessing-with-recipes",
    "href": "posts/serie_ouro/lstm/modelo_sltm.html#preprocessing-with-recipes",
    "title": "Cotação do ouro - Parte 5",
    "section": "Preprocessing With Recipes",
    "text": "Preprocessing With Recipes\nThe LSTM algorithm requires the input data to be centered and scaled. We can preprocess the data using the recipes package.\n\nrec_obj &lt;- recipe(value ~ ., df) %&gt;%\n    step_sqrt(value) %&gt;%\n    step_center(value) %&gt;%\n    step_scale(value) %&gt;%\n   # step_BoxCox(value) %&gt;% \n    prep()\n\ndf_processed_tbl &lt;- bake(rec_obj, df)\n\ndf_processed_tbl\n\n# A tibble: 60 × 3\n   index      key      value\n   &lt;date&gt;     &lt;fct&gt;    &lt;dbl&gt;\n 1 2013-01-01 training 3.40 \n 2 2013-02-01 training 3.08 \n 3 2013-03-01 training 2.80 \n 4 2013-04-01 training 1.93 \n 5 2013-05-01 training 1.37 \n 6 2013-06-01 training 0.733\n 7 2013-07-01 training 0.277\n 8 2013-08-01 training 0.823\n 9 2013-09-01 training 0.783\n10 2013-10-01 training 0.500\n# ℹ 50 more rows\n\n\nA seguir, vamos capturar o histórico do centro/escala para que possamos inverter o centro e a escala após a modelagem. A transformação da raiz quadrada pode ser revertida ao elevar ao quadrado os valores invertidos do centro/escala.\n\n   center_history &lt;- rec_obj$steps[[2]]$means[\"value\"]\n   scale_history  &lt;- rec_obj$steps[[3]]$sds[\"value\"]\n\nc(\"center\" = center_history, \"scale\" = scale_history)\n\ncenter.value  scale.value \n  11.0197810    0.4985484"
  },
  {
    "objectID": "posts/serie_ouro/lstm/modelo_sltm.html#lstm-plan",
    "href": "posts/serie_ouro/lstm/modelo_sltm.html#lstm-plan",
    "title": "Cotação do ouro - Parte 5",
    "section": "LSTM Plan",
    "text": "LSTM Plan\n\n# Model inputs\nlag_setting  &lt;- 12 # = nrow(df_tst)\nbatch_size   &lt;- 1\ntrain_length &lt;- 12*6\ntsteps       &lt;- 1\nepochs       &lt;- 50\n\n\n# Training Set\nlag_train_tbl &lt;- df_processed_tbl %&gt;%\n    mutate(value_lag = lag(value, n = lag_setting)) %&gt;%\n    filter(!is.na(value_lag)) %&gt;%\n    filter(key == \"training\") %&gt;%\n    tail(train_length)\n\nx_train_vec &lt;- lag_train_tbl$value_lag\nx_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n\ny_train_vec &lt;- lag_train_tbl$value\ny_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n\n# Testing Set\nlag_test_tbl &lt;- df_processed_tbl %&gt;%\n    mutate(\n        value_lag = lag(value, n = lag_setting)\n    ) %&gt;%\n    filter(!is.na(value_lag)) %&gt;%\n    filter(key == \"testing\")\n\nx_test_vec &lt;- lag_test_tbl$value_lag\nx_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n\ny_test_vec &lt;- lag_test_tbl$value\ny_test_arr &lt;- array(data = y_test_vec, dim = c(length(y_test_vec), 1))\n\n\nConstruindo o modelo LSTM\n\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n    layer_lstm(units            = 50, \n               input_shape      = c(tsteps, 1), \n               batch_size       = batch_size,\n               return_sequences = TRUE, \n               stateful         = TRUE) %&gt;% \n     layer_lstm(units            = 50, \n                return_sequences = FALSE, \n                stateful         = TRUE) %&gt;% \n    layer_dense(units = 1)\n\nmodel %&gt;% \n    compile(\n      loss = 'mae',\n      optimizer = 'adam',\n      ) \n  \n\nmodel\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n lstm_1 (LSTM)                      (1, 1, 50)                      10400       \n lstm (LSTM)                        (1, 50)                         20200       \n dense (Dense)                      (1, 1)                          51          \n================================================================================\nTotal params: 30,651\nTrainable params: 30,651\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\nFitting The LSTM Model\n\nfor (i in 1:epochs) {\n    model %&gt;% fit(x          = x_train_arr, \n                  y          = y_train_arr, \n                  batch_size = batch_size,\n                  epochs     = 1, \n                  verbose    = 1, \n                  shuffle    = FALSE)\n    \n    model %&gt;% reset_states()\n    cat(\"Epoch: \", i)\n    \n}\n\n\n 1/36 [..............................] - ETA: 3:00 - loss: 0.1243\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.4318  \n19/36 [==============&gt;...............] - ETA: 0s - loss: 0.5704\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.6901\n36/36 [==============================] - 5s 5ms/step - loss: 0.6645\nEpoch:  1\n 1/36 [..............................] - ETA: 0s - loss: 0.1467\n11/36 [========&gt;.....................] - ETA: 0s - loss: 0.3043\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.5850\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.5647\n36/36 [==============================] - 0s 5ms/step - loss: 0.5695\nEpoch:  2\n 1/36 [..............................] - ETA: 0s - loss: 0.1486\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2500\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.4841\n36/36 [==============================] - 0s 3ms/step - loss: 0.4988\nEpoch:  3\n 1/36 [..............................] - ETA: 0s - loss: 0.1409\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2064\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.4307\n36/36 [==============================] - 0s 3ms/step - loss: 0.4298\nEpoch:  4\n 1/36 [..............................] - ETA: 0s - loss: 0.1175\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2427\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.4171\n36/36 [==============================] - 0s 4ms/step - loss: 0.4952\nEpoch:  5\n 1/36 [..............................] - ETA: 0s - loss: 0.1757\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.5734\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.5841\n36/36 [==============================] - 0s 4ms/step - loss: 0.5819\nEpoch:  6\n 1/36 [..............................] - ETA: 0s - loss: 0.0461\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.4321\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.6246\n36/36 [==============================] - 0s 4ms/step - loss: 0.5631\nEpoch:  7\n 1/36 [..............................] - ETA: 0s - loss: 0.0969\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2807\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.4755\n36/36 [==============================] - 0s 4ms/step - loss: 0.5151\nEpoch:  8\n 1/36 [..............................] - ETA: 0s - loss: 0.1292\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2458\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.4488\n36/36 [==============================] - 0s 4ms/step - loss: 0.4624\nEpoch:  9\n 1/36 [..............................] - ETA: 0s - loss: 0.0639\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2729\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.4001\n36/36 [==============================] - 0s 4ms/step - loss: 0.4741\nEpoch:  10\n 1/36 [..............................] - ETA: 0s - loss: 0.1547\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.4588\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.5089\n36/36 [==============================] - 0s 4ms/step - loss: 0.5267\nEpoch:  11\n 1/36 [..............................] - ETA: 0s - loss: 0.0314\n11/36 [========&gt;.....................] - ETA: 0s - loss: 0.4457\n22/36 [=================&gt;............] - ETA: 0s - loss: 0.4224\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.5152\n36/36 [==============================] - 0s 6ms/step - loss: 0.5176\nEpoch:  12\n 1/36 [..............................] - ETA: 0s - loss: 0.0910\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1971\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.3761\n36/36 [==============================] - 0s 4ms/step - loss: 0.4396\nEpoch:  13\n 1/36 [..............................] - ETA: 0s - loss: 0.0319\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.3109\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.4136\n36/36 [==============================] - 0s 4ms/step - loss: 0.4416\nEpoch:  14\n 1/36 [..............................] - ETA: 0s - loss: 0.1291\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3056\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.4298\n36/36 [==============================] - 0s 4ms/step - loss: 0.4613\nEpoch:  15\n 1/36 [..............................] - ETA: 0s - loss: 0.0386\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.3788\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.4192\n36/36 [==============================] - 0s 4ms/step - loss: 0.5487\nEpoch:  16\n 1/36 [..............................] - ETA: 0s - loss: 0.1373\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2367\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.4756\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.4319\n36/36 [==============================] - 0s 5ms/step - loss: 0.4471\nEpoch:  17\n 1/36 [..............................] - ETA: 0s - loss: 0.1056\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.2104\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.3645\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3699\n36/36 [==============================] - 0s 5ms/step - loss: 0.3901\nEpoch:  18\n 1/36 [..............................] - ETA: 0s - loss: 0.0995\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2136\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.3064\n36/36 [==============================] - 0s 4ms/step - loss: 0.3577\nEpoch:  19\n 1/36 [..............................] - ETA: 0s - loss: 0.0615\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2026\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.2314\n36/36 [==============================] - 0s 4ms/step - loss: 0.3305\nEpoch:  20\n 1/36 [..............................] - ETA: 0s - loss: 0.0694\n 9/36 [======&gt;.......................] - ETA: 0s - loss: 0.1583\n21/36 [================&gt;.............] - ETA: 0s - loss: 0.2660\n36/36 [==============================] - 0s 4ms/step - loss: 0.3743\nEpoch:  21\n 1/36 [..............................] - ETA: 0s - loss: 0.0566\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2504\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3632\n36/36 [==============================] - 0s 3ms/step - loss: 0.4082\nEpoch:  22\n 1/36 [..............................] - ETA: 0s - loss: 0.1442\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1770\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.4192\n36/36 [==============================] - 0s 3ms/step - loss: 0.4180\nEpoch:  23\n 1/36 [..............................] - ETA: 0s - loss: 0.1505\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2297\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3695\n36/36 [==============================] - 0s 3ms/step - loss: 0.3876\nEpoch:  24\n 1/36 [..............................] - ETA: 0s - loss: 0.0478\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2641\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.3524\n36/36 [==============================] - 0s 4ms/step - loss: 0.3763\nEpoch:  25\n 1/36 [..............................] - ETA: 0s - loss: 0.0894\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2783\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3993\n36/36 [==============================] - 0s 3ms/step - loss: 0.4062\nEpoch:  26\n 1/36 [..............................] - ETA: 0s - loss: 0.0367\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1881\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3248\n36/36 [==============================] - 0s 4ms/step - loss: 0.3462\nEpoch:  27\n 1/36 [..............................] - ETA: 0s - loss: 0.0145\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1862\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3052\n36/36 [==============================] - 0s 3ms/step - loss: 0.3250\nEpoch:  28\n 1/36 [..............................] - ETA: 0s - loss: 0.0817\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2294\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3295\n36/36 [==============================] - 0s 3ms/step - loss: 0.3434\nEpoch:  29\n 1/36 [..............................] - ETA: 0s - loss: 0.0826\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3017\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3068\n36/36 [==============================] - 0s 3ms/step - loss: 0.3187\nEpoch:  30\n 1/36 [..............................] - ETA: 0s - loss: 0.0711\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1930\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.4104\n36/36 [==============================] - 0s 4ms/step - loss: 0.4402\nEpoch:  31\n 1/36 [..............................] - ETA: 0s - loss: 0.0767\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1753\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.3256\n36/36 [==============================] - 0s 4ms/step - loss: 0.3594\nEpoch:  32\n 1/36 [..............................] - ETA: 0s - loss: 0.0837\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1827\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2828\n36/36 [==============================] - 0s 3ms/step - loss: 0.3033\nEpoch:  33\n 1/36 [..............................] - ETA: 0s - loss: 0.0510\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2474\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3199\n36/36 [==============================] - 0s 3ms/step - loss: 0.3424\nEpoch:  34\n 1/36 [..............................] - ETA: 0s - loss: 0.0468\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1937\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.4436\n36/36 [==============================] - 0s 3ms/step - loss: 0.4448\nEpoch:  35\n 1/36 [..............................] - ETA: 0s - loss: 0.0494\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1585\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3010\n36/36 [==============================] - 0s 3ms/step - loss: 0.3154\nEpoch:  36\n 1/36 [..............................] - ETA: 0s - loss: 0.0161\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2359\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3025\n36/36 [==============================] - 0s 3ms/step - loss: 0.3217\nEpoch:  37\n 1/36 [..............................] - ETA: 0s - loss: 0.0493\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1486\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3525\n36/36 [==============================] - 0s 3ms/step - loss: 0.3656\nEpoch:  38\n 1/36 [..............................] - ETA: 0s - loss: 0.0474\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2141\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3205\n36/36 [==============================] - 0s 3ms/step - loss: 0.3370\nEpoch:  39\n 1/36 [..............................] - ETA: 0s - loss: 0.0948\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2909\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3675\n36/36 [==============================] - 0s 3ms/step - loss: 0.3881\nEpoch:  40\n 1/36 [..............................] - ETA: 0s - loss: 0.0078\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1668\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3506\n36/36 [==============================] - 0s 3ms/step - loss: 0.3645\nEpoch:  41\n 1/36 [..............................] - ETA: 0s - loss: 0.0494\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2053\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2934\n36/36 [==============================] - 0s 3ms/step - loss: 0.3140\nEpoch:  42\n 1/36 [..............................] - ETA: 0s - loss: 0.0443\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2114\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2626\n36/36 [==============================] - 0s 3ms/step - loss: 0.2887\nEpoch:  43\n 1/36 [..............................] - ETA: 0s - loss: 0.0025\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1334\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3064\n36/36 [==============================] - 0s 3ms/step - loss: 0.3291\nEpoch:  44\n 1/36 [..............................] - ETA: 0s - loss: 0.0159\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1999\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2743\n36/36 [==============================] - 0s 3ms/step - loss: 0.2980\nEpoch:  45\n 1/36 [..............................] - ETA: 0s - loss: 0.0955\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2496\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3432\n36/36 [==============================] - 0s 3ms/step - loss: 0.3563\nEpoch:  46\n 1/36 [..............................] - ETA: 0s - loss: 0.0217\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2108\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3665\n36/36 [==============================] - 0s 3ms/step - loss: 0.3771\nEpoch:  47\n 1/36 [..............................] - ETA: 0s - loss: 0.0277\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1914\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.3271\n36/36 [==============================] - 0s 4ms/step - loss: 0.3495\nEpoch:  48\n 1/36 [..............................] - ETA: 0s - loss: 0.0255\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1785\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3308\n36/36 [==============================] - 0s 3ms/step - loss: 0.3450\nEpoch:  49\n 1/36 [..............................] - ETA: 0s - loss: 0.0929\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2178\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3201\n36/36 [==============================] - 0s 3ms/step - loss: 0.3302\nEpoch:  50\n\n\n\n\nPredicting Using The LSTM Model\n\n# Make Predictions\npred_out &lt;- model %&gt;% \n    predict(x_test_arr, batch_size = batch_size) %&gt;%\n    .[,1] \n\n\n 1/12 [=&gt;............................] - ETA: 10s\n12/12 [==============================] - 1s 2ms/step\n\n# Retransform values\npred_tbl &lt;- tibble(\n    index   = lag_test_tbl$index,\n    value   = (pred_out * scale_history + center_history)^2\n) \n\n# Combine actual data with predictions\ntbl_1 &lt;- df_trn %&gt;%\n    add_column(key = \"actual\")\n\ntbl_2 &lt;- df_tst %&gt;%\n    add_column(key = \"actual\")\n\ntbl_3 &lt;- pred_tbl %&gt;%\n    add_column(key = \"predict\")\n\n# Create time_bind_rows() to solve dplyr issue\ntime_bind_rows &lt;- function(data_1, data_2, index) {\n    index_expr &lt;- enquo(index)\n    bind_rows(data_1, data_2) %&gt;%\n        as_tbl_time(index = !! index_expr)\n}\n\nret &lt;- list(tbl_1, tbl_2, tbl_3) %&gt;%\n    reduce(time_bind_rows, index = index) %&gt;%\n    arrange(key, index) %&gt;%\n    mutate(key = as_factor(key))\n\nret\n\n# A time tibble: 72 × 3\n# Index:         index\n   index      value key   \n   &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt; \n 1 2013-01-01  162. actual\n 2 2013-02-01  158. actual\n 3 2013-03-01  154. actual\n 4 2013-04-01  144. actual\n 5 2013-05-01  137. actual\n 6 2013-06-01  130. actual\n 7 2013-07-01  124. actual\n 8 2013-08-01  131. actual\n 9 2013-09-01  130. actual\n10 2013-10-01  127. actual\n# ℹ 62 more rows\n\n\n\n\nAssessing Performance Of The LSTM On A Single Split\n\ncalc_rmse &lt;- function(prediction_tbl) {\n    \n    rmse_calculation &lt;- function(data) {\n        data %&gt;%\n            spread(key = key, value = value) %&gt;%\n            select(-index) %&gt;%\n            filter(!is.na(predict)) %&gt;%\n            rename(\n                truth    = actual,\n                estimate = predict\n            ) %&gt;%\n            rmse(truth, estimate) %&gt;% \n      select(.estimate)\n    }\n    \n    safe_rmse &lt;- possibly(rmse_calculation, otherwise = NA)\n    \n    safe_rmse(prediction_tbl)\n        \n}\n\nverificando o RMSE\n\ncalc_rmse(ret)\n\n# A tibble: 1 × 1\n  .estimate\n      &lt;dbl&gt;\n1      2.88\n\n\n\n\nVisualizing The Single Prediction\n\n# Setup single plot function\nplot_prediction &lt;- function(data, id, alpha = 1, size = 2, base_size = 14) {\n    \n    rmse_val &lt;- calc_rmse(data)\n    \n    g &lt;- data %&gt;%\n        ggplot(aes(index, value, color = key)) +\n        geom_point(alpha = alpha, size = size) + \n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        theme(legend.position = \"none\") +\n        labs(\n            title = glue(\"{id}, RMSE: {round(rmse_val, digits = 1)}\"),\n            x = \"\", y = \"\"\n        )\n    \n    return(g)\n}\n\n\nret %&gt;% \n    plot_prediction(id = split_id, alpha = 0.65) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n# trend &lt;- fit %&gt;% \n#    filter(index &gt;=  \"2023-01-08\", index &lt;= \"2023-02-06\") %&gt;% \n#    select(trend) %&gt;% \n#    tibble()\n# \n# \n# predito &lt;- ret %&gt;% \n#   filter(key==\"predict\") %&gt;% \n#   mutate(\n#     value = value + trend\n#   )\n# \n# original &lt;- gold_original %&gt;% \n#   filter(index &gt;=  \"2023-01-08\", index &lt;= \"2023-02-06\")\n# \n# left_join(original, predito, by = \"index\") %&gt;% \n#   ggplot(aes(x=index))+\n#   geom_line(aes(y=value.x))+\n#   geom_line(aes(y=value.y$trend), color=\"blue\")\n\n\n\nBacktesting The LSTM On All Samples Creating An LSTM Prediction Function\n\npredict_keras_lstm &lt;- function(split, epochs = 50, ...) {\n    \n    lstm_prediction &lt;- function(split, epochs, ...) {\n        \n        # 5.1.2 Data Setup\n        df_trn &lt;- training(split)\n        df_tst &lt;- testing(split)\n        \n        df &lt;- bind_rows(\n            df_trn %&gt;% add_column(key = \"training\"),\n            df_tst %&gt;% add_column(key = \"testing\")\n        ) %&gt;% \n            as_tbl_time(index = index)\n        \n        # 5.1.3 Preprocessing\n        rec_obj &lt;- recipe(value ~ ., df) %&gt;%\n            step_sqrt(value) %&gt;%\n            step_center(value) %&gt;%\n            step_scale(value) %&gt;%\n            prep()\n        \n        df_processed_tbl &lt;- bake(rec_obj, df)\n        \n        center_history &lt;- rec_obj$steps[[2]]$means[\"value\"]\n        scale_history  &lt;- rec_obj$steps[[3]]$sds[\"value\"]\n        \n        # 5.1.4 LSTM Plan\n        lag_setting  &lt;- 12 # = nrow(df_tst)\n        batch_size   &lt;- 1\n        train_length &lt;- 12*5\n        tsteps       &lt;- 1\n        epochs       &lt;- epochs\n        \n        # 5.1.5 Train/Test Setup\n        lag_train_tbl &lt;- df_processed_tbl %&gt;%\n            mutate(value_lag = lag(value, n = lag_setting)) %&gt;%\n            filter(!is.na(value_lag)) %&gt;%\n            filter(key == \"training\") %&gt;%\n            tail(train_length)\n        \n        x_train_vec &lt;- lag_train_tbl$value_lag\n        x_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n        \n        y_train_vec &lt;- lag_train_tbl$value\n        y_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n        \n        lag_test_tbl &lt;- df_processed_tbl %&gt;%\n            mutate(\n                value_lag = lag(value, n = lag_setting)\n            ) %&gt;%\n            filter(!is.na(value_lag)) %&gt;%\n            filter(key == \"testing\")\n        \n        x_test_vec &lt;- lag_test_tbl$value_lag\n        x_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n        \n        y_test_vec &lt;- lag_test_tbl$value\n        y_test_arr &lt;- array(data = y_test_vec, dim = c(length(y_test_vec), 1))\n                \n        # 5.1.6 LSTM Model\n        model &lt;- keras_model_sequential()\n\n        model %&gt;%\n            layer_lstm(units            = 50, \n                       input_shape      = c(tsteps, 1), \n                       batch_size       = batch_size,\n                       return_sequences = TRUE, \n                       stateful         = TRUE) %&gt;% \n            layer_lstm(units            = 50, \n                       return_sequences = FALSE, \n                       stateful         = TRUE) %&gt;% \n            layer_dense(units = 1)\n        \n        model %&gt;% \n            compile(loss = 'mae', optimizer = 'adam')\n        \n        # 5.1.7 Fitting LSTM\n        for (i in 1:epochs) {\n            model %&gt;% fit(x          = x_train_arr, \n                          y          = y_train_arr, \n                          batch_size = batch_size,\n                          epochs     = 1, \n                          verbose    = 1, \n                          shuffle    = FALSE)\n            \n            model %&gt;% reset_states()\n            cat(\"Epoch: \", i)\n            \n        }\n        \n        # 5.1.8 Predict and Return Tidy Data\n        # Make Predictions\n        pred_out &lt;- model %&gt;% \n            predict(x_test_arr, batch_size = batch_size) %&gt;%\n            .[,1] \n        \n        # Retransform values\n        pred_tbl &lt;- tibble(\n            index   = lag_test_tbl$index,\n            value   = (pred_out * scale_history + center_history)^2\n        ) \n        \n        # Combine actual data with predictions\n        tbl_1 &lt;- df_trn %&gt;%\n            add_column(key = \"actual\")\n        \n        tbl_2 &lt;- df_tst %&gt;%\n            add_column(key = \"actual\")\n        \n        tbl_3 &lt;- pred_tbl %&gt;%\n            add_column(key = \"predict\")\n        \n        # Create time_bind_rows() to solve dplyr issue\n        time_bind_rows &lt;- function(data_1, data_2, index) {\n            index_expr &lt;- enquo(index)\n            bind_rows(data_1, data_2) %&gt;%\n                as_tbl_time(index = !! index_expr)\n        }\n        \n        ret &lt;- list(tbl_1, tbl_2, tbl_3) %&gt;%\n            reduce(time_bind_rows, index = index) %&gt;%\n            arrange(key, index) %&gt;%\n            mutate(key = as_factor(key))\n\n        return(ret)\n        \n    }\n    \n    safe_lstm &lt;- possibly(lstm_prediction, otherwise = NA)\n    \n    safe_lstm(split, epochs, ...)\n    \n}\n\ntestando\n\npredict_keras_lstm(split, epochs = 10)\n\n\n 1/36 [..............................] - ETA: 2:05 - loss: 0.1131\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.5612  \n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7333\n36/36 [==============================] - 4s 3ms/step - loss: 0.7037\nEpoch:  1\n 1/36 [..............................] - ETA: 0s - loss: 0.1400\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3220\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.5406\n36/36 [==============================] - 0s 4ms/step - loss: 0.5454\nEpoch:  2\n 1/36 [..............................] - ETA: 0s - loss: 0.1209\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2583\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.5068\n36/36 [==============================] - 0s 3ms/step - loss: 0.5182\nEpoch:  3\n 1/36 [..............................] - ETA: 0s - loss: 0.1461\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2193\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.3845\n36/36 [==============================] - 0s 3ms/step - loss: 0.4089\nEpoch:  4\n 1/36 [..............................] - ETA: 0s - loss: 0.0806\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3829\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.5230\n36/36 [==============================] - 0s 3ms/step - loss: 0.5190\nEpoch:  5\n 1/36 [..............................] - ETA: 0s - loss: 0.1778\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3816\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.5146\n36/36 [==============================] - 0s 4ms/step - loss: 0.5149\nEpoch:  6\n 1/36 [..............................] - ETA: 0s - loss: 0.0640\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.4098\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.4974\n36/36 [==============================] - 0s 4ms/step - loss: 0.5160\nEpoch:  7\n 1/36 [..............................] - ETA: 0s - loss: 0.1149\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2257\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.4316\n36/36 [==============================] - 0s 4ms/step - loss: 0.4561\nEpoch:  8\n 1/36 [..............................] - ETA: 0s - loss: 0.0993\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1982\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.4050\n36/36 [==============================] - 0s 3ms/step - loss: 0.4216\nEpoch:  9\n 1/36 [..............................] - ETA: 0s - loss: 0.1228\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2277\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.3579\n36/36 [==============================] - 0s 4ms/step - loss: 0.3944\nEpoch:  10\n 1/12 [=&gt;............................] - ETA: 11s\n12/12 [==============================] - 1s 2ms/step\n\n\n# A time tibble: 72 × 3\n# Index:         index\n   index      value key   \n   &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt; \n 1 2013-01-01  162. actual\n 2 2013-02-01  158. actual\n 3 2013-03-01  154. actual\n 4 2013-04-01  144. actual\n 5 2013-05-01  137. actual\n 6 2013-06-01  130. actual\n 7 2013-07-01  124. actual\n 8 2013-08-01  131. actual\n 9 2013-09-01  130. actual\n10 2013-10-01  127. actual\n# ℹ 62 more rows\n\n\n\n\nMapping The LSTM Prediction Function Over The Samples\n\nsample_predictions_lstm_tbl &lt;- rolling_origin_resamples %&gt;%\n     mutate(predict = map(splits, predict_keras_lstm, epochs = 50))\n\n\n 1/36 [..............................] - ETA: 2:22 - loss: 0.1293\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.5531  \n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.7498\n36/36 [==============================] - 4s 3ms/step - loss: 0.7178\nEpoch:  1\n 1/36 [..............................] - ETA: 0s - loss: 0.1395\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3604\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.5733\n36/36 [==============================] - 0s 3ms/step - loss: 0.5627\nEpoch:  2\n 1/36 [..............................] - ETA: 0s - loss: 0.1365\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2893\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.5449\n36/36 [==============================] - 0s 3ms/step - loss: 0.5338\nEpoch:  3\n 1/36 [..............................] - ETA: 0s - loss: 0.1493\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2044\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.3803\n36/36 [==============================] - 0s 4ms/step - loss: 0.4155\nEpoch:  4\n 1/36 [..............................] - ETA: 0s - loss: 0.0819\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.4520\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.5468\n36/36 [==============================] - 0s 4ms/step - loss: 0.5453\nEpoch:  5\n 1/36 [..............................] - ETA: 0s - loss: 0.1864\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.3830\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.6012\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.5206\n36/36 [==============================] - 0s 5ms/step - loss: 0.5335\nEpoch:  6\n 1/36 [..............................] - ETA: 0s - loss: 0.0905\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.3459\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.5613\n36/36 [==============================] - 0s 4ms/step - loss: 0.5248\nEpoch:  7\n 1/36 [..............................] - ETA: 0s - loss: 0.1253\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.2312\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.4667\n36/36 [==============================] - 0s 4ms/step - loss: 0.5013\nEpoch:  8\n 1/36 [..............................] - ETA: 0s - loss: 0.1580\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2496\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.4226\n36/36 [==============================] - 0s 4ms/step - loss: 0.4409\nEpoch:  9\n 1/36 [..............................] - ETA: 0s - loss: 0.0905\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.2942\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.4302\n36/36 [==============================] - 0s 4ms/step - loss: 0.4601\nEpoch:  10\n 1/36 [..............................] - ETA: 0s - loss: 0.1805\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.3993\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.4885\n36/36 [==============================] - 0s 4ms/step - loss: 0.4703\nEpoch:  11\n 1/36 [..............................] - ETA: 0s - loss: 0.0420\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.4062\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.5588\n36/36 [==============================] - 0s 4ms/step - loss: 0.5201\nEpoch:  12\n 1/36 [..............................] - ETA: 0s - loss: 0.0746\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.2865\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.4345\n36/36 [==============================] - 0s 4ms/step - loss: 0.4921\nEpoch:  13\n 1/36 [..............................] - ETA: 0s - loss: 0.1050\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.2147\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.3511\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3971\n36/36 [==============================] - 0s 5ms/step - loss: 0.4144\nEpoch:  14\n 1/36 [..............................] - ETA: 0s - loss: 0.0978\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1673\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.2998\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3732\n36/36 [==============================] - 0s 5ms/step - loss: 0.3942\nEpoch:  15\n 1/36 [..............................] - ETA: 0s - loss: 0.1381\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.2469\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.3806\n36/36 [==============================] - 0s 4ms/step - loss: 0.4020\nEpoch:  16\n 1/36 [..............................] - ETA: 0s - loss: 0.0988\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2573\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3877\n36/36 [==============================] - 0s 3ms/step - loss: 0.4110\nEpoch:  17\n 1/36 [..............................] - ETA: 0s - loss: 0.1783\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2975\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.4049\n36/36 [==============================] - 0s 4ms/step - loss: 0.4429\nEpoch:  18\n 1/36 [..............................] - ETA: 0s - loss: 0.1018\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.3248\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.4614\n36/36 [==============================] - 0s 4ms/step - loss: 0.4601\nEpoch:  19\n 1/36 [..............................] - ETA: 0s - loss: 0.1772\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.2426\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.3944\n36/36 [==============================] - ETA: 0s - loss: 0.4227\n36/36 [==============================] - 0s 5ms/step - loss: 0.4227\nEpoch:  20\n 1/36 [..............................] - ETA: 0s - loss: 0.1457\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.1917\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.3422\n36/36 [==============================] - 0s 5ms/step - loss: 0.3707\nEpoch:  21\n 1/36 [..............................] - ETA: 0s - loss: 0.1353\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.2007\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.3341\n36/36 [==============================] - 0s 4ms/step - loss: 0.3983\nEpoch:  22\n 1/36 [..............................] - ETA: 0s - loss: 0.1229\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1489\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.2384\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2841\n36/36 [==============================] - 0s 5ms/step - loss: 0.3214\nEpoch:  23\n 1/36 [..............................] - ETA: 0s - loss: 0.1348\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.1794\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.3703\n36/36 [==============================] - 0s 4ms/step - loss: 0.4169\nEpoch:  24\n 1/36 [..............................] - ETA: 0s - loss: 0.1636\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1821\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2690\n36/36 [==============================] - 0s 4ms/step - loss: 0.3023\nEpoch:  25\n 1/36 [..............................] - ETA: 0s - loss: 0.0698\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.2095\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.3802\n36/36 [==============================] - 0s 4ms/step - loss: 0.3790\nEpoch:  26\n 1/36 [..............................] - ETA: 0s - loss: 0.1537\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2179\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.4200\n36/36 [==============================] - 0s 4ms/step - loss: 0.4491\nEpoch:  27\n 1/36 [..............................] - ETA: 0s - loss: 0.2531\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.3155\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.4601\n36/36 [==============================] - 0s 4ms/step - loss: 0.4531\nEpoch:  28\n 1/36 [..............................] - ETA: 0s - loss: 0.0740\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.2099\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.2642\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3904\n36/36 [==============================] - 0s 5ms/step - loss: 0.4086\nEpoch:  29\n 1/36 [..............................] - ETA: 0s - loss: 0.1113\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.2349\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.4397\n36/36 [==============================] - 0s 4ms/step - loss: 0.4259\nEpoch:  30\n 1/36 [..............................] - ETA: 0s - loss: 0.1075\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.1785\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.3077\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3377\n36/36 [==============================] - 0s 5ms/step - loss: 0.3546\nEpoch:  31\n 1/36 [..............................] - ETA: 0s - loss: 0.1164\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2058\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.3337\n36/36 [==============================] - 0s 4ms/step - loss: 0.4077\nEpoch:  32\n 1/36 [..............................] - ETA: 0s - loss: 0.1603\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2179\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.3982\n36/36 [==============================] - 0s 4ms/step - loss: 0.4160\nEpoch:  33\n 1/36 [..............................] - ETA: 0s - loss: 0.0897\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1814\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.2648\n36/36 [==============================] - 0s 4ms/step - loss: 0.3482\nEpoch:  34\n 1/36 [..............................] - ETA: 0s - loss: 0.1472\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.2773\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.4083\n36/36 [==============================] - 0s 4ms/step - loss: 0.4211\nEpoch:  35\n 1/36 [..............................] - ETA: 0s - loss: 0.0039\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2432\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3889\n36/36 [==============================] - 0s 3ms/step - loss: 0.4053\nEpoch:  36\n 1/36 [..............................] - ETA: 0s - loss: 0.0608\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2032\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.3470\n36/36 [==============================] - 0s 4ms/step - loss: 0.3793\nEpoch:  37\n 1/36 [..............................] - ETA: 0s - loss: 0.0601\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2094\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3602\n36/36 [==============================] - 0s 3ms/step - loss: 0.3726\nEpoch:  38\n 1/36 [..............................] - ETA: 0s - loss: 0.0647\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1630\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2993\n36/36 [==============================] - 0s 3ms/step - loss: 0.3093\nEpoch:  39\n 1/36 [..............................] - ETA: 0s - loss: 0.1166\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.2701\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.4348\n36/36 [==============================] - 0s 3ms/step - loss: 0.4350\nEpoch:  40\n 1/36 [..............................] - ETA: 0s - loss: 0.0306\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2182\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3266\n36/36 [==============================] - 0s 3ms/step - loss: 0.3439\nEpoch:  41\n 1/36 [..............................] - ETA: 0s - loss: 0.0733\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1752\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.5937\n36/36 [==============================] - 0s 3ms/step - loss: 0.6043\nEpoch:  42\n 1/36 [..............................] - ETA: 0s - loss: 0.0153\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1818\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3982\n36/36 [==============================] - 0s 3ms/step - loss: 0.4003\nEpoch:  43\n 1/36 [..............................] - ETA: 0s - loss: 0.0708\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1865\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3292\n36/36 [==============================] - 0s 3ms/step - loss: 0.3453\nEpoch:  44\n 1/36 [..............................] - ETA: 0s - loss: 0.0681\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1888\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3057\n36/36 [==============================] - 0s 3ms/step - loss: 0.3170\nEpoch:  45\n 1/36 [..............................] - ETA: 0s - loss: 0.0065\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1320\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3746\n36/36 [==============================] - 0s 3ms/step - loss: 0.3792\nEpoch:  46\n 1/36 [..............................] - ETA: 0s - loss: 0.0386\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1704\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2923\n36/36 [==============================] - 0s 3ms/step - loss: 0.3118\nEpoch:  47\n 1/36 [..............................] - ETA: 0s - loss: 0.0730\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1829\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2795\n36/36 [==============================] - 0s 3ms/step - loss: 0.3002\nEpoch:  48\n 1/36 [..............................] - ETA: 0s - loss: 0.0295\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1935\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3884\n36/36 [==============================] - 0s 3ms/step - loss: 0.3936\nEpoch:  49\n 1/36 [..............................] - ETA: 0s - loss: 0.0079\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1416\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2533\n36/36 [==============================] - 0s 3ms/step - loss: 0.2715\nEpoch:  50\n 1/12 [=&gt;............................] - ETA: 11s\n12/12 [==============================] - 1s 2ms/step\n\n 1/36 [..............................] - ETA: 2:06 - loss: 0.6188\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3858  \n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.4170\n36/36 [==============================] - 4s 3ms/step - loss: 0.4048\nEpoch:  1\n 1/36 [..............................] - ETA: 0s - loss: 0.6185\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3626\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3996\n36/36 [==============================] - 0s 3ms/step - loss: 0.3883\nEpoch:  2\n 1/36 [..............................] - ETA: 0s - loss: 0.6241\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3455\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3917\n36/36 [==============================] - 0s 3ms/step - loss: 0.3807\nEpoch:  3\n 1/36 [..............................] - ETA: 0s - loss: 0.6247\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3386\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3856\n36/36 [==============================] - 0s 3ms/step - loss: 0.3764\nEpoch:  4\n 1/36 [..............................] - ETA: 0s - loss: 0.6375\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3382\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3738\n36/36 [==============================] - 0s 3ms/step - loss: 0.3637\nEpoch:  5\n 1/36 [..............................] - ETA: 0s - loss: 0.6320\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3643\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3803\n36/36 [==============================] - 0s 3ms/step - loss: 0.3706\nEpoch:  6\n 1/36 [..............................] - ETA: 0s - loss: 0.6491\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.4066\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.4022\n36/36 [==============================] - 0s 3ms/step - loss: 0.3961\nEpoch:  7\n 1/36 [..............................] - ETA: 0s - loss: 0.6340\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3658\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3867\n36/36 [==============================] - 0s 3ms/step - loss: 0.3815\nEpoch:  8\n 1/36 [..............................] - ETA: 0s - loss: 0.6268\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3415\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3838\n36/36 [==============================] - 0s 3ms/step - loss: 0.3762\nEpoch:  9\n 1/36 [..............................] - ETA: 0s - loss: 0.6257\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3358\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3776\n36/36 [==============================] - 0s 3ms/step - loss: 0.3689\nEpoch:  10\n 1/36 [..............................] - ETA: 0s - loss: 0.6385\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3400\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3746\n36/36 [==============================] - 0s 3ms/step - loss: 0.3645\nEpoch:  11\n 1/36 [..............................] - ETA: 0s - loss: 0.6335\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3401\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3611\n36/36 [==============================] - 0s 3ms/step - loss: 0.3600\nEpoch:  12\n 1/36 [..............................] - ETA: 0s - loss: 0.6416\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3601\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3831\n36/36 [==============================] - 0s 3ms/step - loss: 0.3728\nEpoch:  13\n 1/36 [..............................] - ETA: 0s - loss: 0.6343\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3318\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3723\n36/36 [==============================] - 0s 3ms/step - loss: 0.3633\nEpoch:  14\n 1/36 [..............................] - ETA: 0s - loss: 0.6394\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.3762\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.3093\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3670\n36/36 [==============================] - 0s 5ms/step - loss: 0.3587\nEpoch:  15\n 1/36 [..............................] - ETA: 0s - loss: 0.6355\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3333\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3653\n36/36 [==============================] - 0s 3ms/step - loss: 0.3579\nEpoch:  16\n 1/36 [..............................] - ETA: 0s - loss: 0.6293\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3378\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3715\n36/36 [==============================] - 0s 3ms/step - loss: 0.3617\nEpoch:  17\n 1/36 [..............................] - ETA: 0s - loss: 0.6265\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3566\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3735\n36/36 [==============================] - 0s 3ms/step - loss: 0.3643\nEpoch:  18\n 1/36 [..............................] - ETA: 0s - loss: 0.6431\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3983\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3995\n36/36 [==============================] - 0s 3ms/step - loss: 0.3929\nEpoch:  19\n 1/36 [..............................] - ETA: 0s - loss: 0.6293\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3614\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3818\n36/36 [==============================] - 0s 3ms/step - loss: 0.3768\nEpoch:  20\n 1/36 [..............................] - ETA: 0s - loss: 0.6226\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3382\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3735\n36/36 [==============================] - 0s 3ms/step - loss: 0.3660\nEpoch:  21\n 1/36 [..............................] - ETA: 0s - loss: 0.6279\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3322\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3605\n36/36 [==============================] - 0s 3ms/step - loss: 0.3541\nEpoch:  22\n 1/36 [..............................] - ETA: 0s - loss: 0.6360\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3701\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3805\n36/36 [==============================] - 0s 3ms/step - loss: 0.3764\nEpoch:  23\n 1/36 [..............................] - ETA: 0s - loss: 0.6298\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3452\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3727\n36/36 [==============================] - 0s 3ms/step - loss: 0.3630\nEpoch:  24\n 1/36 [..............................] - ETA: 0s - loss: 0.6233\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3380\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3556\n36/36 [==============================] - 0s 3ms/step - loss: 0.3541\nEpoch:  25\n 1/36 [..............................] - ETA: 0s - loss: 0.6364\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3893\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3744\n36/36 [==============================] - 0s 3ms/step - loss: 0.3778\nEpoch:  26\n 1/36 [..............................] - ETA: 0s - loss: 0.6303\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3694\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.3277\n36/36 [==============================] - 0s 4ms/step - loss: 0.3614\nEpoch:  27\n 1/36 [..............................] - ETA: 0s - loss: 0.6192\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3387\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3561\n36/36 [==============================] - 0s 3ms/step - loss: 0.3544\nEpoch:  28\n 1/36 [..............................] - ETA: 0s - loss: 0.6301\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3662\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3726\n36/36 [==============================] - 0s 3ms/step - loss: 0.3685\nEpoch:  29\n 1/36 [..............................] - ETA: 0s - loss: 0.6195\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3359\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3557\n36/36 [==============================] - 0s 3ms/step - loss: 0.3523\nEpoch:  30\n 1/36 [..............................] - ETA: 0s - loss: 0.6023\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3679\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3727\n36/36 [==============================] - 0s 3ms/step - loss: 0.3639\nEpoch:  31\n 1/36 [..............................] - ETA: 0s - loss: 0.6276\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3824\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.4106\n36/36 [==============================] - 0s 3ms/step - loss: 0.4013\nEpoch:  32\n 1/36 [..............................] - ETA: 0s - loss: 0.6290\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3631\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3723\n36/36 [==============================] - 0s 3ms/step - loss: 0.3680\nEpoch:  33\n 1/36 [..............................] - ETA: 0s - loss: 0.6248\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3520\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3390\n36/36 [==============================] - 0s 3ms/step - loss: 0.3468\nEpoch:  34\n 1/36 [..............................] - ETA: 0s - loss: 0.6164\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3399\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3477\n36/36 [==============================] - 0s 3ms/step - loss: 0.3458\nEpoch:  35\n 1/36 [..............................] - ETA: 0s - loss: 0.6088\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3394\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3528\n36/36 [==============================] - 0s 3ms/step - loss: 0.3481\nEpoch:  36\n 1/36 [..............................] - ETA: 0s - loss: 0.6127\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3340\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3365\n36/36 [==============================] - 0s 3ms/step - loss: 0.3381\nEpoch:  37\n 1/36 [..............................] - ETA: 0s - loss: 0.6093\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3439\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3339\n36/36 [==============================] - 0s 3ms/step - loss: 0.3408\nEpoch:  38\n 1/36 [..............................] - ETA: 0s - loss: 0.6049\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3419\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3660\n36/36 [==============================] - 0s 3ms/step - loss: 0.3617\nEpoch:  39\n 1/36 [..............................] - ETA: 0s - loss: 0.6303\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.5211\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.5619\n36/36 [==============================] - 0s 3ms/step - loss: 0.5422\nEpoch:  40\n 1/36 [..............................] - ETA: 0s - loss: 0.6195\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3507\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3773\n36/36 [==============================] - 0s 3ms/step - loss: 0.3724\nEpoch:  41\n 1/36 [..............................] - ETA: 0s - loss: 0.6189\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3245\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3617\n36/36 [==============================] - 0s 3ms/step - loss: 0.3559\nEpoch:  42\n 1/36 [..............................] - ETA: 0s - loss: 0.6236\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3264\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3468\n36/36 [==============================] - 0s 3ms/step - loss: 0.3463\nEpoch:  43\n 1/36 [..............................] - ETA: 0s - loss: 0.6221\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3474\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3561\n36/36 [==============================] - 0s 3ms/step - loss: 0.3525\nEpoch:  44\n 1/36 [..............................] - ETA: 0s - loss: 0.6055\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3675\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3526\n36/36 [==============================] - 0s 3ms/step - loss: 0.3517\nEpoch:  45\n 1/36 [..............................] - ETA: 0s - loss: 0.6062\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.3726\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.4219\n36/36 [==============================] - 0s 3ms/step - loss: 0.4105\nEpoch:  46\n 1/36 [..............................] - ETA: 0s - loss: 0.6075\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3774\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.4017\n36/36 [==============================] - 0s 3ms/step - loss: 0.3932\nEpoch:  47\n 1/36 [..............................] - ETA: 0s - loss: 0.6094\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3702\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3973\n36/36 [==============================] - 0s 3ms/step - loss: 0.3871\nEpoch:  48\n 1/36 [..............................] - ETA: 0s - loss: 0.6098\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3614\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3872\n36/36 [==============================] - 0s 3ms/step - loss: 0.3810\nEpoch:  49\n 1/36 [..............................] - ETA: 0s - loss: 0.6099\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3486\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3782\n36/36 [==============================] - 0s 3ms/step - loss: 0.3732\nEpoch:  50\n 1/12 [=&gt;............................] - ETA: 8s\n12/12 [==============================] - 1s 2ms/step\n\n 1/36 [..............................] - ETA: 1:58 - loss: 0.7248\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6026  \n34/36 [===========================&gt;..] - ETA: 0s - loss: 1.0176\n36/36 [==============================] - 4s 3ms/step - loss: 1.0300\nEpoch:  1\n 1/36 [..............................] - ETA: 0s - loss: 0.7135\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.8785\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.8507\n36/36 [==============================] - 0s 3ms/step - loss: 0.8692\nEpoch:  2\n 1/36 [..............................] - ETA: 0s - loss: 0.7138\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.9735\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.8140\n36/36 [==============================] - 0s 3ms/step - loss: 0.8614\nEpoch:  3\n 1/36 [..............................] - ETA: 0s - loss: 0.7084\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.9619\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.6935\n36/36 [==============================] - 0s 4ms/step - loss: 0.8500\nEpoch:  4\n 1/36 [..............................] - ETA: 0s - loss: 0.7107\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9220\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.8185\n36/36 [==============================] - 0s 3ms/step - loss: 0.8482\nEpoch:  5\n 1/36 [..............................] - ETA: 0s - loss: 0.7058\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9036\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.8152\n36/36 [==============================] - 0s 3ms/step - loss: 0.8435\nEpoch:  6\n 1/36 [..............................] - ETA: 0s - loss: 0.7069\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.9577\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.8002\n36/36 [==============================] - 0s 3ms/step - loss: 0.8415\nEpoch:  7\n 1/36 [..............................] - ETA: 0s - loss: 0.7002\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.9350\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7999\n36/36 [==============================] - 0s 3ms/step - loss: 0.8382\nEpoch:  8\n 1/36 [..............................] - ETA: 0s - loss: 0.6996\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9149\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.8141\n36/36 [==============================] - 0s 3ms/step - loss: 0.8284\nEpoch:  9\n 1/36 [..............................] - ETA: 0s - loss: 0.7002\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9266\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7885\n36/36 [==============================] - 0s 3ms/step - loss: 0.8227\nEpoch:  10\n 1/36 [..............................] - ETA: 0s - loss: 0.6990\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9298\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7989\n36/36 [==============================] - 0s 3ms/step - loss: 0.8181\nEpoch:  11\n 1/36 [..............................] - ETA: 0s - loss: 0.6958\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9340\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.8016\n36/36 [==============================] - 0s 3ms/step - loss: 0.8080\nEpoch:  12\n 1/36 [..............................] - ETA: 0s - loss: 0.6899\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9287\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.8012\n36/36 [==============================] - 0s 3ms/step - loss: 0.7972\nEpoch:  13\n 1/36 [..............................] - ETA: 0s - loss: 0.6799\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9371\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7793\n36/36 [==============================] - 0s 3ms/step - loss: 0.7380\nEpoch:  14\n 1/36 [..............................] - ETA: 0s - loss: 0.6619\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9590\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7833\n36/36 [==============================] - 0s 3ms/step - loss: 0.7798\nEpoch:  15\n 1/36 [..............................] - ETA: 0s - loss: 0.6395\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9803\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7849\n36/36 [==============================] - 0s 3ms/step - loss: 0.7844\nEpoch:  16\n 1/36 [..............................] - ETA: 0s - loss: 0.6263\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.9273\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7825\n36/36 [==============================] - 0s 3ms/step - loss: 0.7279\nEpoch:  17\n 1/36 [..............................] - ETA: 0s - loss: 0.6026\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.8639\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7567\n36/36 [==============================] - 0s 3ms/step - loss: 0.7677\nEpoch:  18\n 1/36 [..............................] - ETA: 0s - loss: 0.5775\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.8687\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7767\n36/36 [==============================] - 0s 3ms/step - loss: 0.7924\nEpoch:  19\n 1/36 [..............................] - ETA: 0s - loss: 0.5711\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.8327\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7646\n36/36 [==============================] - 0s 3ms/step - loss: 0.7532\nEpoch:  20\n 1/36 [..............................] - ETA: 0s - loss: 0.5494\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7507\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7516\n36/36 [==============================] - 0s 3ms/step - loss: 0.7359\nEpoch:  21\n 1/36 [..............................] - ETA: 0s - loss: 0.5182\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.7901\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7391\n36/36 [==============================] - 0s 3ms/step - loss: 0.7257\nEpoch:  22\n 1/36 [..............................] - ETA: 0s - loss: 0.5004\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6979\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7164\n36/36 [==============================] - 0s 3ms/step - loss: 0.6983\nEpoch:  23\n 1/36 [..............................] - ETA: 0s - loss: 0.4885\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7953\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7302\n36/36 [==============================] - 0s 3ms/step - loss: 0.7581\nEpoch:  24\n 1/36 [..............................] - ETA: 0s - loss: 0.4890\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7794\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.7172\n36/36 [==============================] - 0s 3ms/step - loss: 0.7192\nEpoch:  25\n 1/36 [..............................] - ETA: 0s - loss: 0.4568\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7196\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.6748\n36/36 [==============================] - 0s 3ms/step - loss: 0.6194\nEpoch:  26\n 1/36 [..............................] - ETA: 0s - loss: 0.4293\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7296\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.6640\n36/36 [==============================] - 0s 3ms/step - loss: 0.6451\nEpoch:  27\n 1/36 [..............................] - ETA: 0s - loss: 0.4037\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7974\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.6785\n36/36 [==============================] - 0s 3ms/step - loss: 0.6721\nEpoch:  28\n 1/36 [..............................] - ETA: 0s - loss: 0.3729\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7555\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.5941\n36/36 [==============================] - 0s 3ms/step - loss: 0.5559\nEpoch:  29\n 1/36 [..............................] - ETA: 0s - loss: 0.3213\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.8689\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.5752\n36/36 [==============================] - 0s 3ms/step - loss: 0.5554\nEpoch:  30\n 1/36 [..............................] - ETA: 0s - loss: 0.2320\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.5754\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.7658\n36/36 [==============================] - 0s 3ms/step - loss: 0.7406\nEpoch:  31\n 1/36 [..............................] - ETA: 0s - loss: 0.2426\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.8147\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.6970\n36/36 [==============================] - 0s 3ms/step - loss: 0.6872\nEpoch:  32\n 1/36 [..............................] - ETA: 0s - loss: 0.1988\n17/36 [=============&gt;................] - ETA: 0s - loss: 1.0346\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.8468\n36/36 [==============================] - 0s 3ms/step - loss: 0.8704\nEpoch:  33\n 1/36 [..............................] - ETA: 0s - loss: 0.1989\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.7587\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.8244\n36/36 [==============================] - 0s 3ms/step - loss: 0.9096\nEpoch:  34\n 1/36 [..............................] - ETA: 0s - loss: 0.1415\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3925\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7331\n36/36 [==============================] - 0s 3ms/step - loss: 0.7041\nEpoch:  35\n 1/36 [..............................] - ETA: 0s - loss: 0.2047\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6671\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.9414\n36/36 [==============================] - 0s 3ms/step - loss: 0.9982\nEpoch:  36\n 1/36 [..............................] - ETA: 0s - loss: 0.2246\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6411\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.7844\n36/36 [==============================] - 0s 3ms/step - loss: 0.8466\nEpoch:  37\n 1/36 [..............................] - ETA: 0s - loss: 0.2275\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.6330\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.8217\n36/36 [==============================] - 0s 3ms/step - loss: 0.8370\nEpoch:  38\n 1/36 [..............................] - ETA: 0s - loss: 0.2047\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6594\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7459\n36/36 [==============================] - 0s 3ms/step - loss: 0.8321\nEpoch:  39\n 1/36 [..............................] - ETA: 0s - loss: 0.1733\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6443\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.6580\n36/36 [==============================] - 0s 3ms/step - loss: 0.6522\nEpoch:  40\n 1/36 [..............................] - ETA: 0s - loss: 0.1142\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.7031\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7977\n36/36 [==============================] - 0s 3ms/step - loss: 0.8732\nEpoch:  41\n 1/36 [..............................] - ETA: 0s - loss: 0.0621\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6803\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.7751\n36/36 [==============================] - 0s 3ms/step - loss: 0.8093\nEpoch:  42\n 1/36 [..............................] - ETA: 0s - loss: 0.0195\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.6962\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7188\n36/36 [==============================] - 0s 3ms/step - loss: 0.7990\nEpoch:  43\n 1/36 [..............................] - ETA: 0s - loss: 0.0544\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.5680\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.8015\n36/36 [==============================] - 0s 3ms/step - loss: 0.7780\nEpoch:  44\n 1/36 [..............................] - ETA: 0s - loss: 0.0039\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.5144\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.7611\n36/36 [==============================] - 0s 3ms/step - loss: 0.7174\nEpoch:  45\n 1/36 [..............................] - ETA: 0s - loss: 0.1372\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.5956\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.7304\n36/36 [==============================] - 0s 3ms/step - loss: 0.7017\nEpoch:  46\n 1/36 [..............................] - ETA: 0s - loss: 0.1300\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.7374\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.6977\n36/36 [==============================] - 0s 4ms/step - loss: 0.6134\nEpoch:  47\n 1/36 [..............................] - ETA: 0s - loss: 0.0946\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.6078\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.6501\n36/36 [==============================] - 0s 3ms/step - loss: 0.6575\nEpoch:  48\n 1/36 [..............................] - ETA: 0s - loss: 0.0373\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6315\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.6516\n36/36 [==============================] - 0s 3ms/step - loss: 0.6352\nEpoch:  49\n 1/36 [..............................] - ETA: 0s - loss: 0.0461\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.5675\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.6293\n36/36 [==============================] - 0s 3ms/step - loss: 0.5749\nEpoch:  50\n 1/12 [=&gt;............................] - ETA: 8s\n12/12 [==============================] - 1s 2ms/step\n\n\n\nsample_predictions_lstm_tbl\n\n# Rolling origin forecast resampling \n# A tibble: 3 × 3\n  splits          id     predict            \n  &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;             \n1 &lt;split [48/12]&gt; Slice1 &lt;tbl_time [72 × 3]&gt;\n2 &lt;split [48/12]&gt; Slice2 &lt;tbl_time [72 × 3]&gt;\n3 &lt;split [48/12]&gt; Slice3 &lt;tbl_time [72 × 3]&gt;\n\n\n\n\nAssessing The Backtested Performance\n\nsample_rmse_tbl &lt;- sample_predictions_lstm_tbl %&gt;%\n    mutate(rmse = map_df(predict, calc_rmse)) %&gt;%\n    select(id, rmse) \n\nsample_rmse_tbl\n\n# A tibble: 3 × 2\n  id     rmse$.estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Slice1           2.60\n2 Slice2          15.6 \n3 Slice3          24.9 \n\n\n\nsample_rmse_tbl %&gt;%\n    ggplot(aes(rmse$.estimate)) +\n    geom_histogram(aes(y = ..density..), fill = palette_light()[[1]], bins = 16) +\n    geom_density(fill = palette_light()[[1]], alpha = 0.5) +\n    theme_tq() +\n    ggtitle(\"Histogram of RMSE\")\n\n\n\n\n\nsample_rmse_tbl$rmse %&gt;%\n    summarize(\n        mean_rmse = mean(.estimate),\n        sd_rmse   = sd(.estimate)\n    )\n\n# A tibble: 1 × 2\n  mean_rmse sd_rmse\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      14.4    11.2\n\n\nVisualizing The Backtest Results\n\nplot_predictions &lt;- function(sampling_tbl, predictions_col, \n                             ncol = 3, alpha = 1, size = 2, base_size = 14,\n                             title = \"Backtested Predictions\") {\n    \n    predictions_col_expr &lt;- enquo(predictions_col)\n    \n    # Map plot_split() to sampling_tbl\n    sampling_tbl_with_plots &lt;- sampling_tbl %&gt;%\n        mutate(gg_plots = map2(!! predictions_col_expr, id, \n                               .f        = plot_prediction, \n                               alpha     = alpha, \n                               size      = size, \n                               base_size = base_size)) \n    \n    # Make plots with cowplot\n    plot_list &lt;- sampling_tbl_with_plots$gg_plots \n    \n    p_temp &lt;- plot_list[[1]] + theme(legend.position = \"bottom\")\n    legend &lt;- get_legend(p_temp)\n    \n    p_body  &lt;- plot_grid(plotlist = plot_list, ncol = ncol)\n    \n    \n    \n    p_title &lt;- ggdraw() + \n        draw_label(title, size = 18, fontface = \"bold\", colour = palette_light()[[1]])\n    \n    g &lt;- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n    \n    return(g)\n    \n}\n\n\nsample_predictions_lstm_tbl %&gt;%\n    plot_predictions(predictions_col = predict, alpha = 0.5, size = 1, base_size = 10,\n                     title = \"Keras Stateful LSTM: Backtested Predictions\")\n\n\n\n\n\n\nPredicting The Next 10 Years\n\npredict_keras_lstm_future &lt;- function(data, epochs = 300, ...) {\n    \n    lstm_prediction &lt;- function(data, epochs, ...) {\n        \n        # 5.1.2 Data Setup (MODIFIED)\n        df &lt;- data\n        \n        # 5.1.3 Preprocessing\n        rec_obj &lt;- recipe(value ~ ., df) %&gt;%\n            step_sqrt(value) %&gt;%\n            step_center(value) %&gt;%\n            step_scale(value) %&gt;%\n            prep()\n        \n        df_processed_tbl &lt;- bake(rec_obj, df)\n        \n        center_history &lt;- rec_obj$steps[[2]]$means[\"value\"]\n        scale_history  &lt;- rec_obj$steps[[3]]$sds[\"value\"]\n        \n        # 5.1.4 LSTM Plan\n        lag_setting  &lt;- 12 # = nrow(df_tst)\n        batch_size   &lt;- 1\n        train_length &lt;- 12*3\n        tsteps       &lt;- 1\n        epochs       &lt;- epochs\n        \n        # 5.1.5 Train Setup (MODIFIED)\n        lag_train_tbl &lt;- df_processed_tbl %&gt;%\n            mutate(value_lag = lag(value, n = lag_setting)) %&gt;%\n            filter(!is.na(value_lag)) %&gt;%\n            tail(train_length)\n        \n        x_train_vec &lt;- lag_train_tbl$value_lag\n        x_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n        \n        y_train_vec &lt;- lag_train_tbl$value\n        y_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n        \n        x_test_vec &lt;- y_train_vec %&gt;% tail(lag_setting)\n        x_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n                \n        # 5.1.6 LSTM Model\n        model &lt;- keras_model_sequential()\n\n        model %&gt;%\n            layer_lstm(units            = 50, \n                       input_shape      = c(tsteps, 1), \n                       batch_size       = batch_size,\n                       return_sequences = TRUE, \n                       stateful         = TRUE) %&gt;% \n            layer_lstm(units            = 50, \n                       return_sequences = FALSE, \n                       stateful         = TRUE) %&gt;% \n            layer_dense(units = 1)\n        \n        model %&gt;% \n            compile(loss = 'mae', optimizer = 'adam')\n        \n        # 5.1.7 Fitting LSTM\n        for (i in 1:epochs) {\n            model %&gt;% fit(x          = x_train_arr, \n                          y          = y_train_arr, \n                          batch_size = batch_size,\n                          epochs     = 1, \n                          verbose    = 1, \n                          shuffle    = FALSE)\n            \n            model %&gt;% reset_states()\n            cat(\"Epoch: \", i)\n            \n        }\n        \n        # 5.1.8 Predict and Return Tidy Data (MODIFIED)\n        # Make Predictions\n        pred_out &lt;- model %&gt;% \n            predict(x_test_arr, batch_size = batch_size) %&gt;%\n            .[,1] \n        \n        # Make future index using tk_make_future_timeseries()\n        idx &lt;- data %&gt;%\n            tk_index() %&gt;%\n            tk_make_future_timeseries(length_out = lag_setting)\n        \n        # Retransform values\n        pred_tbl &lt;- tibble(\n            index   = idx,\n            value   = (pred_out * scale_history + center_history)^2\n        )\n        \n        # Combine actual data with predictions\n        tbl_1 &lt;- df %&gt;%\n            add_column(key = \"actual\")\n\n        tbl_3 &lt;- pred_tbl %&gt;%\n            add_column(key = \"predict\")\n\n        # Create time_bind_rows() to solve dplyr issue\n        time_bind_rows &lt;- function(data_1, data_2, index) {\n            index_expr &lt;- enquo(index)\n            bind_rows(data_1, data_2) %&gt;%\n                as_tbl_time(index = !! index_expr)\n        }\n\n        ret &lt;- list(tbl_1, tbl_3) %&gt;%\n            reduce(time_bind_rows, index = index) %&gt;%\n            arrange(key, index) %&gt;%\n            mutate(key = as_factor(key))\n\n        return(ret)\n        \n    }\n    \n    safe_lstm &lt;- possibly(lstm_prediction, otherwise = NA)\n    \n    safe_lstm(data, epochs, ...)\n    \n}\n\n\nfuture_gold_ts_tbl &lt;- predict_keras_lstm_future(gold_mean, epochs = 300)\n\n\n 1/36 [..............................] - ETA: 2:00 - loss: 1.5409\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.8848  \n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.8329\n36/36 [==============================] - 4s 3ms/step - loss: 0.7856\nEpoch:  1\n 1/36 [..............................] - ETA: 0s - loss: 1.4715\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6275\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.5144\n36/36 [==============================] - 0s 3ms/step - loss: 0.5007\nEpoch:  2\n 1/36 [..............................] - ETA: 0s - loss: 1.4297\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.6316\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.5545\n36/36 [==============================] - 0s 3ms/step - loss: 0.5638\nEpoch:  3\n 1/36 [..............................] - ETA: 0s - loss: 1.4154\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.5522\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.4461\n36/36 [==============================] - 0s 3ms/step - loss: 0.4206\nEpoch:  4\n 1/36 [..............................] - ETA: 0s - loss: 1.3582\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.5188\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.4561\n36/36 [==============================] - 0s 3ms/step - loss: 0.4538\nEpoch:  5\n 1/36 [..............................] - ETA: 0s - loss: 1.3433\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.4306\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3802\n36/36 [==============================] - 0s 3ms/step - loss: 0.3850\nEpoch:  6\n 1/36 [..............................] - ETA: 0s - loss: 1.3144\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3556\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3484\n36/36 [==============================] - 0s 3ms/step - loss: 0.3559\nEpoch:  7\n 1/36 [..............................] - ETA: 0s - loss: 1.2928\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3348\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3363\n36/36 [==============================] - 0s 3ms/step - loss: 0.3399\nEpoch:  8\n 1/36 [..............................] - ETA: 0s - loss: 1.2721\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.3567\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3397\n36/36 [==============================] - 0s 3ms/step - loss: 0.3452\nEpoch:  9\n 1/36 [..............................] - ETA: 0s - loss: 1.2557\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3153\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3258\n36/36 [==============================] - 0s 3ms/step - loss: 0.3302\nEpoch:  10\n 1/36 [..............................] - ETA: 0s - loss: 1.2317\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.3171\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3287\n36/36 [==============================] - 0s 3ms/step - loss: 0.3344\nEpoch:  11\n 1/36 [..............................] - ETA: 0s - loss: 1.2119\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2980\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3131\n36/36 [==============================] - 0s 3ms/step - loss: 0.3201\nEpoch:  12\n 1/36 [..............................] - ETA: 0s - loss: 1.1835\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2962\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.3125\n36/36 [==============================] - 0s 3ms/step - loss: 0.3218\nEpoch:  13\n 1/36 [..............................] - ETA: 0s - loss: 1.1617\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2776\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3071\n36/36 [==============================] - 0s 3ms/step - loss: 0.3078\nEpoch:  14\n 1/36 [..............................] - ETA: 0s - loss: 1.1294\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2742\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.3018\n36/36 [==============================] - 0s 3ms/step - loss: 0.3060\nEpoch:  15\n 1/36 [..............................] - ETA: 0s - loss: 1.1065\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2717\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.3008\n36/36 [==============================] - 0s 3ms/step - loss: 0.3023\nEpoch:  16\n 1/36 [..............................] - ETA: 0s - loss: 1.0814\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2645\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2973\n36/36 [==============================] - 0s 3ms/step - loss: 0.2990\nEpoch:  17\n 1/36 [..............................] - ETA: 0s - loss: 1.0543\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2488\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2901\n36/36 [==============================] - 0s 3ms/step - loss: 0.2961\nEpoch:  18\n 1/36 [..............................] - ETA: 0s - loss: 1.0249\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2413\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2830\n36/36 [==============================] - 0s 3ms/step - loss: 0.2934\nEpoch:  19\n 1/36 [..............................] - ETA: 0s - loss: 0.9936\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2343\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2787\n36/36 [==============================] - 0s 3ms/step - loss: 0.2891\nEpoch:  20\n 1/36 [..............................] - ETA: 0s - loss: 0.9598\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2333\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.3035\n36/36 [==============================] - 0s 3ms/step - loss: 0.3090\nEpoch:  21\n 1/36 [..............................] - ETA: 0s - loss: 0.9158\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2351\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2852\n36/36 [==============================] - 0s 3ms/step - loss: 0.2942\nEpoch:  22\n 1/36 [..............................] - ETA: 0s - loss: 0.8915\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1969\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2675\n36/36 [==============================] - 0s 3ms/step - loss: 0.2785\nEpoch:  23\n 1/36 [..............................] - ETA: 0s - loss: 0.8509\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1880\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2634\n36/36 [==============================] - 0s 3ms/step - loss: 0.2748\nEpoch:  24\n 1/36 [..............................] - ETA: 0s - loss: 0.8066\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1789\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2588\n36/36 [==============================] - 0s 3ms/step - loss: 0.2705\nEpoch:  25\n 1/36 [..............................] - ETA: 0s - loss: 0.7590\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1687\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2533\n36/36 [==============================] - 0s 3ms/step - loss: 0.2651\nEpoch:  26\n 1/36 [..............................] - ETA: 0s - loss: 0.7069\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1512\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2417\n36/36 [==============================] - 0s 3ms/step - loss: 0.2519\nEpoch:  27\n 1/36 [..............................] - ETA: 0s - loss: 0.6650\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1758\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2500\n36/36 [==============================] - 0s 3ms/step - loss: 0.2537\nEpoch:  28\n 1/36 [..............................] - ETA: 0s - loss: 0.6035\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1433\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2342\n36/36 [==============================] - 0s 3ms/step - loss: 0.2421\nEpoch:  29\n 1/36 [..............................] - ETA: 0s - loss: 0.6087\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1638\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2413\n36/36 [==============================] - 0s 3ms/step - loss: 0.2598\nEpoch:  30\n 1/36 [..............................] - ETA: 0s - loss: 0.5576\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1232\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2168\n36/36 [==============================] - 0s 3ms/step - loss: 0.2171\nEpoch:  31\n 1/36 [..............................] - ETA: 0s - loss: 0.4852\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1740\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2436\n36/36 [==============================] - 0s 3ms/step - loss: 0.2491\nEpoch:  32\n 1/36 [..............................] - ETA: 0s - loss: 0.5447\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1510\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2323\n36/36 [==============================] - 0s 3ms/step - loss: 0.2357\nEpoch:  33\n 1/36 [..............................] - ETA: 0s - loss: 0.4692\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1245\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2208\n36/36 [==============================] - 0s 3ms/step - loss: 0.2217\nEpoch:  34\n 1/36 [..............................] - ETA: 0s - loss: 0.3938\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.2067\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2755\n36/36 [==============================] - 0s 3ms/step - loss: 0.2785\nEpoch:  35\n 1/36 [..............................] - ETA: 0s - loss: 0.4861\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1641\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2403\n36/36 [==============================] - 0s 3ms/step - loss: 0.2401\nEpoch:  36\n 1/36 [..............................] - ETA: 0s - loss: 0.3228\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.2095\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2697\n36/36 [==============================] - 0s 3ms/step - loss: 0.2743\nEpoch:  37\n 1/36 [..............................] - ETA: 0s - loss: 0.3780\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1483\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2301\n36/36 [==============================] - 0s 3ms/step - loss: 0.2449\nEpoch:  38\n 1/36 [..............................] - ETA: 0s - loss: 0.3149\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1441\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2296\n36/36 [==============================] - 0s 3ms/step - loss: 0.2329\nEpoch:  39\n 1/36 [..............................] - ETA: 0s - loss: 0.2753\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1367\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2293\n36/36 [==============================] - 0s 3ms/step - loss: 0.2368\nEpoch:  40\n 1/36 [..............................] - ETA: 0s - loss: 0.2480\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1217\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2198\n36/36 [==============================] - 0s 3ms/step - loss: 0.2257\nEpoch:  41\n 1/36 [..............................] - ETA: 0s - loss: 0.2056\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1177\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2165\n36/36 [==============================] - 0s 3ms/step - loss: 0.2210\nEpoch:  42\n 1/36 [..............................] - ETA: 0s - loss: 0.1968\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1282\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2209\n36/36 [==============================] - 0s 3ms/step - loss: 0.2221\nEpoch:  43\n 1/36 [..............................] - ETA: 0s - loss: 0.1159\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1028\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2126\n36/36 [==============================] - 0s 3ms/step - loss: 0.2114\nEpoch:  44\n 1/36 [..............................] - ETA: 0s - loss: 0.0685\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1887\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2629\n36/36 [==============================] - 0s 3ms/step - loss: 0.2661\nEpoch:  45\n 1/36 [..............................] - ETA: 0s - loss: 0.1374\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1302\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1971\n36/36 [==============================] - 0s 4ms/step - loss: 0.2261\nEpoch:  46\n 1/36 [..............................] - ETA: 0s - loss: 0.0310\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1639\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2395\n36/36 [==============================] - 0s 3ms/step - loss: 0.2421\nEpoch:  47\n 1/36 [..............................] - ETA: 0s - loss: 0.1023\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1267\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2225\n36/36 [==============================] - 0s 3ms/step - loss: 0.2222\nEpoch:  48\n 1/36 [..............................] - ETA: 0s - loss: 0.0448\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.0946\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2078\n36/36 [==============================] - 0s 3ms/step - loss: 0.2082\nEpoch:  49\n 1/36 [..............................] - ETA: 0s - loss: 0.1060\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1609\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2255\n36/36 [==============================] - 0s 4ms/step - loss: 0.2337\nEpoch:  50\n 1/36 [..............................] - ETA: 0s - loss: 0.0495\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1174\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2074\n36/36 [==============================] - 0s 3ms/step - loss: 0.2118\nEpoch:  51\n 1/36 [..............................] - ETA: 0s - loss: 0.0684\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1307\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.2016\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2128\n36/36 [==============================] - 0s 5ms/step - loss: 0.2109\nEpoch:  52\n 1/36 [..............................] - ETA: 0s - loss: 0.0577\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1130\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.1984\n36/36 [==============================] - 0s 3ms/step - loss: 0.1972\nEpoch:  53\n 1/36 [..............................] - ETA: 0s - loss: 0.0913\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1355\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2148\n36/36 [==============================] - 0s 3ms/step - loss: 0.2043\nEpoch:  54\n 1/36 [..............................] - ETA: 0s - loss: 0.0441\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.0875\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.1949\n36/36 [==============================] - 0s 3ms/step - loss: 0.1889\nEpoch:  55\n 1/36 [..............................] - ETA: 0s - loss: 0.0178\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.0898\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1895\n36/36 [==============================] - 0s 3ms/step - loss: 0.1814\nEpoch:  56\n 1/36 [..............................] - ETA: 0s - loss: 0.0342\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1094\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.1895\n36/36 [==============================] - 0s 3ms/step - loss: 0.1854\nEpoch:  57\n 1/36 [..............................] - ETA: 0s - loss: 0.0715\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1149\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2186\n36/36 [==============================] - 0s 3ms/step - loss: 0.2226\nEpoch:  58\n 1/36 [..............................] - ETA: 0s - loss: 0.1127\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1688\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2490\n36/36 [==============================] - 0s 3ms/step - loss: 0.2721\nEpoch:  59\n 1/36 [..............................] - ETA: 0s - loss: 0.0523\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1261\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2306\n36/36 [==============================] - 0s 3ms/step - loss: 0.2351\nEpoch:  60\n 1/36 [..............................] - ETA: 0s - loss: 0.0351\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1008\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2226\n36/36 [==============================] - 0s 3ms/step - loss: 0.2295\nEpoch:  61\n 1/36 [..............................] - ETA: 0s - loss: 0.0293\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1176\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2260\n36/36 [==============================] - 0s 3ms/step - loss: 0.2407\nEpoch:  62\n 1/36 [..............................] - ETA: 0s - loss: 0.0098\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1016\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2185\n36/36 [==============================] - 0s 3ms/step - loss: 0.2337\nEpoch:  63\n 1/36 [..............................] - ETA: 0s - loss: 0.0270\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1062\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2185\n36/36 [==============================] - 0s 3ms/step - loss: 0.2320\nEpoch:  64\n 1/36 [..............................] - ETA: 0s - loss: 0.0335\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1082\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2010\n36/36 [==============================] - 0s 3ms/step - loss: 0.2323\nEpoch:  65\n 1/36 [..............................] - ETA: 0s - loss: 0.0031\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1119\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2183\n36/36 [==============================] - 0s 3ms/step - loss: 0.2295\nEpoch:  66\n 1/36 [..............................] - ETA: 0s - loss: 0.0601\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1121\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2276\n36/36 [==============================] - 0s 3ms/step - loss: 0.2346\nEpoch:  67\n 1/36 [..............................] - ETA: 0s - loss: 0.0256\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1181\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2167\n36/36 [==============================] - 0s 3ms/step - loss: 0.2344\nEpoch:  68\n 1/36 [..............................] - ETA: 0s - loss: 0.0485\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1029\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1985\n36/36 [==============================] - 0s 4ms/step - loss: 0.2330\nEpoch:  69\n 1/36 [..............................] - ETA: 0s - loss: 0.0363\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1178\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2195\n36/36 [==============================] - 0s 3ms/step - loss: 0.2292\nEpoch:  70\n 1/36 [..............................] - ETA: 0s - loss: 0.0582\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1129\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2239\n36/36 [==============================] - 0s 3ms/step - loss: 0.2392\nEpoch:  71\n 1/36 [..............................] - ETA: 0s - loss: 0.0468\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1313\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2268\n36/36 [==============================] - 0s 3ms/step - loss: 0.2303\nEpoch:  72\n 1/36 [..............................] - ETA: 0s - loss: 0.0458\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1083\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2251\n36/36 [==============================] - 0s 3ms/step - loss: 0.2321\nEpoch:  73\n 1/36 [..............................] - ETA: 0s - loss: 0.0382\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1171\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2216\n36/36 [==============================] - 0s 3ms/step - loss: 0.2334\nEpoch:  74\n 1/36 [..............................] - ETA: 0s - loss: 0.0365\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1104\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2240\n36/36 [==============================] - 0s 3ms/step - loss: 0.2300\nEpoch:  75\n 1/36 [..............................] - ETA: 0s - loss: 0.0015\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1044\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2162\n36/36 [==============================] - 0s 3ms/step - loss: 0.2291\nEpoch:  76\n 1/36 [..............................] - ETA: 0s - loss: 0.0147\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1168\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2180\n36/36 [==============================] - 0s 3ms/step - loss: 0.2271\nEpoch:  77\n 1/36 [..............................] - ETA: 0s - loss: 0.0111\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1099\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2146\n36/36 [==============================] - 0s 3ms/step - loss: 0.2361\nEpoch:  78\n 1/36 [..............................] - ETA: 0s - loss: 0.0938\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1170\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2200\n36/36 [==============================] - 0s 3ms/step - loss: 0.2303\nEpoch:  79\n 1/36 [..............................] - ETA: 0s - loss: 0.0019\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1109\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2173\n36/36 [==============================] - 0s 3ms/step - loss: 0.2286\nEpoch:  80\n 1/36 [..............................] - ETA: 0s - loss: 0.0181\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1298\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2235\n36/36 [==============================] - 0s 3ms/step - loss: 0.2289\nEpoch:  81\n 1/36 [..............................] - ETA: 0s - loss: 0.0202\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1126\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2020\n36/36 [==============================] - 0s 3ms/step - loss: 0.2328\nEpoch:  82\n 1/36 [..............................] - ETA: 0s - loss: 0.0240\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1142\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2117\n36/36 [==============================] - 0s 3ms/step - loss: 0.2259\nEpoch:  83\n 1/36 [..............................] - ETA: 0s - loss: 0.0202\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1125\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2186\n36/36 [==============================] - 0s 3ms/step - loss: 0.2301\nEpoch:  84\n 1/36 [..............................] - ETA: 0s - loss: 0.0679\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1202\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2259\n36/36 [==============================] - 0s 3ms/step - loss: 0.2307\nEpoch:  85\n 1/36 [..............................] - ETA: 0s - loss: 0.0095\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1041\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2159\n36/36 [==============================] - 0s 3ms/step - loss: 0.2287\nEpoch:  86\n 1/36 [..............................] - ETA: 0s - loss: 0.0762\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1223\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2135\n36/36 [==============================] - 0s 3ms/step - loss: 0.2249\nEpoch:  87\n 1/36 [..............................] - ETA: 0s - loss: 0.0209\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1139\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2214\n36/36 [==============================] - 0s 3ms/step - loss: 0.2345\nEpoch:  88\n 1/36 [..............................] - ETA: 0s - loss: 0.0843\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1148\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2213\n36/36 [==============================] - 0s 3ms/step - loss: 0.2251\nEpoch:  89\n 1/36 [..............................] - ETA: 0s - loss: 0.0110\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1092\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2158\n36/36 [==============================] - 0s 3ms/step - loss: 0.2262\nEpoch:  90\n 1/36 [..............................] - ETA: 0s - loss: 0.0736\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1227\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2198\n36/36 [==============================] - 0s 3ms/step - loss: 0.2275\nEpoch:  91\n 1/36 [..............................] - ETA: 0s - loss: 0.0071\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.0994\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2082\n36/36 [==============================] - 0s 3ms/step - loss: 0.2261\nEpoch:  92\n 1/36 [..............................] - ETA: 0s - loss: 0.0801\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1186\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2127\n36/36 [==============================] - 0s 3ms/step - loss: 0.2259\nEpoch:  93\n 1/36 [..............................] - ETA: 0s - loss: 2.9087e-05\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.0973    \n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2124\n36/36 [==============================] - 0s 3ms/step - loss: 0.2247\nEpoch:  94\n 1/36 [..............................] - ETA: 0s - loss: 0.0191\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1243\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2140\n36/36 [==============================] - 0s 3ms/step - loss: 0.2160\nEpoch:  95\n 1/36 [..............................] - ETA: 0s - loss: 0.0149\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1265\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2165\n36/36 [==============================] - 0s 3ms/step - loss: 0.2191\nEpoch:  96\n 1/36 [..............................] - ETA: 0s - loss: 0.0712\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1193\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2150\n36/36 [==============================] - 0s 3ms/step - loss: 0.2183\nEpoch:  97\n 1/36 [..............................] - ETA: 0s - loss: 0.0120\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.0963\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2083\n36/36 [==============================] - 0s 3ms/step - loss: 0.2093\nEpoch:  98\n 1/36 [..............................] - ETA: 0s - loss: 0.0383\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1402\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2296\n36/36 [==============================] - 0s 3ms/step - loss: 0.2332\nEpoch:  99\n 1/36 [..............................] - ETA: 0s - loss: 0.0696\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1232\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2159\n36/36 [==============================] - 0s 3ms/step - loss: 0.2180\nEpoch:  100\n 1/36 [..............................] - ETA: 0s - loss: 0.1008\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1495\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2374\n36/36 [==============================] - 0s 3ms/step - loss: 0.2625\nEpoch:  101\n 1/36 [..............................] - ETA: 0s - loss: 0.0550\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1035\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2142\n36/36 [==============================] - 0s 3ms/step - loss: 0.2245\nEpoch:  102\n 1/36 [..............................] - ETA: 0s - loss: 0.0719\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1297\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2307\n36/36 [==============================] - 0s 3ms/step - loss: 0.2447\nEpoch:  103\n 1/36 [..............................] - ETA: 0s - loss: 0.0322\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1072\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2250\n36/36 [==============================] - 0s 3ms/step - loss: 0.2322\nEpoch:  104\n 1/36 [..............................] - ETA: 0s - loss: 0.0296\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1083\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2188\n36/36 [==============================] - 0s 3ms/step - loss: 0.2321\nEpoch:  105\n 1/36 [..............................] - ETA: 0s - loss: 0.0109\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1283\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2257\n36/36 [==============================] - 0s 3ms/step - loss: 0.2294\nEpoch:  106\n 1/36 [..............................] - ETA: 0s - loss: 0.0554\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1155\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2197\n36/36 [==============================] - 0s 3ms/step - loss: 0.2311\nEpoch:  107\n 1/36 [..............................] - ETA: 0s - loss: 0.0140\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1046\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2170\n36/36 [==============================] - 0s 3ms/step - loss: 0.2309\nEpoch:  108\n 1/36 [..............................] - ETA: 0s - loss: 0.0726\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1199\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2239\n36/36 [==============================] - 0s 3ms/step - loss: 0.2282\nEpoch:  109\n 1/36 [..............................] - ETA: 0s - loss: 0.0230\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1142\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2225\n36/36 [==============================] - 0s 3ms/step - loss: 0.2367\nEpoch:  110\n 1/36 [..............................] - ETA: 0s - loss: 0.0804\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1160\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2187\n36/36 [==============================] - 0s 3ms/step - loss: 0.2290\nEpoch:  111\n 1/36 [..............................] - ETA: 0s - loss: 0.0137\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1099\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2239\n36/36 [==============================] - 0s 3ms/step - loss: 0.2304\nEpoch:  112\n 1/36 [..............................] - ETA: 0s - loss: 0.0705\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1319\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2260\n36/36 [==============================] - 0s 3ms/step - loss: 0.2295\nEpoch:  113\n 1/36 [..............................] - ETA: 0s - loss: 0.0061\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1147\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2201\n36/36 [==============================] - 0s 3ms/step - loss: 0.2324\nEpoch:  114\n 1/36 [..............................] - ETA: 0s - loss: 0.0443\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1068\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2113\n36/36 [==============================] - 0s 3ms/step - loss: 0.2296\nEpoch:  115\n 1/36 [..............................] - ETA: 0s - loss: 0.0021\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1047\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2151\n36/36 [==============================] - 0s 3ms/step - loss: 0.2276\nEpoch:  116\n 1/36 [..............................] - ETA: 0s - loss: 0.0405\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1111\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2111\n36/36 [==============================] - 0s 3ms/step - loss: 0.2270\nEpoch:  117\n 1/36 [..............................] - ETA: 0s - loss: 0.0282\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1261\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2260\n36/36 [==============================] - 0s 3ms/step - loss: 0.2302\nEpoch:  118\n 1/36 [..............................] - ETA: 0s - loss: 0.0583\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1266\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.1879\n36/36 [==============================] - 0s 4ms/step - loss: 0.2263\nEpoch:  119\n 1/36 [..............................] - ETA: 0s - loss: 0.0396\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1242\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1872\n36/36 [==============================] - 0s 4ms/step - loss: 0.2365\nEpoch:  120\n 1/36 [..............................] - ETA: 0s - loss: 0.0661\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1169\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2178\n36/36 [==============================] - 0s 3ms/step - loss: 0.2273\nEpoch:  121\n 1/36 [..............................] - ETA: 0s - loss: 0.0300\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1141\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2188\n36/36 [==============================] - 0s 3ms/step - loss: 0.2303\nEpoch:  122\n 1/36 [..............................] - ETA: 0s - loss: 0.0562\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1366\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2271\n36/36 [==============================] - 0s 3ms/step - loss: 0.2303\nEpoch:  123\n 1/36 [..............................] - ETA: 0s - loss: 0.0230\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1072\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2112\n36/36 [==============================] - 0s 3ms/step - loss: 0.2297\nEpoch:  124\n 1/36 [..............................] - ETA: 0s - loss: 0.0637\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1202\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2177\n36/36 [==============================] - 0s 3ms/step - loss: 0.2259\nEpoch:  125\n 1/36 [..............................] - ETA: 0s - loss: 0.0343\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1184\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2228\n36/36 [==============================] - 0s 3ms/step - loss: 0.2358\nEpoch:  126\n 1/36 [..............................] - ETA: 0s - loss: 0.0712\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1170\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2175\n36/36 [==============================] - 0s 3ms/step - loss: 0.2268\nEpoch:  127\n 1/36 [..............................] - ETA: 0s - loss: 0.0251\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1316\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2262\n36/36 [==============================] - 0s 3ms/step - loss: 0.2298\nEpoch:  128\n 1/36 [..............................] - ETA: 0s - loss: 0.0611\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1165\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2178\n36/36 [==============================] - 0s 3ms/step - loss: 0.2277\nEpoch:  129\n 1/36 [..............................] - ETA: 0s - loss: 0.0054\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1126\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2190\n36/36 [==============================] - 0s 3ms/step - loss: 0.2315\nEpoch:  130\n 1/36 [..............................] - ETA: 0s - loss: 0.0546\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1226\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2014\n36/36 [==============================] - 0s 3ms/step - loss: 0.2246\nEpoch:  131\n 1/36 [..............................] - ETA: 0s - loss: 0.0455\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1191\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2295\n36/36 [==============================] - 0s 3ms/step - loss: 0.2364\nEpoch:  132\n 1/36 [..............................] - ETA: 0s - loss: 0.0626\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1136\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2113\n36/36 [==============================] - 0s 3ms/step - loss: 0.2255\nEpoch:  133\n 1/36 [..............................] - ETA: 0s - loss: 0.0355\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1156\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2193\n36/36 [==============================] - 0s 3ms/step - loss: 0.2307\nEpoch:  134\n 1/36 [..............................] - ETA: 0s - loss: 0.0529\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1140\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2167\n36/36 [==============================] - 0s 3ms/step - loss: 0.2268\nEpoch:  135\n 1/36 [..............................] - ETA: 0s - loss: 0.0148\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1108\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2242\n36/36 [==============================] - 0s 3ms/step - loss: 0.2309\nEpoch:  136\n 1/36 [..............................] - ETA: 0s - loss: 0.0461\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1063\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2146\n36/36 [==============================] - 0s 3ms/step - loss: 0.2262\nEpoch:  137\n 1/36 [..............................] - ETA: 0s - loss: 0.0225\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.0993\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2085\n36/36 [==============================] - 0s 3ms/step - loss: 0.2285\nEpoch:  138\n 1/36 [..............................] - ETA: 0s - loss: 0.0641\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1160\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2169\n36/36 [==============================] - 0s 3ms/step - loss: 0.2261\nEpoch:  139\n 1/36 [..............................] - ETA: 0s - loss: 0.0339\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1144\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2184\n36/36 [==============================] - 0s 3ms/step - loss: 0.2296\nEpoch:  140\n 1/36 [..............................] - ETA: 0s - loss: 0.0532\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1195\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2191\n36/36 [==============================] - 0s 3ms/step - loss: 0.2289\nEpoch:  141\n 1/36 [..............................] - ETA: 0s - loss: 0.0270\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1090\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2111\n36/36 [==============================] - 0s 3ms/step - loss: 0.2291\nEpoch:  142\n 1/36 [..............................] - ETA: 0s - loss: 0.0603\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1194\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2167\n36/36 [==============================] - 0s 3ms/step - loss: 0.2247\nEpoch:  143\n 1/36 [..............................] - ETA: 0s - loss: 0.0384\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1192\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2225\n36/36 [==============================] - 0s 3ms/step - loss: 0.2350\nEpoch:  144\n 1/36 [..............................] - ETA: 0s - loss: 0.0676\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1326\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2214\n36/36 [==============================] - 0s 3ms/step - loss: 0.2256\nEpoch:  145\n 1/36 [..............................] - ETA: 0s - loss: 0.0293\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1164\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2133\n36/36 [==============================] - 0s 3ms/step - loss: 0.2293\nEpoch:  146\n 1/36 [..............................] - ETA: 0s - loss: 0.0575\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1163\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2220\n36/36 [==============================] - 0s 3ms/step - loss: 0.2265\nEpoch:  147\n 1/36 [..............................] - ETA: 0s - loss: 0.0101\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1132\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2185\n36/36 [==============================] - 0s 3ms/step - loss: 0.2305\nEpoch:  148\n 1/36 [..............................] - ETA: 0s - loss: 0.0508\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1089\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2098\n36/36 [==============================] - 0s 3ms/step - loss: 0.2260\nEpoch:  149\n 1/36 [..............................] - ETA: 0s - loss: 0.0177\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1096\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2149\n36/36 [==============================] - 0s 3ms/step - loss: 0.2256\nEpoch:  150\n 1/36 [..............................] - ETA: 0s - loss: 0.0254\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1090\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2156\n36/36 [==============================] - 0s 3ms/step - loss: 0.2271\nEpoch:  151\n 1/36 [..............................] - ETA: 0s - loss: 7.1549e-04\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1150    \n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2195\n36/36 [==============================] - 0s 3ms/step - loss: 0.2234\nEpoch:  152\n 1/36 [..............................] - ETA: 0s - loss: 0.0122\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1188\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2210\n36/36 [==============================] - 0s 3ms/step - loss: 0.2326\nEpoch:  153\n 1/36 [..............................] - ETA: 0s - loss: 0.0040\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1182\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2106\n36/36 [==============================] - 0s 3ms/step - loss: 0.2226\nEpoch:  154\n 1/36 [..............................] - ETA: 0s - loss: 0.0992\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1232\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2243\n36/36 [==============================] - 0s 3ms/step - loss: 0.2365\nEpoch:  155\n 1/36 [..............................] - ETA: 0s - loss: 0.0111\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1145\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1872\n36/36 [==============================] - 0s 4ms/step - loss: 0.2234\nEpoch:  156\n 1/36 [..............................] - ETA: 0s - loss: 0.0884\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1196\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2255\n36/36 [==============================] - 0s 3ms/step - loss: 0.2308\nEpoch:  157\n 1/36 [..............................] - ETA: 0s - loss: 0.0015\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1131\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2151\n36/36 [==============================] - 0s 3ms/step - loss: 0.2245\nEpoch:  158\n 1/36 [..............................] - ETA: 0s - loss: 0.0677\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1166\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2199\n36/36 [==============================] - 0s 3ms/step - loss: 0.2316\nEpoch:  159\n 1/36 [..............................] - ETA: 0s - loss: 0.0043\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1096\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2132\n36/36 [==============================] - 0s 3ms/step - loss: 0.2226\nEpoch:  160\n 1/36 [..............................] - ETA: 0s - loss: 0.0115\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1142\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2197\n36/36 [==============================] - 0s 3ms/step - loss: 0.2321\nEpoch:  161\n 1/36 [..............................] - ETA: 0s - loss: 0.0031\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1101\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2128\n36/36 [==============================] - 0s 3ms/step - loss: 0.2216\nEpoch:  162\n 1/36 [..............................] - ETA: 0s - loss: 0.1010\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1392\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2270\n36/36 [==============================] - 0s 3ms/step - loss: 0.2320\nEpoch:  163\n 1/36 [..............................] - ETA: 0s - loss: 0.0085\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1082\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2141\n36/36 [==============================] - 0s 3ms/step - loss: 0.2248\nEpoch:  164\n 1/36 [..............................] - ETA: 0s - loss: 0.0377\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1068\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2144\n36/36 [==============================] - 0s 3ms/step - loss: 0.2259\nEpoch:  165\n 1/36 [..............................] - ETA: 0s - loss: 0.0106\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1154\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2167\n36/36 [==============================] - 0s 3ms/step - loss: 0.2265\nEpoch:  166\n 1/36 [..............................] - ETA: 0s - loss: 0.0729\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1090\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2223\n36/36 [==============================] - 0s 3ms/step - loss: 0.2286\nEpoch:  167\n 1/36 [..............................] - ETA: 0s - loss: 0.0154\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1180\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2105\n36/36 [==============================] - 0s 3ms/step - loss: 0.2224\nEpoch:  168\n 1/36 [..............................] - ETA: 0s - loss: 0.0841\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1185\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2217\n36/36 [==============================] - 0s 3ms/step - loss: 0.2339\nEpoch:  169\n 1/36 [..............................] - ETA: 0s - loss: 0.0227\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1130\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2190\n36/36 [==============================] - 0s 3ms/step - loss: 0.2230\nEpoch:  170\n 1/36 [..............................] - ETA: 0s - loss: 0.0749\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1166\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2130\n36/36 [==============================] - 0s 3ms/step - loss: 0.2284\nEpoch:  171\n 1/36 [..............................] - ETA: 0s - loss: 0.0124\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1121\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2191\n36/36 [==============================] - 0s 3ms/step - loss: 0.2234\nEpoch:  172\n 1/36 [..............................] - ETA: 0s - loss: 0.0561\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1174\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2164\n36/36 [==============================] - 0s 3ms/step - loss: 0.2250\nEpoch:  173\n 1/36 [..............................] - ETA: 0s - loss: 0.0121\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1276\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2216\n36/36 [==============================] - 0s 3ms/step - loss: 0.2273\nEpoch:  174\n 1/36 [..............................] - ETA: 0s - loss: 0.0511\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1145\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2154\n36/36 [==============================] - 0s 3ms/step - loss: 0.2247\nEpoch:  175\n 1/36 [..............................] - ETA: 0s - loss: 0.0326\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1055\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2152\n36/36 [==============================] - 0s 3ms/step - loss: 0.2274\nEpoch:  176\n 1/36 [..............................] - ETA: 0s - loss: 0.0577\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1113\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2125\n36/36 [==============================] - 0s 3ms/step - loss: 0.2201\nEpoch:  177\n 1/36 [..............................] - ETA: 0s - loss: 0.0412\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1176\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2177\n36/36 [==============================] - 0s 3ms/step - loss: 0.2268\nEpoch:  178\n 1/36 [..............................] - ETA: 0s - loss: 0.0463\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1115\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2126\n36/36 [==============================] - 0s 3ms/step - loss: 0.2204\nEpoch:  179\n 1/36 [..............................] - ETA: 0s - loss: 0.0234\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1081\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2136\n36/36 [==============================] - 0s 3ms/step - loss: 0.2234\nEpoch:  180\n 1/36 [..............................] - ETA: 0s - loss: 0.0605\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1225\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2126\n36/36 [==============================] - 0s 3ms/step - loss: 0.2233\nEpoch:  181\n 1/36 [..............................] - ETA: 0s - loss: 0.0209\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1039\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2125\n36/36 [==============================] - 0s 3ms/step - loss: 0.2230\nEpoch:  182\n 1/36 [..............................] - ETA: 0s - loss: 0.0651\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1154\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2147\n36/36 [==============================] - 0s 3ms/step - loss: 0.2222\nEpoch:  183\n 1/36 [..............................] - ETA: 0s - loss: 0.0158\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.0986\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2108\n36/36 [==============================] - 0s 3ms/step - loss: 0.2221\nEpoch:  184\n 1/36 [..............................] - ETA: 0s - loss: 0.0695\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1119\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2108\n36/36 [==============================] - 0s 3ms/step - loss: 0.2160\nEpoch:  185\n 1/36 [..............................] - ETA: 0s - loss: 0.0268\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1152\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1990\n36/36 [==============================] - 0s 3ms/step - loss: 0.2176\nEpoch:  186\n 1/36 [..............................] - ETA: 0s - loss: 0.0559\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1156\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2107\n36/36 [==============================] - 0s 3ms/step - loss: 0.2217\nEpoch:  187\n 1/36 [..............................] - ETA: 0s - loss: 0.0091\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.0899\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2046\n36/36 [==============================] - 0s 3ms/step - loss: 0.2129\nEpoch:  188\n 1/36 [..............................] - ETA: 0s - loss: 0.0438\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1221\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2118\n36/36 [==============================] - 0s 3ms/step - loss: 0.2204\nEpoch:  189\n 1/36 [..............................] - ETA: 0s - loss: 0.0047\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.0999\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2036\n36/36 [==============================] - 0s 3ms/step - loss: 0.2154\nEpoch:  190\n 1/36 [..............................] - ETA: 0s - loss: 0.0964\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1190\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2013\n36/36 [==============================] - 0s 3ms/step - loss: 0.2181\nEpoch:  191\n 1/36 [..............................] - ETA: 0s - loss: 0.0086\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1005\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1914\n36/36 [==============================] - 0s 3ms/step - loss: 0.2054\nEpoch:  192\n 1/36 [..............................] - ETA: 0s - loss: 0.0345\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.0991\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.1997\n36/36 [==============================] - 0s 3ms/step - loss: 0.2027\nEpoch:  193\n 1/36 [..............................] - ETA: 0s - loss: 0.0015\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1106\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2051\n36/36 [==============================] - 0s 3ms/step - loss: 0.1975\nEpoch:  194\n 1/36 [..............................] - ETA: 0s - loss: 0.0589\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1177\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2226\n36/36 [==============================] - 0s 3ms/step - loss: 0.2301\nEpoch:  195\n 1/36 [..............................] - ETA: 0s - loss: 0.1236\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1643\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2570\n36/36 [==============================] - 0s 3ms/step - loss: 0.2762\nEpoch:  196\n 1/36 [..............................] - ETA: 0s - loss: 0.0272\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.0928\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.1725\n36/36 [==============================] - 0s 4ms/step - loss: 0.2297\nEpoch:  197\n 1/36 [..............................] - ETA: 0s - loss: 0.0497\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1083\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.1807\n36/36 [==============================] - 0s 4ms/step - loss: 0.2322\nEpoch:  198\n 1/36 [..............................] - ETA: 0s - loss: 0.0168\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1158\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.1756\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2181\n36/36 [==============================] - 0s 5ms/step - loss: 0.2319\nEpoch:  199\n 1/36 [..............................] - ETA: 0s - loss: 0.0127\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.1165\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.1737\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2162\n36/36 [==============================] - 0s 6ms/step - loss: 0.2300\nEpoch:  200\n 1/36 [..............................] - ETA: 0s - loss: 0.0311\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1146\n22/36 [=================&gt;............] - ETA: 0s - loss: 0.1418\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2242\n36/36 [==============================] - 0s 5ms/step - loss: 0.2282\nEpoch:  201\n 1/36 [..............................] - ETA: 0s - loss: 0.0366\n11/36 [========&gt;.....................] - ETA: 0s - loss: 0.1070\n21/36 [================&gt;.............] - ETA: 0s - loss: 0.1333\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2098\n36/36 [==============================] - 0s 5ms/step - loss: 0.2304\nEpoch:  202\n 1/36 [..............................] - ETA: 0s - loss: 0.0448\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1197\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.1477\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2162\n36/36 [==============================] - 0s 5ms/step - loss: 0.2264\nEpoch:  203\n 1/36 [..............................] - ETA: 0s - loss: 0.0479\n11/36 [========&gt;.....................] - ETA: 0s - loss: 0.1159\n20/36 [===============&gt;..............] - ETA: 0s - loss: 0.1424\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1884\n36/36 [==============================] - 0s 6ms/step - loss: 0.2308\nEpoch:  204\n 1/36 [..............................] - ETA: 0s - loss: 0.0339\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1177\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.1861\n36/36 [==============================] - 0s 4ms/step - loss: 0.2268\nEpoch:  205\n 1/36 [..............................] - ETA: 0s - loss: 0.0328\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1159\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.1797\n36/36 [==============================] - 0s 4ms/step - loss: 0.2312\nEpoch:  206\n 1/36 [..............................] - ETA: 0s - loss: 0.0278\n 8/36 [=====&gt;........................] - ETA: 0s - loss: 0.1179\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1162\n20/36 [===============&gt;..............] - ETA: 0s - loss: 0.1368\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1866\n36/36 [==============================] - 0s 7ms/step - loss: 0.2235\nEpoch:  207\n 1/36 [..............................] - ETA: 0s - loss: 0.0673\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1279\n26/36 [====================&gt;.........] - ETA: 0s - loss: 0.1874\n36/36 [==============================] - 0s 4ms/step - loss: 0.2358\nEpoch:  208\n 1/36 [..............................] - ETA: 0s - loss: 0.0340\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1267\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2207\n36/36 [==============================] - 0s 3ms/step - loss: 0.2241\nEpoch:  209\n 1/36 [..............................] - ETA: 0s - loss: 0.0595\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.1279\n29/36 [=======================&gt;......] - ETA: 0s - loss: 0.1842\n36/36 [==============================] - 0s 4ms/step - loss: 0.2308\nEpoch:  210\n 1/36 [..............................] - ETA: 0s - loss: 0.0241\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1139\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.1814\n36/36 [==============================] - 0s 4ms/step - loss: 0.2251\nEpoch:  211\n 1/36 [..............................] - ETA: 0s - loss: 0.0440\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1122\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1818\n36/36 [==============================] - 0s 4ms/step - loss: 0.2286\nEpoch:  212\n 1/36 [..............................] - ETA: 0s - loss: 0.0379\n11/36 [========&gt;.....................] - ETA: 0s - loss: 0.1124\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.1824\n36/36 [==============================] - 0s 4ms/step - loss: 0.2239\nEpoch:  213\n 1/36 [..............................] - ETA: 0s - loss: 0.0558\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1168\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1895\n36/36 [==============================] - 0s 4ms/step - loss: 0.2337\nEpoch:  214\n 1/36 [..............................] - ETA: 0s - loss: 0.0434\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1137\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2151\n36/36 [==============================] - 0s 3ms/step - loss: 0.2246\nEpoch:  215\n 1/36 [..............................] - ETA: 0s - loss: 0.0488\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1199\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1880\n36/36 [==============================] - 0s 4ms/step - loss: 0.2289\nEpoch:  216\n 1/36 [..............................] - ETA: 0s - loss: 0.0331\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1179\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1867\n36/36 [==============================] - 0s 4ms/step - loss: 0.2251\nEpoch:  217\n 1/36 [..............................] - ETA: 0s - loss: 0.0343\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1174\n21/36 [================&gt;.............] - ETA: 0s - loss: 0.1352\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1843\n36/36 [==============================] - 0s 6ms/step - loss: 0.2296\nEpoch:  218\n 1/36 [..............................] - ETA: 0s - loss: 0.0268\n 8/36 [=====&gt;........................] - ETA: 0s - loss: 0.1163\n20/36 [===============&gt;..............] - ETA: 0s - loss: 0.1365\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1868\n36/36 [==============================] - 0s 5ms/step - loss: 0.2222\nEpoch:  219\n 1/36 [..............................] - ETA: 0s - loss: 0.0688\n13/36 [=========&gt;....................] - ETA: 0s - loss: 0.1335\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.1836\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2282\n36/36 [==============================] - 0s 5ms/step - loss: 0.2347\nEpoch:  220\n 1/36 [..............................] - ETA: 0s - loss: 0.0328\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1160\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.1788\n36/36 [==============================] - 0s 4ms/step - loss: 0.2228\nEpoch:  221\n 1/36 [..............................] - ETA: 0s - loss: 0.0610\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1236\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1885\n36/36 [==============================] - 0s 4ms/step - loss: 0.2300\nEpoch:  222\n 1/36 [..............................] - ETA: 0s - loss: 0.0229\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1144\n25/36 [===================&gt;..........] - ETA: 0s - loss: 0.1787\n36/36 [==============================] - 0s 4ms/step - loss: 0.2240\nEpoch:  223\n 1/36 [..............................] - ETA: 0s - loss: 0.0458\n 9/36 [======&gt;.......................] - ETA: 0s - loss: 0.1107\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.1461\n36/36 [==============================] - 0s 4ms/step - loss: 0.2280\nEpoch:  224\n 1/36 [..............................] - ETA: 0s - loss: 0.0362\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1216\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.1891\n36/36 [==============================] - 0s 4ms/step - loss: 0.2228\nEpoch:  225\n 1/36 [..............................] - ETA: 0s - loss: 0.0576\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1261\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.1860\n36/36 [==============================] - 0s 4ms/step - loss: 0.2334\nEpoch:  226\n 1/36 [..............................] - ETA: 0s - loss: 0.0417\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1169\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1879\n36/36 [==============================] - 0s 4ms/step - loss: 0.2235\nEpoch:  227\n 1/36 [..............................] - ETA: 0s - loss: 0.0505\n11/36 [========&gt;.....................] - ETA: 0s - loss: 0.1154\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.1641\n36/36 [==============================] - 0s 5ms/step - loss: 0.2321\nEpoch:  228\n 1/36 [..............................] - ETA: 0s - loss: 0.0480\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1175\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1869\n36/36 [==============================] - 0s 4ms/step - loss: 0.2242\nEpoch:  229\n 1/36 [..............................] - ETA: 0s - loss: 0.0436\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1131\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1869\n36/36 [==============================] - 0s 4ms/step - loss: 0.2279\nEpoch:  230\n 1/36 [..............................] - ETA: 0s - loss: 0.0377\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1126\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1862\n36/36 [==============================] - 0s 4ms/step - loss: 0.2245\nEpoch:  231\n 1/36 [..............................] - ETA: 0s - loss: 0.0296\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1180\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.1634\n36/36 [==============================] - ETA: 0s - loss: 0.2250\n36/36 [==============================] - 0s 5ms/step - loss: 0.2250\nEpoch:  232\n 1/36 [..............................] - ETA: 0s - loss: 0.0150\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1151\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.1825\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2147\n36/36 [==============================] - 0s 5ms/step - loss: 0.2253\nEpoch:  233\n 1/36 [..............................] - ETA: 0s - loss: 0.0173\n12/36 [=========&gt;....................] - ETA: 0s - loss: 0.1151\n24/36 [===================&gt;..........] - ETA: 0s - loss: 0.1616\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2095\n36/36 [==============================] - 0s 5ms/step - loss: 0.2249\nEpoch:  234\n 1/36 [..............................] - ETA: 0s - loss: 0.0271\n10/36 [=======&gt;......................] - ETA: 0s - loss: 0.1035\n23/36 [==================&gt;...........] - ETA: 0s - loss: 0.1463\n36/36 [==============================] - ETA: 0s - loss: 0.2273\n36/36 [==============================] - 0s 5ms/step - loss: 0.2273\nEpoch:  235\n 1/36 [..............................] - ETA: 0s - loss: 0.0247\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1186\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1989\n36/36 [==============================] - 0s 4ms/step - loss: 0.2225\nEpoch:  236\n 1/36 [..............................] - ETA: 0s - loss: 0.0051\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1199\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1876\n36/36 [==============================] - 0s 4ms/step - loss: 0.2272\nEpoch:  237\n 1/36 [..............................] - ETA: 0s - loss: 0.0786\n14/36 [==========&gt;...................] - ETA: 0s - loss: 0.1244\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1906\n36/36 [==============================] - 0s 4ms/step - loss: 0.2265\nEpoch:  238\n 1/36 [..............................] - ETA: 0s - loss: 0.0022\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1150\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2211\n36/36 [==============================] - 0s 3ms/step - loss: 0.2261\nEpoch:  239\n 1/36 [..............................] - ETA: 0s - loss: 0.0328\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1123\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2145\n36/36 [==============================] - 0s 3ms/step - loss: 0.2246\nEpoch:  240\n 1/36 [..............................] - ETA: 0s - loss: 0.0128\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1099\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1849\n36/36 [==============================] - 0s 4ms/step - loss: 0.2249\nEpoch:  241\n 1/36 [..............................] - ETA: 0s - loss: 0.0196\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1136\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2158\n36/36 [==============================] - 0s 3ms/step - loss: 0.2263\nEpoch:  242\n 1/36 [..............................] - ETA: 0s - loss: 0.0167\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1242\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2210\n36/36 [==============================] - 0s 3ms/step - loss: 0.2248\nEpoch:  243\n 1/36 [..............................] - ETA: 0s - loss: 0.0153\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1143\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2187\n36/36 [==============================] - 0s 3ms/step - loss: 0.2228\nEpoch:  244\n 1/36 [..............................] - ETA: 0s - loss: 0.0040\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1153\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2010\n36/36 [==============================] - 0s 3ms/step - loss: 0.2292\nEpoch:  245\n 1/36 [..............................] - ETA: 0s - loss: 0.0119\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1119\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2137\n36/36 [==============================] - 0s 3ms/step - loss: 0.2232\nEpoch:  246\n 1/36 [..............................] - ETA: 0s - loss: 0.0081\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1135\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2150\n36/36 [==============================] - 0s 3ms/step - loss: 0.2250\nEpoch:  247\n 1/36 [..............................] - ETA: 0s - loss: 0.0249\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1130\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2147\n36/36 [==============================] - 0s 3ms/step - loss: 0.2246\nEpoch:  248\n 1/36 [..............................] - ETA: 0s - loss: 0.0200\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1137\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2160\n36/36 [==============================] - 0s 3ms/step - loss: 0.2267\nEpoch:  249\n 1/36 [..............................] - ETA: 0s - loss: 0.0322\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1083\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2084\n36/36 [==============================] - 0s 3ms/step - loss: 0.2243\nEpoch:  250\n 1/36 [..............................] - ETA: 0s - loss: 0.0134\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1052\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2128\n36/36 [==============================] - 0s 3ms/step - loss: 0.2245\nEpoch:  251\n 1/36 [..............................] - ETA: 0s - loss: 0.0187\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1139\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2140\n36/36 [==============================] - 0s 3ms/step - loss: 0.2229\nEpoch:  252\n 1/36 [..............................] - ETA: 0s - loss: 5.0116e-04\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1132    \n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2170\n36/36 [==============================] - 0s 3ms/step - loss: 0.2287\nEpoch:  253\n 1/36 [..............................] - ETA: 0s - loss: 0.0152\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1168\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1859\n36/36 [==============================] - 0s 4ms/step - loss: 0.2232\nEpoch:  254\n 1/36 [..............................] - ETA: 0s - loss: 0.0048\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1124\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2098\n36/36 [==============================] - 0s 3ms/step - loss: 0.2247\nEpoch:  255\n 1/36 [..............................] - ETA: 0s - loss: 0.0282\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1119\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2142\n36/36 [==============================] - 0s 3ms/step - loss: 0.2242\nEpoch:  256\n 1/36 [..............................] - ETA: 0s - loss: 0.0166\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1129\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1991\n36/36 [==============================] - 0s 4ms/step - loss: 0.2260\nEpoch:  257\n 1/36 [..............................] - ETA: 0s - loss: 0.0356\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1139\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2090\n36/36 [==============================] - 0s 3ms/step - loss: 0.2216\nEpoch:  258\n 1/36 [..............................] - ETA: 0s - loss: 0.0160\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1148\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1880\n36/36 [==============================] - 0s 4ms/step - loss: 0.2310\nEpoch:  259\n 1/36 [..............................] - ETA: 0s - loss: 0.0850\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1141\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2144\n36/36 [==============================] - 0s 3ms/step - loss: 0.2234\nEpoch:  260\n 1/36 [..............................] - ETA: 0s - loss: 0.0062\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1130\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2158\n36/36 [==============================] - 0s 3ms/step - loss: 0.2266\nEpoch:  261\n 1/36 [..............................] - ETA: 0s - loss: 0.0750\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1184\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2114\n36/36 [==============================] - 0s 3ms/step - loss: 0.2252\nEpoch:  262\n 1/36 [..............................] - ETA: 0s - loss: 0.0012\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1093\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2095\n36/36 [==============================] - 0s 3ms/step - loss: 0.2263\nEpoch:  263\n 1/36 [..............................] - ETA: 0s - loss: 0.0804\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1173\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2103\n36/36 [==============================] - 0s 3ms/step - loss: 0.2219\nEpoch:  264\n 1/36 [..............................] - ETA: 0s - loss: 0.0126\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1185\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2197\n36/36 [==============================] - 0s 3ms/step - loss: 0.2312\nEpoch:  265\n 1/36 [..............................] - ETA: 0s - loss: 0.0847\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1153\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2100\n36/36 [==============================] - 0s 4ms/step - loss: 0.2225\nEpoch:  266\n 1/36 [..............................] - ETA: 0s - loss: 0.0058\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1221\n28/36 [======================&gt;.......] - ETA: 0s - loss: 0.1882\n36/36 [==============================] - 0s 4ms/step - loss: 0.2266\nEpoch:  267\n 1/36 [..............................] - ETA: 0s - loss: 0.0743\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1090\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1847\n36/36 [==============================] - 0s 4ms/step - loss: 0.2252\nEpoch:  268\n 1/36 [..............................] - ETA: 0s - loss: 0.0434\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1322\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2207\n36/36 [==============================] - 0s 3ms/step - loss: 0.2252\nEpoch:  269\n 1/36 [..............................] - ETA: 0s - loss: 0.0334\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1073\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2139\n36/36 [==============================] - 0s 3ms/step - loss: 0.2255\nEpoch:  270\n 1/36 [..............................] - ETA: 0s - loss: 0.0460\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1172\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2004\n36/36 [==============================] - 0s 4ms/step - loss: 0.2219\nEpoch:  271\n 1/36 [..............................] - ETA: 0s - loss: 0.0448\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1162\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2242\n36/36 [==============================] - 0s 3ms/step - loss: 0.2302\nEpoch:  272\n 1/36 [..............................] - ETA: 0s - loss: 0.0512\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1153\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2141\n36/36 [==============================] - 0s 3ms/step - loss: 0.2224\nEpoch:  273\n 1/36 [..............................] - ETA: 0s - loss: 0.0380\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1146\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2157\n36/36 [==============================] - 0s 3ms/step - loss: 0.2257\nEpoch:  274\n 1/36 [..............................] - ETA: 0s - loss: 0.0407\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1195\n27/36 [=====================&gt;........] - ETA: 0s - loss: 0.1861\n36/36 [==============================] - 0s 4ms/step - loss: 0.2226\nEpoch:  275\n 1/36 [..............................] - ETA: 0s - loss: 0.0252\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1139\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1996\n36/36 [==============================] - 0s 4ms/step - loss: 0.2265\nEpoch:  276\n 1/36 [..............................] - ETA: 0s - loss: 0.0351\n18/36 [==============&gt;...............] - ETA: 0s - loss: 0.1305\n35/36 [============================&gt;.] - ETA: 0s - loss: 0.2205\n36/36 [==============================] - 0s 3ms/step - loss: 0.2235\nEpoch:  277\n 1/36 [..............................] - ETA: 0s - loss: 0.0422\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1097\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1854\n36/36 [==============================] - 0s 4ms/step - loss: 0.2265\nEpoch:  278\n 1/36 [..............................] - ETA: 0s - loss: 0.0406\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1136\n34/36 [===========================&gt;..] - ETA: 0s - loss: 0.2165\n36/36 [==============================] - 0s 3ms/step - loss: 0.2199\nEpoch:  279\n 1/36 [..............................] - ETA: 0s - loss: 0.0508\n15/36 [===========&gt;..................] - ETA: 0s - loss: 0.1237\n30/36 [========================&gt;.....] - ETA: 0s - loss: 0.1896\n36/36 [==============================] - 0s 4ms/step - loss: 0.2306\nEpoch:  280\n 1/36 [..............................] - ETA: 0s - loss: 0.0459\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1130\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2079\n36/36 [==============================] - 0s 3ms/step - loss: 0.2202\nEpoch:  281\n 1/36 [..............................] - ETA: 0s - loss: 0.0438\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1164\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2118\n36/36 [==============================] - 0s 3ms/step - loss: 0.2260\nEpoch:  282\n 1/36 [..............................] - ETA: 0s - loss: 0.0356\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1116\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2123\n36/36 [==============================] - 0s 3ms/step - loss: 0.2209\nEpoch:  283\n 1/36 [..............................] - ETA: 0s - loss: 0.0310\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1094\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2137\n36/36 [==============================] - 0s 3ms/step - loss: 0.2241\nEpoch:  284\n 1/36 [..............................] - ETA: 0s - loss: 0.0464\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1178\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2153\n36/36 [==============================] - 0s 3ms/step - loss: 0.2234\nEpoch:  285\n 1/36 [..............................] - ETA: 0s - loss: 0.0286\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1070\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2080\n36/36 [==============================] - 0s 4ms/step - loss: 0.2240\nEpoch:  286\n 1/36 [..............................] - ETA: 0s - loss: 0.0507\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1179\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2102\n36/36 [==============================] - 0s 3ms/step - loss: 0.2229\nEpoch:  287\n 1/36 [..............................] - ETA: 0s - loss: 0.0237\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1035\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2067\n36/36 [==============================] - 0s 3ms/step - loss: 0.2234\nEpoch:  288\n 1/36 [..............................] - ETA: 0s - loss: 0.0550\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1135\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2118\n36/36 [==============================] - 0s 3ms/step - loss: 0.2187\nEpoch:  289\n 1/36 [..............................] - ETA: 0s - loss: 0.0335\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1154\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2101\n36/36 [==============================] - 0s 3ms/step - loss: 0.2224\nEpoch:  290\n 1/36 [..............................] - ETA: 0s - loss: 0.0429\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1123\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2113\n36/36 [==============================] - 0s 3ms/step - loss: 0.2183\nEpoch:  291\n 1/36 [..............................] - ETA: 0s - loss: 0.0230\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1078\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1962\n36/36 [==============================] - 0s 4ms/step - loss: 0.2208\nEpoch:  292\n 1/36 [..............................] - ETA: 0s - loss: 0.0520\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1211\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.2012\n36/36 [==============================] - 0s 3ms/step - loss: 0.2197\nEpoch:  293\n 1/36 [..............................] - ETA: 0s - loss: 0.0220\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1048\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1954\n36/36 [==============================] - 0s 4ms/step - loss: 0.2212\nEpoch:  294\n 1/36 [..............................] - ETA: 0s - loss: 0.0549\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1173\n32/36 [=========================&gt;....] - ETA: 0s - loss: 0.2089\n36/36 [==============================] - 0s 3ms/step - loss: 0.2187\nEpoch:  295\n 1/36 [..............................] - ETA: 0s - loss: 0.0190\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1021\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1942\n36/36 [==============================] - 0s 4ms/step - loss: 0.2197\nEpoch:  296\n 1/36 [..............................] - ETA: 0s - loss: 0.0575\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1132\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2111\n36/36 [==============================] - 0s 3ms/step - loss: 0.2169\nEpoch:  297\n 1/36 [..............................] - ETA: 0s - loss: 0.0165\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.1008\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2088\n36/36 [==============================] - 0s 3ms/step - loss: 0.2182\nEpoch:  298\n 1/36 [..............................] - ETA: 0s - loss: 0.0595\n16/36 [============&gt;.................] - ETA: 0s - loss: 0.1127\n31/36 [========================&gt;.....] - ETA: 0s - loss: 0.1969\n36/36 [==============================] - 0s 4ms/step - loss: 0.2139\nEpoch:  299\n 1/36 [..............................] - ETA: 0s - loss: 0.0149\n17/36 [=============&gt;................] - ETA: 0s - loss: 0.0994\n33/36 [==========================&gt;...] - ETA: 0s - loss: 0.2075\n36/36 [==============================] - 0s 3ms/step - loss: 0.2160\nEpoch:  300\n 1/12 [=&gt;............................] - ETA: 9s\n12/12 [==============================] - 1s 2ms/step\n\n\n\nfuture_gold_ts_tbl %&gt;%\n    filter_time(\"2013\" ~ \"end\") %&gt;%\n    plot_prediction(id = NULL, alpha = 0.4, size = 1.5) +\n    theme(legend.position = \"bottom\") +\n    ggtitle(\"Sunspots: Ten Year Forecast\", subtitle = \"Forecast Horizon: 2013 - 2023\")"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html",
    "href": "posts/serie_ouro/arima/index.html",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "Modelos ARIMA (Autoregressive Integrated Moving Average) e SARIMA (Seasonal ARIMA) são poderosas ferramentas de análise e previsão de séries temporais amplamente usadas em estatísticas e ciência de dados.\nOs modelos ARIMA combinam três componentes principais: o componente autoregressivo (AR), que leva em consideração as dependências lineares entre observações passadas; o componente de média móvel (MA), que modela o ruído da série; e a diferenciação integrada (I), que lida com tendências e sazonalidades não estacionárias. Juntos, esses componentes permitem modelar e prever uma ampla variedade de padrões em séries temporais univariadas.\nOs modelos SARIMA, por sua vez, estendem os ARIMA para lidar com sazonalidades sazonais. Eles incluem componentes adicionais para modelar sazonalidades anuais, trimestrais ou de qualquer período desejado. Isso os torna especialmente úteis para prever séries temporais com padrões sazonais evidentes.\nAmbos os modelos ARIMA e SARIMA exigem a escolha adequada das ordens dos componentes (p, d, q) e (P, D, Q, S), respectivamente, o que pode ser realizado por meio de análise exploratória dos dados, como a função de autocorrelação (ACF) e a função de autocorrelação parcial (PACF). Uma vez ajustados, esses modelos podem gerar previsões úteis e precisas para uma ampla gama de aplicações, desde previsões financeiras até demanda de produtos.\n\n\n\n\n\n\nsplits &lt;- \ntime_series_split(\n  gold_mean,\n  assess = \"1 year\",\n  cumulative = TRUE\n)\n\n\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media)\n\n\n\n\n\n\n\n\nA função auto_arima() é uma parte fundamental do pacote modeltime, que é uma extensão do pacote forecast e é projetado para facilitar a modelagem e previsão de séries temporais no R. auto_arima() é uma ferramenta poderosa que automatiza o processo de seleção do melhor modelo ARIMA (Autoregressive Integrated Moving Average) para uma série temporal específica.\nAqui estão alguns pontos-chave sobre auto_arima() no modeltime:\n\nSeleção Automática de Modelos: O principal objetivo do auto_arima() é determinar automaticamente os valores ideais dos parâmetros (p, d, q) para o modelo ARIMA que melhor se ajusta à sua série temporal. Isso é feito através de uma busca inteligente por meio de várias combinações possíveis desses parâmetros.\nSeleção de Sazonalidade: Além dos parâmetros ARIMA, auto_arima() também pode lidar com modelos SARIMA (Seasonal ARIMA) e selecionar automaticamente os parâmetros sazonais (P, D, Q, S) quando a série apresenta sazonalidade.\nCritérios de Avaliação: A função utiliza critérios de avaliação estatística, como o Critério de Informação de Akaike (AIC) e o Critério de Informação Bayesiano (BIC), para avaliar e comparar o ajuste de diferentes modelos. Ela seleciona o modelo com o menor valor desses critérios, o que geralmente indica um ajuste melhor à série.\nFacilidade de Uso: auto_arima() é projetado para ser fácil de usar. Basta fornecer a série temporal como entrada, e ele cuidará de todo o processo de seleção do modelo. Você não precisa especificar manualmente os parâmetros, a menos que deseje restringir as opções.\nFlexibilidade: Apesar da automação, auto_arima() também permite que você insira algumas restrições ou diretrizes para o processo de seleção, como limitar os valores máximos de p, d e q, ou forçar a inclusão de sazonalidade.\nResultados Interpretais: Uma vez que o auto_arima() encontra o melhor modelo, ele fornece um objeto de modelo que pode ser usado para fazer previsões e também inclui informações sobre os parâmetros selecionados, facilitando a interpretação do modelo final.\n\n\nauto_arima &lt;- \n  arima_reg() %&gt;% \n  set_engine(\"auto_arima\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\n\n\nJá foi descrito no ínicio. Os parâmetros foram determinados durante a análise exploratória.\n\nmodel_arima &lt;- \n  arima_reg(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n        non_seasonal_differences = 1,\n        non_seasonal_ma          = 0,\n        seasonal_ar              = 1,\n        seasonal_differences     = 1,\n        seasonal_ma              = 0\n    ) %&gt;% \n  set_engine(\"arima\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\n\n\nÉ uma extensão do modelo ARIMA tradicional que permite a incorporação de múltiplas sazonalidades em uma série temporal. Este modelo é particularmente útil quando seus dados exibem padrões sazonais em diferentes escalas de tempo.\n\narima_m_season &lt;- \n  seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n  ) %&gt;% \n  set_engine(\"stlm_arima\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\n\n\nA combinação de modelos ARIMA (Autoregressive Integrated Moving Average) com algoritmos de machine learning, como o XGBoost (Extreme Gradient Boosting), é uma abordagem útil para melhorar a previsão de séries temporais, especialmente quando os dados exibem padrões complexos e não lineares. A ideia principal é aproveitar a capacidade de ambos os modelos para capturar diferentes aspectos da série temporal.\n\nmodel_fit_arima_boosted &lt;- \n  arima_boost(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n    non_seasonal_differences = 1,\n    non_seasonal_ma = 0,\n    seasonal_ar     = 1,\n    seasonal_differences = 1,\n    seasonal_ma     = 0,\n\n    # XGBoost Args\n    tree_depth = 6,\n    learn_rate = 0.015,\n    min_n = 2) %&gt;% \n  set_engine(\"arima_xgboost\") %&gt;% \n  fit(media ~ mes + as.numeric(mes) + factor(month(mes, label = TRUE), ordered = F),\n        data = training(splits))\n\n\n\n\n\nmodel_table &lt;- \nmodeltime_table(\n  auto_arima,\n  model_arima,\n  arima_m_season,\n  model_fit_arima_boosted\n)\n\n\n\n\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))\n\n\n\n\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )\n\n\n\n\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 4 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         1 ARIMA(1,1,0)              Test  21.7  12.1   4.63 13.1   23.6 0.655 \n2         2 ARIMA(1,1,0)(1,1,0)[18]   Test   9.88  5.54  2.11  5.73  11.4 0.359 \n3         3 SEASONAL DECOMP: ARIMA(1… Test  20.1  11.2   4.28 12.0   22.0 0.0669\n4         4 ARIMA(1,1,0)(1,1,0)[18] … Test   9.87  5.53  2.11  5.72  11.4 0.364 \n\n\n\n\n\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()\n\n\n\n\n\n\n\n\n\ncalib_arima &lt;- \n  model_arima %&gt;% \n  modeltime_calibrate(testing(splits))\n\nfuture_forecast_tbl &lt;- \n  calib_arima %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#cross-validação",
    "href": "posts/serie_ouro/arima/index.html#cross-validação",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "splits &lt;- \ntime_series_split(\n  gold_mean,\n  assess = \"1 year\",\n  cumulative = TRUE\n)\n\n\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media)"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#auto-arima",
    "href": "posts/serie_ouro/arima/index.html#auto-arima",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "A função auto_arima() é uma parte fundamental do pacote modeltime, que é uma extensão do pacote forecast e é projetado para facilitar a modelagem e previsão de séries temporais no R. auto_arima() é uma ferramenta poderosa que automatiza o processo de seleção do melhor modelo ARIMA (Autoregressive Integrated Moving Average) para uma série temporal específica.\nAqui estão alguns pontos-chave sobre auto_arima() no modeltime:\n\nSeleção Automática de Modelos: O principal objetivo do auto_arima() é determinar automaticamente os valores ideais dos parâmetros (p, d, q) para o modelo ARIMA que melhor se ajusta à sua série temporal. Isso é feito através de uma busca inteligente por meio de várias combinações possíveis desses parâmetros.\nSeleção de Sazonalidade: Além dos parâmetros ARIMA, auto_arima() também pode lidar com modelos SARIMA (Seasonal ARIMA) e selecionar automaticamente os parâmetros sazonais (P, D, Q, S) quando a série apresenta sazonalidade.\nCritérios de Avaliação: A função utiliza critérios de avaliação estatística, como o Critério de Informação de Akaike (AIC) e o Critério de Informação Bayesiano (BIC), para avaliar e comparar o ajuste de diferentes modelos. Ela seleciona o modelo com o menor valor desses critérios, o que geralmente indica um ajuste melhor à série.\nFacilidade de Uso: auto_arima() é projetado para ser fácil de usar. Basta fornecer a série temporal como entrada, e ele cuidará de todo o processo de seleção do modelo. Você não precisa especificar manualmente os parâmetros, a menos que deseje restringir as opções.\nFlexibilidade: Apesar da automação, auto_arima() também permite que você insira algumas restrições ou diretrizes para o processo de seleção, como limitar os valores máximos de p, d e q, ou forçar a inclusão de sazonalidade.\nResultados Interpretais: Uma vez que o auto_arima() encontra o melhor modelo, ele fornece um objeto de modelo que pode ser usado para fazer previsões e também inclui informações sobre os parâmetros selecionados, facilitando a interpretação do modelo final.\n\n\nauto_arima &lt;- \n  arima_reg() %&gt;% \n  set_engine(\"auto_arima\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#arima-model",
    "href": "posts/serie_ouro/arima/index.html#arima-model",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "Já foi descrito no ínicio. Os parâmetros foram determinados durante a análise exploratória.\n\nmodel_arima &lt;- \n  arima_reg(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n        non_seasonal_differences = 1,\n        non_seasonal_ma          = 0,\n        seasonal_ar              = 1,\n        seasonal_differences     = 1,\n        seasonal_ma              = 0\n    ) %&gt;% \n  set_engine(\"arima\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#modelo-arima-sazonal-múltiplo",
    "href": "posts/serie_ouro/arima/index.html#modelo-arima-sazonal-múltiplo",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "É uma extensão do modelo ARIMA tradicional que permite a incorporação de múltiplas sazonalidades em uma série temporal. Este modelo é particularmente útil quando seus dados exibem padrões sazonais em diferentes escalas de tempo.\n\narima_m_season &lt;- \n  seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n  ) %&gt;% \n  set_engine(\"stlm_arima\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#arima-gxboost-dos-erros",
    "href": "posts/serie_ouro/arima/index.html#arima-gxboost-dos-erros",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "A combinação de modelos ARIMA (Autoregressive Integrated Moving Average) com algoritmos de machine learning, como o XGBoost (Extreme Gradient Boosting), é uma abordagem útil para melhorar a previsão de séries temporais, especialmente quando os dados exibem padrões complexos e não lineares. A ideia principal é aproveitar a capacidade de ambos os modelos para capturar diferentes aspectos da série temporal.\n\nmodel_fit_arima_boosted &lt;- \n  arima_boost(\n    seasonal_period = 18,\n    non_seasonal_ar = 1,\n    non_seasonal_differences = 1,\n    non_seasonal_ma = 0,\n    seasonal_ar     = 1,\n    seasonal_differences = 1,\n    seasonal_ma     = 0,\n\n    # XGBoost Args\n    tree_depth = 6,\n    learn_rate = 0.015,\n    min_n = 2) %&gt;% \n  set_engine(\"arima_xgboost\") %&gt;% \n  fit(media ~ mes + as.numeric(mes) + factor(month(mes, label = TRUE), ordered = F),\n        data = training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/arima/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "model_table &lt;- \nmodeltime_table(\n  auto_arima,\n  model_arima,\n  arima_m_season,\n  model_fit_arima_boosted\n)"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#calibrando",
    "href": "posts/serie_ouro/arima/index.html#calibrando",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#residual",
    "href": "posts/serie_ouro/arima/index.html#residual",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#accuracy",
    "href": "posts/serie_ouro/arima/index.html#accuracy",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 4 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         1 ARIMA(1,1,0)              Test  21.7  12.1   4.63 13.1   23.6 0.655 \n2         2 ARIMA(1,1,0)(1,1,0)[18]   Test   9.88  5.54  2.11  5.73  11.4 0.359 \n3         3 SEASONAL DECOMP: ARIMA(1… Test  20.1  11.2   4.28 12.0   22.0 0.0669\n4         4 ARIMA(1,1,0)(1,1,0)[18] … Test   9.87  5.53  2.11  5.72  11.4 0.364"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#test-set-visualization",
    "href": "posts/serie_ouro/arima/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/arima/index.html#forecast-future",
    "href": "posts/serie_ouro/arima/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 2",
    "section": "",
    "text": "calib_arima &lt;- \n  model_arima %&gt;% \n  modeltime_calibrate(testing(splits))\n\nfuture_forecast_tbl &lt;- \n  calib_arima %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/lstm/index.html",
    "href": "posts/serie_ouro/lstm/index.html",
    "title": "Cotação do ouro - Parte 5",
    "section": "",
    "text": "Desenvolvendo uma estratégia de Backtesting\n# Realizar o backtesting com a função rolling_origin()\nperiods_train &lt;- 12 * 4\nperiods_test  &lt;- 12 * 1\nskip_span     &lt;- 12 * 2\n\n\nrolling_origin_resamples &lt;- rsample::rolling_origin(\n    gold_mean,\n    initial    = periods_train,\n    assess     = periods_test,\n    cumulative = FALSE,\n    skip       = skip_span\n)\nrolling_origin_resamples\n\n# Rolling origin forecast resampling \n# A tibble: 3 × 2\n  splits          id    \n  &lt;list&gt;          &lt;chr&gt; \n1 &lt;split [48/12]&gt; Slice1\n2 &lt;split [48/12]&gt; Slice2\n3 &lt;split [48/12]&gt; Slice3\nCriando uma função para visualizar os blocos de Backtest\n# Plotting function for a single split\nplot_split &lt;- function(split, expand_y_axis = TRUE, alpha = 1, size = 1, base_size = 14) {\n    \n    # Manipulate data\n    train_tbl &lt;- training(split) %&gt;%\n        add_column(key = \"training\") \n    \n    test_tbl  &lt;- testing(split) %&gt;%\n        add_column(key = \"testing\") \n    \n    data_manipulated &lt;- bind_rows(train_tbl, test_tbl) %&gt;%\n        as_tbl_time(index = index) %&gt;%\n        mutate(key = fct_relevel(key, \"training\", \"testing\"))\n        \n    # Collect attributes\n    train_time_summary &lt;- train_tbl %&gt;%\n        tk_index() %&gt;%\n        tk_get_timeseries_summary()\n    \n    test_time_summary &lt;- test_tbl %&gt;%\n        tk_index() %&gt;%\n        tk_get_timeseries_summary()\n    \n    # Visualize\n    g &lt;- data_manipulated %&gt;%\n        ggplot(aes(x = index, y = value, color = key)) +\n        geom_line(linewidth = size, alpha = alpha) +\n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        labs(\n            title    = glue(\"Split: {split$id}\"),\n            subtitle = glue(\"{train_time_summary$start} to {test_time_summary$end}\"),\n            y = \"\", x = \"\"\n        ) +\n        theme(legend.position = \"none\") \n    \n    if (expand_y_axis) {\n        \n        gold_ts_time_summary &lt;- gold_mean %&gt;% \n            tk_index() %&gt;% \n            tk_get_timeseries_summary()\n        \n        g &lt;- g +\n            scale_x_date(limits = c(gold_ts_time_summary$start, \n                                    gold_ts_time_summary$end))\n    }\n    \n    return(g)\n}\nVerificando um dos blocos de Backtest\nrolling_origin_resamples$splits[[1]] %&gt;%\n    plot_split(expand_y_axis = TRUE) +\n    theme(legend.position = \"bottom\")\nFunção para colocar os gráficos na escala de tempo do recorte dos dados\n# Plotting function that scales to all splits \nplot_sampling_plan &lt;- function(sampling_tbl, expand_y_axis = TRUE, \n                               ncol = 3, alpha = 1, size = 1, base_size = 14, \n                               title = \"Sampling Plan\") {\n    \n    # Map plot_split() to sampling_tbl\n    sampling_tbl_with_plots &lt;- sampling_tbl %&gt;%\n        mutate(gg_plots = map(splits, plot_split, \n                              expand_y_axis = expand_y_axis,\n                              alpha = alpha, base_size = base_size))\n    \n    # Make plots with cowplot\n    plot_list &lt;- sampling_tbl_with_plots$gg_plots \n    \n    p_temp &lt;- plot_list[[1]] + theme(legend.position = \"bottom\")\n    legend &lt;- get_legend(p_temp)\n    \n    p_body  &lt;- plot_grid(plotlist = plot_list, ncol = ncol)\n    \n    p_title &lt;- ggdraw() + \n        draw_label(title, size = 18, fontface = \"bold\", colour = palette_light()[[1]])\n    \n    g &lt;- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n    \n    return(g)\n    \n}\nPlotando todos os blocos de teste\nrolling_origin_resamples %&gt;%\n    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Rolling Origin Sampling Plan\")\nPlotando os gráficos em uma escala mais apropriada “zoom”\nrolling_origin_resamples %&gt;%\n    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Zoomed In\")"
  },
  {
    "objectID": "posts/serie_ouro/lstm/index.html#modelagem-the-keras-stateful-lstm-model",
    "href": "posts/serie_ouro/lstm/index.html#modelagem-the-keras-stateful-lstm-model",
    "title": "Cotação do ouro - Parte 5",
    "section": "Modelagem The Keras Stateful LSTM Model",
    "text": "Modelagem The Keras Stateful LSTM Model"
  },
  {
    "objectID": "posts/serie_ouro/lstm/index.html#single-lstm",
    "href": "posts/serie_ouro/lstm/index.html#single-lstm",
    "title": "Cotação do ouro - Parte 5",
    "section": "Single LSTM",
    "text": "Single LSTM\n\nsplit    &lt;- rolling_origin_resamples$splits[[1]]\nsplit_id &lt;- rolling_origin_resamples$id[[1]]\n\n\nplot_split(split, expand_y_axis = FALSE, size = 0.5) +\n    theme(legend.position = \"bottom\") +\n    ggtitle(glue(\"Split: {split_id}\"))\n\n\n\n\nCriando um index para treino e teste e combinando em um df\n\ndf_trn &lt;- training(split)\ndf_tst &lt;- testing(split)\n\ndf &lt;- bind_rows(\n    df_trn %&gt;% add_column(key = \"training\"),\n    df_tst %&gt;% add_column(key = \"testing\")\n) %&gt;% \n    as_tbl_time(index = index)\n\ndf\n\n# A time tibble: 60 × 3\n# Index:         index\n   index      value key     \n   &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;   \n 1 2013-01-01  162. training\n 2 2013-02-01  158. training\n 3 2013-03-01  154. training\n 4 2013-04-01  144. training\n 5 2013-05-01  137. training\n 6 2013-06-01  130. training\n 7 2013-07-01  124. training\n 8 2013-08-01  131. training\n 9 2013-09-01  130. training\n10 2013-10-01  127. training\n# ℹ 50 more rows"
  },
  {
    "objectID": "posts/serie_ouro/lstm/index.html#preprocessing-with-recipes",
    "href": "posts/serie_ouro/lstm/index.html#preprocessing-with-recipes",
    "title": "Cotação do ouro - Parte 5",
    "section": "Preprocessing With Recipes",
    "text": "Preprocessing With Recipes\nThe LSTM algorithm requires the input data to be centered and scaled. We can preprocess the data using the recipes package.\n\nrec_obj &lt;- recipe(value ~ ., df) %&gt;%\n    step_sqrt(value) %&gt;%\n    step_center(value) %&gt;%\n    step_scale(value) %&gt;%\n   # step_BoxCox(value) %&gt;% \n    prep()\n\ndf_processed_tbl &lt;- bake(rec_obj, df)\n\ndf_processed_tbl\n\n# A tibble: 60 × 3\n   index      key      value\n   &lt;date&gt;     &lt;fct&gt;    &lt;dbl&gt;\n 1 2013-01-01 training 3.40 \n 2 2013-02-01 training 3.08 \n 3 2013-03-01 training 2.80 \n 4 2013-04-01 training 1.93 \n 5 2013-05-01 training 1.37 \n 6 2013-06-01 training 0.733\n 7 2013-07-01 training 0.277\n 8 2013-08-01 training 0.823\n 9 2013-09-01 training 0.783\n10 2013-10-01 training 0.500\n# ℹ 50 more rows\n\n\nA seguir, vamos capturar o histórico do centro/escala para que possamos inverter o centro e a escala após a modelagem. A transformação da raiz quadrada pode ser revertida ao elevar ao quadrado os valores invertidos do centro/escala.\n\n   center_history &lt;- rec_obj$steps[[2]]$means[\"value\"]\n   scale_history  &lt;- rec_obj$steps[[3]]$sds[\"value\"]\n\nc(\"center\" = center_history, \"scale\" = scale_history)\n\ncenter.value  scale.value \n  11.0197810    0.4985484"
  },
  {
    "objectID": "posts/serie_ouro/lstm/index.html#lstm-plan",
    "href": "posts/serie_ouro/lstm/index.html#lstm-plan",
    "title": "Cotação do ouro - Parte 5",
    "section": "LSTM Plan",
    "text": "LSTM Plan\n\n# Model inputs\nlag_setting  &lt;- 12 # = nrow(df_tst)\nbatch_size   &lt;- 1\ntrain_length &lt;- 12*6\ntsteps       &lt;- 1\nepochs       &lt;- 50\n\n\n# Training Set\nlag_train_tbl &lt;- df_processed_tbl %&gt;%\n    mutate(value_lag = lag(value, n = lag_setting)) %&gt;%\n    filter(!is.na(value_lag)) %&gt;%\n    filter(key == \"training\") %&gt;%\n    tail(train_length)\n\nx_train_vec &lt;- lag_train_tbl$value_lag\nx_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n\ny_train_vec &lt;- lag_train_tbl$value\ny_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n\n# Testing Set\nlag_test_tbl &lt;- df_processed_tbl %&gt;%\n    mutate(\n        value_lag = lag(value, n = lag_setting)\n    ) %&gt;%\n    filter(!is.na(value_lag)) %&gt;%\n    filter(key == \"testing\")\n\nx_test_vec &lt;- lag_test_tbl$value_lag\nx_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n\ny_test_vec &lt;- lag_test_tbl$value\ny_test_arr &lt;- array(data = y_test_vec, dim = c(length(y_test_vec), 1))\n\n\nConstruindo o modelo LSTM\n\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n    layer_lstm(units            = 50, \n               input_shape      = c(tsteps, 1), \n               batch_size       = batch_size,\n               return_sequences = TRUE, \n               stateful         = TRUE) %&gt;% \n     layer_lstm(units            = 50, \n                return_sequences = FALSE, \n                stateful         = TRUE) %&gt;% \n    layer_dense(units = 1)\n\nmodel %&gt;% \n    compile(\n      loss = 'mae',\n      optimizer = 'adam',\n      ) \n  \n\nmodel\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n lstm_1 (LSTM)                      (1, 1, 50)                      10400       \n lstm (LSTM)                        (1, 50)                         20200       \n dense (Dense)                      (1, 1)                          51          \n================================================================================\nTotal params: 30,651\nTrainable params: 30,651\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\nFitting The LSTM Model\n\nfor (i in 1:epochs) {\n    model %&gt;% fit(x          = x_train_arr, \n                  y          = y_train_arr, \n                  batch_size = batch_size,\n                  epochs     = 1, \n                  verbose    = 1, \n                  shuffle    = FALSE)\n    \n    model %&gt;% reset_states()\n    cat(\"Epoch: \", i)\n    \n}\n\n\n\nPredicting Using The LSTM Model\n\n# Make Predictions\npred_out &lt;- model %&gt;% \n    predict(x_test_arr, batch_size = batch_size) %&gt;%\n    .[,1] \n\n\n 1/12 [=&gt;............................] - ETA: 9s\n12/12 [==============================] - 1s 2ms/step\n\n# Retransform values\npred_tbl &lt;- tibble(\n    index   = lag_test_tbl$index,\n    value   = (pred_out * scale_history + center_history)^2\n) \n\n# Combine actual data with predictions\ntbl_1 &lt;- df_trn %&gt;%\n    add_column(key = \"actual\")\n\ntbl_2 &lt;- df_tst %&gt;%\n    add_column(key = \"actual\")\n\ntbl_3 &lt;- pred_tbl %&gt;%\n    add_column(key = \"predict\")\n\n# Create time_bind_rows() to solve dplyr issue\ntime_bind_rows &lt;- function(data_1, data_2, index) {\n    index_expr &lt;- enquo(index)\n    bind_rows(data_1, data_2) %&gt;%\n        as_tbl_time(index = !! index_expr)\n}\n\nret &lt;- list(tbl_1, tbl_2, tbl_3) %&gt;%\n    reduce(time_bind_rows, index = index) %&gt;%\n    arrange(key, index) %&gt;%\n    mutate(key = as_factor(key))\n\nret\n\n# A time tibble: 72 × 3\n# Index:         index\n   index      value key   \n   &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt; \n 1 2013-01-01  162. actual\n 2 2013-02-01  158. actual\n 3 2013-03-01  154. actual\n 4 2013-04-01  144. actual\n 5 2013-05-01  137. actual\n 6 2013-06-01  130. actual\n 7 2013-07-01  124. actual\n 8 2013-08-01  131. actual\n 9 2013-09-01  130. actual\n10 2013-10-01  127. actual\n# ℹ 62 more rows\n\n\n\n\nAssessing Performance Of The LSTM On A Single Split\n\ncalc_rmse &lt;- function(prediction_tbl) {\n    \n    rmse_calculation &lt;- function(data) {\n        data %&gt;%\n            spread(key = key, value = value) %&gt;%\n            select(-index) %&gt;%\n            filter(!is.na(predict)) %&gt;%\n            rename(\n                truth    = actual,\n                estimate = predict\n            ) %&gt;%\n            rmse(truth, estimate) %&gt;% \n      select(.estimate)\n    }\n    \n    safe_rmse &lt;- possibly(rmse_calculation, otherwise = NA)\n    \n    safe_rmse(prediction_tbl)\n        \n}\n\nverificando o RMSE\n\ncalc_rmse(ret)\n\n# A tibble: 1 × 1\n  .estimate\n      &lt;dbl&gt;\n1      3.85\n\n\n\n\nVisualizing The Single Prediction\n\n# Setup single plot function\nplot_prediction &lt;- function(data, id, alpha = 1, size = 2, base_size = 14) {\n    \n    rmse_val &lt;- calc_rmse(data)\n    \n    g &lt;- data %&gt;%\n        ggplot(aes(index, value, color = key)) +\n        geom_point(alpha = alpha, size = size) + \n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        theme(legend.position = \"none\") +\n        labs(\n            title = glue(\"{id}, RMSE: {round(rmse_val, digits = 1)}\"),\n            x = \"\", y = \"\"\n        )\n    \n    return(g)\n}\n\n\nret %&gt;% \n    plot_prediction(id = split_id, alpha = 0.65) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n# trend &lt;- fit %&gt;% \n#    filter(index &gt;=  \"2023-01-08\", index &lt;= \"2023-02-06\") %&gt;% \n#    select(trend) %&gt;% \n#    tibble()\n# \n# \n# predito &lt;- ret %&gt;% \n#   filter(key==\"predict\") %&gt;% \n#   mutate(\n#     value = value + trend\n#   )\n# \n# original &lt;- gold_original %&gt;% \n#   filter(index &gt;=  \"2023-01-08\", index &lt;= \"2023-02-06\")\n# \n# left_join(original, predito, by = \"index\") %&gt;% \n#   ggplot(aes(x=index))+\n#   geom_line(aes(y=value.x))+\n#   geom_line(aes(y=value.y$trend), color=\"blue\")\n\n\n\nBacktesting The LSTM On All Samples Creating An LSTM Prediction Function\n\npredict_keras_lstm &lt;- function(split, epochs = 50, ...) {\n    \n    lstm_prediction &lt;- function(split, epochs, ...) {\n        \n        # 5.1.2 Data Setup\n        df_trn &lt;- training(split)\n        df_tst &lt;- testing(split)\n        \n        df &lt;- bind_rows(\n            df_trn %&gt;% add_column(key = \"training\"),\n            df_tst %&gt;% add_column(key = \"testing\")\n        ) %&gt;% \n            as_tbl_time(index = index)\n        \n        # 5.1.3 Preprocessing\n        rec_obj &lt;- recipe(value ~ ., df) %&gt;%\n            step_sqrt(value) %&gt;%\n            step_center(value) %&gt;%\n            step_scale(value) %&gt;%\n            prep()\n        \n        df_processed_tbl &lt;- bake(rec_obj, df)\n        \n        center_history &lt;- rec_obj$steps[[2]]$means[\"value\"]\n        scale_history  &lt;- rec_obj$steps[[3]]$sds[\"value\"]\n        \n        # 5.1.4 LSTM Plan\n        lag_setting  &lt;- 12 # = nrow(df_tst)\n        batch_size   &lt;- 1\n        train_length &lt;- 12*5\n        tsteps       &lt;- 1\n        epochs       &lt;- epochs\n        \n        # 5.1.5 Train/Test Setup\n        lag_train_tbl &lt;- df_processed_tbl %&gt;%\n            mutate(value_lag = lag(value, n = lag_setting)) %&gt;%\n            filter(!is.na(value_lag)) %&gt;%\n            filter(key == \"training\") %&gt;%\n            tail(train_length)\n        \n        x_train_vec &lt;- lag_train_tbl$value_lag\n        x_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n        \n        y_train_vec &lt;- lag_train_tbl$value\n        y_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n        \n        lag_test_tbl &lt;- df_processed_tbl %&gt;%\n            mutate(\n                value_lag = lag(value, n = lag_setting)\n            ) %&gt;%\n            filter(!is.na(value_lag)) %&gt;%\n            filter(key == \"testing\")\n        \n        x_test_vec &lt;- lag_test_tbl$value_lag\n        x_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n        \n        y_test_vec &lt;- lag_test_tbl$value\n        y_test_arr &lt;- array(data = y_test_vec, dim = c(length(y_test_vec), 1))\n                \n        # 5.1.6 LSTM Model\n        model &lt;- keras_model_sequential()\n\n        model %&gt;%\n            layer_lstm(units            = 50, \n                       input_shape      = c(tsteps, 1), \n                       batch_size       = batch_size,\n                       return_sequences = TRUE, \n                       stateful         = TRUE) %&gt;% \n            layer_lstm(units            = 50, \n                       return_sequences = FALSE, \n                       stateful         = TRUE) %&gt;% \n            layer_dense(units = 1)\n        \n        model %&gt;% \n            compile(loss = 'mae', optimizer = 'adam')\n        \n        # 5.1.7 Fitting LSTM\n        for (i in 1:epochs) {\n            model %&gt;% fit(x          = x_train_arr, \n                          y          = y_train_arr, \n                          batch_size = batch_size,\n                          epochs     = 1, \n                          verbose    = 1, \n                          shuffle    = FALSE)\n            \n            model %&gt;% reset_states()\n            cat(\"Epoch: \", i)\n            \n        }\n        \n        # 5.1.8 Predict and Return Tidy Data\n        # Make Predictions\n        pred_out &lt;- model %&gt;% \n            predict(x_test_arr, batch_size = batch_size) %&gt;%\n            .[,1] \n        \n        # Retransform values\n        pred_tbl &lt;- tibble(\n            index   = lag_test_tbl$index,\n            value   = (pred_out * scale_history + center_history)^2\n        ) \n        \n        # Combine actual data with predictions\n        tbl_1 &lt;- df_trn %&gt;%\n            add_column(key = \"actual\")\n        \n        tbl_2 &lt;- df_tst %&gt;%\n            add_column(key = \"actual\")\n        \n        tbl_3 &lt;- pred_tbl %&gt;%\n            add_column(key = \"predict\")\n        \n        # Create time_bind_rows() to solve dplyr issue\n        time_bind_rows &lt;- function(data_1, data_2, index) {\n            index_expr &lt;- enquo(index)\n            bind_rows(data_1, data_2) %&gt;%\n                as_tbl_time(index = !! index_expr)\n        }\n        \n        ret &lt;- list(tbl_1, tbl_2, tbl_3) %&gt;%\n            reduce(time_bind_rows, index = index) %&gt;%\n            arrange(key, index) %&gt;%\n            mutate(key = as_factor(key))\n\n        return(ret)\n        \n    }\n    \n    safe_lstm &lt;- possibly(lstm_prediction, otherwise = NA)\n    \n    safe_lstm(split, epochs, ...)\n    \n}\n\ntestando\n\n#predict_keras_lstm(split, epochs = 10)\n\n\n\nMapping The LSTM Prediction Function Over The Samples\n\nsample_predictions_lstm_tbl &lt;- rolling_origin_resamples %&gt;%\n     mutate(predict = map(splits, predict_keras_lstm, epochs = 50))\n\n\nsample_predictions_lstm_tbl\n\n# Rolling origin forecast resampling \n# A tibble: 3 × 3\n  splits          id     predict            \n  &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;             \n1 &lt;split [48/12]&gt; Slice1 &lt;tbl_time [72 × 3]&gt;\n2 &lt;split [48/12]&gt; Slice2 &lt;tbl_time [72 × 3]&gt;\n3 &lt;split [48/12]&gt; Slice3 &lt;tbl_time [72 × 3]&gt;\n\n\n\n\nAssessing The Backtested Performance\n\nsample_rmse_tbl &lt;- sample_predictions_lstm_tbl %&gt;%\n    mutate(rmse = map_df(predict, calc_rmse)) %&gt;%\n    select(id, rmse) \n\nsample_rmse_tbl\n\n# A tibble: 3 × 2\n  id     rmse$.estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Slice1           4.04\n2 Slice2          14.7 \n3 Slice3          54.1 \n\n\n\nsample_rmse_tbl %&gt;%\n    ggplot(aes(rmse$.estimate)) +\n    geom_histogram(aes(y = ..density..), fill = palette_light()[[1]], bins = 16) +\n    geom_density(fill = palette_light()[[1]], alpha = 0.5) +\n    theme_tq() +\n    ggtitle(\"Histogram of RMSE\")\n\n\n\n\n\nsample_rmse_tbl$rmse %&gt;%\n    summarize(\n        mean_rmse = mean(.estimate),\n        sd_rmse   = sd(.estimate)\n    )\n\n# A tibble: 1 × 2\n  mean_rmse sd_rmse\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      24.3    26.4\n\n\nVisualizing The Backtest Results\n\nplot_predictions &lt;- function(sampling_tbl, predictions_col, \n                             ncol = 3, alpha = 1, size = 2, base_size = 14,\n                             title = \"Backtested Predictions\") {\n    \n    predictions_col_expr &lt;- enquo(predictions_col)\n    \n    # Map plot_split() to sampling_tbl\n    sampling_tbl_with_plots &lt;- sampling_tbl %&gt;%\n        mutate(gg_plots = map2(!! predictions_col_expr, id, \n                               .f        = plot_prediction, \n                               alpha     = alpha, \n                               size      = size, \n                               base_size = base_size)) \n    \n    # Make plots with cowplot\n    plot_list &lt;- sampling_tbl_with_plots$gg_plots \n    \n    p_temp &lt;- plot_list[[1]] + theme(legend.position = \"bottom\")\n    legend &lt;- get_legend(p_temp)\n    \n    p_body  &lt;- plot_grid(plotlist = plot_list, ncol = ncol)\n    \n    \n    \n    p_title &lt;- ggdraw() + \n        draw_label(title, size = 18, fontface = \"bold\", colour = palette_light()[[1]])\n    \n    g &lt;- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n    \n    return(g)\n    \n}\n\n\nsample_predictions_lstm_tbl %&gt;%\n    plot_predictions(predictions_col = predict, alpha = 0.5, size = 1, base_size = 10,\n                     title = \"Keras Stateful LSTM: Backtested Predictions\")\n\n\n\n\n\n\nPredicting The Next 10 Years\n\npredict_keras_lstm_future &lt;- function(data, epochs = 300, ...) {\n    \n    lstm_prediction &lt;- function(data, epochs, ...) {\n        \n        # 5.1.2 Data Setup (MODIFIED)\n        df &lt;- data\n        \n        # 5.1.3 Preprocessing\n        rec_obj &lt;- recipe(value ~ ., df) %&gt;%\n            step_sqrt(value) %&gt;%\n            step_center(value) %&gt;%\n            step_scale(value) %&gt;%\n            prep()\n        \n        df_processed_tbl &lt;- bake(rec_obj, df)\n        \n        center_history &lt;- rec_obj$steps[[2]]$means[\"value\"]\n        scale_history  &lt;- rec_obj$steps[[3]]$sds[\"value\"]\n        \n        # 5.1.4 LSTM Plan\n        lag_setting  &lt;- 12 # = nrow(df_tst)\n        batch_size   &lt;- 1\n        train_length &lt;- 12*3\n        tsteps       &lt;- 1\n        epochs       &lt;- epochs\n        \n        # 5.1.5 Train Setup (MODIFIED)\n        lag_train_tbl &lt;- df_processed_tbl %&gt;%\n            mutate(value_lag = lag(value, n = lag_setting)) %&gt;%\n            filter(!is.na(value_lag)) %&gt;%\n            tail(train_length)\n        \n        x_train_vec &lt;- lag_train_tbl$value_lag\n        x_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n        \n        y_train_vec &lt;- lag_train_tbl$value\n        y_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n        \n        x_test_vec &lt;- y_train_vec %&gt;% tail(lag_setting)\n        x_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n                \n        # 5.1.6 LSTM Model\n        model &lt;- keras_model_sequential()\n\n        model %&gt;%\n            layer_lstm(units            = 50, \n                       input_shape      = c(tsteps, 1), \n                       batch_size       = batch_size,\n                       return_sequences = TRUE, \n                       stateful         = TRUE) %&gt;% \n            layer_lstm(units            = 50, \n                       return_sequences = FALSE, \n                       stateful         = TRUE) %&gt;% \n            layer_dense(units = 1)\n        \n        model %&gt;% \n            compile(loss = 'mae', optimizer = 'adam')\n        \n        # 5.1.7 Fitting LSTM\n        for (i in 1:epochs) {\n            model %&gt;% fit(x          = x_train_arr, \n                          y          = y_train_arr, \n                          batch_size = batch_size,\n                          epochs     = 1, \n                          verbose    = 1, \n                          shuffle    = FALSE)\n            \n            model %&gt;% reset_states()\n            cat(\"Epoch: \", i)\n            \n        }\n        \n        # 5.1.8 Predict and Return Tidy Data (MODIFIED)\n        # Make Predictions\n        pred_out &lt;- model %&gt;% \n            predict(x_test_arr, batch_size = batch_size) %&gt;%\n            .[,1] \n        \n        # Make future index using tk_make_future_timeseries()\n        idx &lt;- data %&gt;%\n            tk_index() %&gt;%\n            tk_make_future_timeseries(length_out = lag_setting)\n        \n        # Retransform values\n        pred_tbl &lt;- tibble(\n            index   = idx,\n            value   = (pred_out * scale_history + center_history)^2\n        )\n        \n        # Combine actual data with predictions\n        tbl_1 &lt;- df %&gt;%\n            add_column(key = \"actual\")\n\n        tbl_3 &lt;- pred_tbl %&gt;%\n            add_column(key = \"predict\")\n\n        # Create time_bind_rows() to solve dplyr issue\n        time_bind_rows &lt;- function(data_1, data_2, index) {\n            index_expr &lt;- enquo(index)\n            bind_rows(data_1, data_2) %&gt;%\n                as_tbl_time(index = !! index_expr)\n        }\n\n        ret &lt;- list(tbl_1, tbl_3) %&gt;%\n            reduce(time_bind_rows, index = index) %&gt;%\n            arrange(key, index) %&gt;%\n            mutate(key = as_factor(key))\n\n        return(ret)\n        \n    }\n    \n    safe_lstm &lt;- possibly(lstm_prediction, otherwise = NA)\n    \n    safe_lstm(data, epochs, ...)\n    \n}\n\n\nfuture_gold_ts_tbl &lt;- predict_keras_lstm_future(gold_mean, epochs = 300)\n\n\nfuture_gold_ts_tbl %&gt;%\n    filter_time(\"2013\" ~ \"end\") %&gt;%\n    plot_prediction(id = NULL, alpha = 0.4, size = 1.5) +\n    theme(legend.position = \"bottom\") +\n    ggtitle(\"Sunspots: Ten Year Forecast\", subtitle = \"Forecast Horizon: 2013 - 2023\")"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html",
    "href": "posts/serie_ouro/prophet/index.html",
    "title": "Cotação do ouro - Parte 4",
    "section": "",
    "text": "splits &lt;-\n   gold_mean %&gt;% \n   time_series_split(\n     assess = \"1 year\",\n     cumulative = TRUE\n     )\n\ncv &lt;- \n  gold_mean %&gt;% \n  time_series_cv(\n    assess      = 12 * 0.5,\n    initial     = 12 * 4,\n    skip        = 12 * 1,\n    slice_limit = 6,\n    culmulatime = FALSE\n  )\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 1, .interactive = FALSE)\ncv %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#modelos",
    "href": "posts/serie_ouro/prophet/index.html#modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Modelos",
    "text": "Modelos\n\nprothet_mod &lt;- \n  prophet_reg(\n    prior_scale_seasonality = 10\n  ) %&gt;% \n  set_engine(\"prophet\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\nprothet_boost &lt;- \n  prophet_boost() %&gt;% \n  set_engine(\"prophet_xgboost\") %&gt;% \n  fit(media ~ mes + as.numeric(mes) + factor(month(mes, label = TRUE), ordered = F),\n        data = training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/prophet/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nmodel_table &lt;- \nmodeltime_table(\n  prothet_mod,\n  prothet_boost\n  )"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#calibrando",
    "href": "posts/serie_ouro/prophet/index.html#calibrando",
    "title": "Cotação do ouro - Parte 4",
    "section": "Calibrando",
    "text": "Calibrando\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#residual",
    "href": "posts/serie_ouro/prophet/index.html#residual",
    "title": "Cotação do ouro - Parte 4",
    "section": "Residual",
    "text": "Residual\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#accuracy",
    "href": "posts/serie_ouro/prophet/index.html#accuracy",
    "title": "Cotação do ouro - Parte 4",
    "section": "Accuracy",
    "text": "Accuracy\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 2 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET                   Test  13.0   7.64  2.82  7.28  14.4 0.635\n2         2 PROPHET W/ XGBOOST ERRORS Test   9.38  5.28  2.03  5.43  10.4 0.635"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#test-set-visualization",
    "href": "posts/serie_ouro/prophet/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 4",
    "section": "Test set visualization",
    "text": "Test set visualization\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#validação-cruzada",
    "href": "posts/serie_ouro/prophet/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 4",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 2 × 4\n  .model_id .model   .model_desc               .resample_results\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                     &lt;list&gt;           \n1         1 &lt;fit[+]&gt; PROPHET                   &lt;rsmp[+]&gt;        \n2         2 &lt;fit[+]&gt; PROPHET W/ XGBOOST ERRORS &lt;lgl [1]&gt;        \n\n\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nPROPHET\nResamples\n6\n14.58\n9.26\n4.25\n9.22\n15.95\n0.63"
  },
  {
    "objectID": "posts/serie_ouro/prophet/index.html#forecast-future",
    "href": "posts/serie_ouro/prophet/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 4",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  calib_table %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html",
    "href": "posts/serie_ouro/stl/index.html",
    "title": "Cotação do ouro - Parte 3",
    "section": "",
    "text": "splits &lt;-\n   gold_mean %&gt;% \n   time_series_split(\n     assess = \"1 year\",\n     cumulative = TRUE\n     )\n\ncv &lt;- \n  gold_mean %&gt;% \n  time_series_cv(\n    assess      = 12 * 0.5,\n    initial     = 12 * 4,\n    skip        = 12 * 1,\n    slice_limit = 6,\n    culmulatime = FALSE\n  )\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 1, .interactive = FALSE)\ncv %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#modelos",
    "href": "posts/serie_ouro/stl/index.html#modelos",
    "title": "Cotação do ouro - Parte 3",
    "section": "Modelos",
    "text": "Modelos\n\nets_mod &lt;- seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n  ) %&gt;% \n  set_engine(\"stlm_ets\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\ntbats_mod &lt;- seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n    ) %&gt;% \n  set_engine(\"tbats\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/stl/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 3",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nmodel_table &lt;- \nmodeltime_table(\n  ets_mod,\n  tbats_mod\n  )"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#calibrando",
    "href": "posts/serie_ouro/stl/index.html#calibrando",
    "title": "Cotação do ouro - Parte 3",
    "section": "Calibrando",
    "text": "Calibrando\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#residual",
    "href": "posts/serie_ouro/stl/index.html#residual",
    "title": "Cotação do ouro - Parte 3",
    "section": "Residual",
    "text": "Residual\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#accuracy",
    "href": "posts/serie_ouro/stl/index.html#accuracy",
    "title": "Cotação do ouro - Parte 3",
    "section": "Accuracy",
    "text": "Accuracy\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 2 × 9\n  .model_id .model_desc              .type   mae  mape  mase smape  rmse     rsq\n      &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1         1 SEASONAL DECOMP: ETS(M,… Test   21.8  12.2  4.73  13.2  23.9 0.00338\n2         2 BATS(0.103, {0,0}, 0.92… Test   20.0  11.2  4.33  12.0  21.9 0.689"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#test-set-visualization",
    "href": "posts/serie_ouro/stl/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 3",
    "section": "Test set visualization",
    "text": "Test set visualization\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#validação-cruzada",
    "href": "posts/serie_ouro/stl/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 3",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 2 × 4\n  .model_id .model   .model_desc                  .resample_results\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                        &lt;list&gt;           \n1         1 &lt;fit[+]&gt; SEASONAL DECOMP: ETS(M,AD,N) &lt;rsmp[+]&gt;        \n2         2 &lt;fit[+]&gt; BATS(0.103, {0,0}, 0.929, -) &lt;rsmp[+]&gt;        \n\n\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nSEASONAL DECOMP: ETS(M,AD,N)\nResamples\n6\n9.17\n5.69\n2.25\n5.69\n10.66\n0.55\n    2\nBATS(0.103, {0,0}, 0.929, -)\nResamples\n6\n10.54\n6.56\n2.38\n6.64\n11.83\nNA"
  },
  {
    "objectID": "posts/serie_ouro/stl/index.html#forecast-future",
    "href": "posts/serie_ouro/stl/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 3",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  calib_table %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html",
    "href": "posts/serie_ouro/exp/index.html",
    "title": "Cotação do ouro - Parte 4",
    "section": "",
    "text": "splits &lt;-\n   gold_mean %&gt;% \n   time_series_split(\n     assess = \"1 year\",\n     cumulative = TRUE\n     )\n\ncv &lt;- \n  gold_mean %&gt;% \n  time_series_cv(\n    assess      = 12 * 0.5,\n    initial     = 12 * 4,\n    skip        = 12 * 1,\n    slice_limit = 6,\n    culmulatime = FALSE\n  )\nsplits %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 1, .interactive = FALSE)\ncv %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#modelos",
    "href": "posts/serie_ouro/exp/index.html#modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Modelos",
    "text": "Modelos\n\nexp_ets &lt;- \n exp_smoothing(\n   seasonal_period  = 12,\n   error            = \"multiplicative\",\n   trend            = \"additive\",\n   season           = \"multiplicative\"\n ) %&gt;% \n  set_engine(\"ets\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\nexp_croston &lt;- \n exp_smoothing(\n   smooth_level = 0.2\n ) %&gt;% \n  set_engine(\"croston\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\nexp_theta &lt;- \n exp_smoothing() %&gt;% \n  set_engine(\"theta\") %&gt;% \n  fit(media ~ mes, training(splits))\n\n\nexp_smooth &lt;- \n exp_smoothing(\n   seasonal_period  = 12,\n   error            = \"multiplicative\",\n   trend            = \"additive_damped\",\n   season           = \"additive\"\n ) %&gt;% \n  set_engine(\"smooth_es\") %&gt;% \n  fit(media ~ mes, training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/exp/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nmodel_table &lt;- \nmodeltime_table(\n  exp_ets,\n  exp_croston,\n  exp_theta,\n  exp_smooth\n  )"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#calibrando",
    "href": "posts/serie_ouro/exp/index.html#calibrando",
    "title": "Cotação do ouro - Parte 4",
    "section": "Calibrando",
    "text": "Calibrando\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#residual",
    "href": "posts/serie_ouro/exp/index.html#residual",
    "title": "Cotação do ouro - Parte 4",
    "section": "Residual",
    "text": "Residual\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#accuracy",
    "href": "posts/serie_ouro/exp/index.html#accuracy",
    "title": "Cotação do ouro - Parte 4",
    "section": "Accuracy",
    "text": "Accuracy\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 4 × 9\n  .model_id .model_desc    .type   mae  mape  mase smape  rmse     rsq\n      &lt;int&gt; &lt;chr&gt;          &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1         1 ETS(M,A,M)     Test   22.9 12.8   5.02 13.8   24.8  0.0431\n2         2 CROSTON METHOD Test   11.1  6.27  2.44  6.45  12.2 NA     \n3         3 THETA METHOD   Test   17.3  9.68  3.81 10.3   19.1  0.582 \n4         4 ETS(MADA)      Test   25.5 14.3   5.60 15.6   27.3  0.115"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#test-set-visualization",
    "href": "posts/serie_ouro/exp/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 4",
    "section": "Test set visualization",
    "text": "Test set visualization\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#validação-cruzada",
    "href": "posts/serie_ouro/exp/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 4",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 4 × 4\n  .model_id .model   .model_desc    .resample_results\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;          &lt;list&gt;           \n1         1 &lt;fit[+]&gt; ETS(M,A,M)     &lt;rsmp[+]&gt;        \n2         2 &lt;fit[+]&gt; CROSTON METHOD &lt;rsmp[+]&gt;        \n3         3 &lt;fit[+]&gt; THETA METHOD   &lt;rsmp[+]&gt;        \n4         4 &lt;fit[+]&gt; ETS(MADA)      &lt;rsmp[+]&gt;        \n\n\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nETS(M,A,M)\nResamples\n6\n11.77\n7.53\n2.80\n7.53\n13.25\n0.54\n    2\nCROSTON METHOD\nResamples\n6\n11.02\n6.87\n2.82\n7.23\n12.41\nNA\n    3\nTHETA METHOD\nResamples\n6\n10.31\n6.59\n2.38\n6.65\n12.02\n0.77\n    4\nETS(MADA)\nResamples\n6\n14.37\n9.08\n3.43\n8.99\n16.00\n0.58"
  },
  {
    "objectID": "posts/serie_ouro/exp/index.html#forecast-future",
    "href": "posts/serie_ouro/exp/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 4",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  calib_table %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#modelos",
    "href": "posts/serie_ouro/lm_mars/index.html#modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Modelos",
    "text": "Modelos\n\nTrend: Modeled using as.numeric(date)\nSeasonal: Modeled using month(date)\n\n\nlm_model &lt;- \nmodel_fit_lm &lt;- linear_reg() %&gt;%\n    set_engine(\"lm\") %&gt;%\n    fit(media ~ as.numeric(mes) + factor(month(mes, label = TRUE), ordered = FALSE),\n        data = training(splits))\n\n\nmars_model &lt;- \n # receita do modelo\nmodel_spec_mars &lt;- mars(mode = \"regression\") %&gt;%\n    set_engine(\"earth\") \n#receita para preparar os dados\nrecipe_spec &lt;- recipe(media ~ mes, data = training(splits)) %&gt;%\n    step_date(mes, features = \"month\", ordinal = FALSE) %&gt;%\n    step_mutate(date_num = as.numeric(mes)) %&gt;%\n    step_normalize(date_num) %&gt;%\n    step_rm(mes)\n\n#colocando as receitas em um fluxo de trabalho e ajustando o modelo  \nwflw_fit_mars &lt;- workflow() %&gt;%\n    add_recipe(recipe_spec) %&gt;%\n    add_model(model_spec_mars) %&gt;%\n    fit(training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/lm_mars/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nmodel_table &lt;- \nmodeltime_table(\n  lm_model,\n  wflw_fit_mars\n  )"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#calibrando",
    "href": "posts/serie_ouro/lm_mars/index.html#calibrando",
    "title": "Cotação do ouro - Parte 4",
    "section": "Calibrando",
    "text": "Calibrando\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#residual",
    "href": "posts/serie_ouro/lm_mars/index.html#residual",
    "title": "Cotação do ouro - Parte 4",
    "section": "Residual",
    "text": "Residual\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#accuracy",
    "href": "posts/serie_ouro/lm_mars/index.html#accuracy",
    "title": "Cotação do ouro - Parte 4",
    "section": "Accuracy",
    "text": "Accuracy\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 2 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 LM          Test   10.5  5.89  2.30  6.09  11.5 0.624\n2         2 EARTH       Test   13.3  7.49  2.92  7.79  14.6 0.580"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#test-set-visualization",
    "href": "posts/serie_ouro/lm_mars/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 4",
    "section": "Test set visualization",
    "text": "Test set visualization\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#validação-cruzada",
    "href": "posts/serie_ouro/lm_mars/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 4",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 2 × 4\n  .model_id .model     .model_desc .resample_results\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;       &lt;list&gt;           \n1         1 &lt;fit[+]&gt;   LM          &lt;lgl [1]&gt;        \n2         2 &lt;workflow&gt; EARTH       &lt;rsmp[+]&gt;        \n\n\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    2\nEARTH\nResamples\n6\n12.06\n7.96\n3.48\n8.03\n13.76\nNA"
  },
  {
    "objectID": "posts/serie_ouro/lm_mars/index.html#forecast-future",
    "href": "posts/serie_ouro/lm_mars/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 4",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  calib_table %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#modelos",
    "href": "posts/serie_ouro/nnetar/index.html#modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Modelos",
    "text": "Modelos\n\nTrend: Modeled using as.numeric(date)\nSeasonal: Modeled using month(date)\n\n\nlm_model &lt;- \nmodel_fit_lm &lt;- linear_reg() %&gt;%\n    set_engine(\"lm\") %&gt;%\n    fit(media ~ as.numeric(mes) + factor(month(mes, label = TRUE), ordered = FALSE),\n        data = training(splits))\n\n\nmars_model &lt;- \n # receita do modelo\nmodel_spec_mars &lt;- mars(mode = \"regression\") %&gt;%\n    set_engine(\"earth\") \n#receita para preparar os dados\nrecipe_spec &lt;- recipe(media ~ mes, data = training(splits)) %&gt;%\n    step_date(mes, features = \"month\", ordinal = FALSE) %&gt;%\n    step_mutate(date_num = as.numeric(mes)) %&gt;%\n    step_normalize(date_num) %&gt;%\n    step_rm(mes)\n\n#colocando as receitas em um fluxo de trabalho e ajustando o modelo  \nwflw_fit_mars &lt;- workflow() %&gt;%\n    add_recipe(recipe_spec) %&gt;%\n    add_model(model_spec_mars) %&gt;%\n    fit(training(splits))"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/nnetar/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 4",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nmodel_table &lt;- \nmodeltime_table(\n  lm_model,\n  wflw_fit_mars\n  )"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#calibrando",
    "href": "posts/serie_ouro/nnetar/index.html#calibrando",
    "title": "Cotação do ouro - Parte 4",
    "section": "Calibrando",
    "text": "Calibrando\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(splits))"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#residual",
    "href": "posts/serie_ouro/nnetar/index.html#residual",
    "title": "Cotação do ouro - Parte 4",
    "section": "Residual",
    "text": "Residual\n\n# calib_table %&gt;% \n#   modeltime_residuals() %&gt;% \n#   plot_modeltime_residuals(\n#     .type = \"seasonality\",\n#     .interactive = FALSE\n#     )"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#accuracy",
    "href": "posts/serie_ouro/nnetar/index.html#accuracy",
    "title": "Cotação do ouro - Parte 4",
    "section": "Accuracy",
    "text": "Accuracy\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 2 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 LM          Test   10.5  5.89  2.30  6.09  11.5 0.624\n2         2 EARTH       Test   13.3  7.49  2.92  7.79  14.6 0.580"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#test-set-visualization",
    "href": "posts/serie_ouro/nnetar/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 4",
    "section": "Test set visualization",
    "text": "Test set visualization\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#validação-cruzada",
    "href": "posts/serie_ouro/nnetar/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 4",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 2 × 4\n  .model_id .model     .model_desc .resample_results\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;       &lt;list&gt;           \n1         1 &lt;fit[+]&gt;   LM          &lt;lgl [1]&gt;        \n2         2 &lt;workflow&gt; EARTH       &lt;rsmp[+]&gt;        \n\n\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    2\nEARTH\nResamples\n6\n12.06\n7.96\n3.48\n8.03\n13.76\nNA"
  },
  {
    "objectID": "posts/serie_ouro/nnetar/index.html#forecast-future",
    "href": "posts/serie_ouro/nnetar/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 4",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  calib_table %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#tuning",
    "href": "posts/serie_ouro/svm/index.html#tuning",
    "title": "Cotação do ouro - Parte 4",
    "section": "Tuning",
    "text": "Tuning\nIdentificando o número de cores disponíveis\n\nparallel::detectCores(logical = TRUE)\n\n[1] 4\n\n\n\nparallel_start(4, .method = \"parallel\")"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#recipe",
    "href": "posts/serie_ouro/svm/index.html#recipe",
    "title": "Cotação do ouro - Parte 4",
    "section": "Recipe",
    "text": "Recipe\n\nrecipe_spec_1 &lt;- recipe(media ~ ., data = training(splits)) %&gt;%\n  step_timeseries_signature(mes) %&gt;%\n  step_rm(mes) %&gt;%\n  step_nzv(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n \n\nrecipe_spec_1 %&gt;%  prep() %&gt;% bake(new_data=NULL) \n\n# A tibble: 117 × 20\n   media mes_index.num mes_year mes_year.iso mes_half mes_quarter mes_month\n   &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1  162.         -1.71    -1.55        -1.55   -0.971      -1.32     -1.57 \n 2  158.         -1.68    -1.55        -1.55   -0.971      -1.32     -1.28 \n 3  154.         -1.65    -1.55        -1.55   -0.971      -1.32     -0.986\n 4  144.         -1.62    -1.55        -1.55   -0.971      -0.416    -0.695\n 5  137.         -1.59    -1.55        -1.55   -0.971      -0.416    -0.404\n 6  130.         -1.56    -1.55        -1.55   -0.971      -0.416    -0.112\n 7  124.         -1.53    -1.55        -1.55    1.02        0.485     0.179\n 8  131.         -1.50    -1.55        -1.55    1.02        0.485     0.471\n 9  130.         -1.47    -1.55        -1.55    1.02        0.485     0.762\n10  127.         -1.44    -1.55        -1.55    1.02        1.39      1.05 \n# ℹ 107 more rows\n# ℹ 13 more variables: mes_month.xts &lt;dbl&gt;, mes_month.lbl &lt;ord&gt;,\n#   mes_wday &lt;dbl&gt;, mes_wday.xts &lt;dbl&gt;, mes_wday.lbl &lt;ord&gt;, mes_qday &lt;dbl&gt;,\n#   mes_yday &lt;dbl&gt;, mes_mweek &lt;dbl&gt;, mes_week &lt;dbl&gt;, mes_week.iso &lt;dbl&gt;,\n#   mes_week2 &lt;dbl&gt;, mes_week3 &lt;dbl&gt;, mes_week4 &lt;dbl&gt;"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#model-specifications",
    "href": "posts/serie_ouro/svm/index.html#model-specifications",
    "title": "Cotação do ouro - Parte 4",
    "section": "Model Specifications",
    "text": "Model Specifications\n\nmodel_tbl &lt;- tibble(\n  cost = c(0.001, 0.010, 0.100, 1, 10, 100),\n  #rbf_sigma = c(0.010, 0.020, 0.030, 0.040, 0.05, 0.06)\n) %&gt;%\n  create_model_grid(\n    f_model_spec = svm_rbf,\n    engine_name  = \"kernlab\",\n    mode         = \"regression\"\n  )\n\nmodel_tbl\n\n# A tibble: 6 × 2\n     cost .models  \n    &lt;dbl&gt; &lt;list&gt;   \n1   0.001 &lt;spec[+]&gt;\n2   0.01  &lt;spec[+]&gt;\n3   0.1   &lt;spec[+]&gt;\n4   1     &lt;spec[+]&gt;\n5  10     &lt;spec[+]&gt;\n6 100     &lt;spec[+]&gt;\n\n\n##Extracting the model list\n\nmodel_list &lt;- model_tbl$.models\n\nmodel_list\n\n[[1]]\nRadial Basis Function Support Vector Machine Model Specification (regression)\n\nMain Arguments:\n  cost = 0.001\n\nComputational engine: kernlab \n\n\n[[2]]\nRadial Basis Function Support Vector Machine Model Specification (regression)\n\nMain Arguments:\n  cost = 0.01\n\nComputational engine: kernlab \n\n\n[[3]]\nRadial Basis Function Support Vector Machine Model Specification (regression)\n\nMain Arguments:\n  cost = 0.1\n\nComputational engine: kernlab \n\n\n[[4]]\nRadial Basis Function Support Vector Machine Model Specification (regression)\n\nMain Arguments:\n  cost = 1\n\nComputational engine: kernlab \n\n\n[[5]]\nRadial Basis Function Support Vector Machine Model Specification (regression)\n\nMain Arguments:\n  cost = 10\n\nComputational engine: kernlab \n\n\n[[6]]\nRadial Basis Function Support Vector Machine Model Specification (regression)\n\nMain Arguments:\n  cost = 100\n\nComputational engine: kernlab"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#workflowsets",
    "href": "posts/serie_ouro/svm/index.html#workflowsets",
    "title": "Cotação do ouro - Parte 4",
    "section": "Workflowsets",
    "text": "Workflowsets\n\nmodel_wfset &lt;- workflow_set(\n  preproc = list(\n    recipe_spec_1\n  ),\n  models = model_list, \n  cross = TRUE\n)\n\nmodel_wfset\n\n# A workflow set/tibble: 6 × 4\n  wflow_id         info             option    result    \n  &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_svm_rbf_1 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_svm_rbf_2 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 recipe_svm_rbf_3 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_svm_rbf_4 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 recipe_svm_rbf_5 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 recipe_svm_rbf_6 &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#parallel-training-fitting",
    "href": "posts/serie_ouro/svm/index.html#parallel-training-fitting",
    "title": "Cotação do ouro - Parte 4",
    "section": "Parallel Training (Fitting)",
    "text": "Parallel Training (Fitting)\n\ncontrol_fit_workflowset(\n  verbose   = TRUE,\n  allow_par = TRUE\n)\n\nworkflowset control object\n--------------------------\nallow_par : TRUE \ncores     : 4 \nverbose   : TRUE \npackages  : modeltime parsnip workflows dplyr stats lubridate tidymodels timetk rsample recipes yardstick dials tune modeltime.resample xgboost workflowsets modeldata infer scales broom tidyquant quantmod TTR PerformanceAnalytics xts zoo forcats stringr purrr readr tidyr tibble ggplot2 tidyverse graphics grDevices utils datasets methods base"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#fitting-using-parallel-backend",
    "href": "posts/serie_ouro/svm/index.html#fitting-using-parallel-backend",
    "title": "Cotação do ouro - Parte 4",
    "section": "Fitting Using Parallel Backend",
    "text": "Fitting Using Parallel Backend\n\nmodel_parallel_tbl &lt;- model_wfset %&gt;%\n  modeltime_fit_workflowset(\n    data    = training(splits),\n    control = control_fit_workflowset(\n      verbose   = TRUE,\n      allow_par = TRUE\n    )\n  )\n\n\nmodel_parallel_tbl\n\n# Modeltime Table\n# A tibble: 6 × 3\n  .model_id .model     .model_desc     \n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;           \n1         1 &lt;workflow&gt; RECIPE_SVM_RBF_1\n2         2 &lt;workflow&gt; RECIPE_SVM_RBF_2\n3         3 &lt;workflow&gt; RECIPE_SVM_RBF_3\n4         4 &lt;workflow&gt; RECIPE_SVM_RBF_4\n5         5 &lt;workflow&gt; RECIPE_SVM_RBF_5\n6         6 &lt;workflow&gt; RECIPE_SVM_RBF_6"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#comparison-to-sequential-backend",
    "href": "posts/serie_ouro/svm/index.html#comparison-to-sequential-backend",
    "title": "Cotação do ouro - Parte 4",
    "section": "Comparison to Sequential Backend",
    "text": "Comparison to Sequential Backend\n\nmodel_sequential_tbl &lt;- model_wfset %&gt;%\n  modeltime_fit_workflowset(\n    data    = training(splits),\n    control = control_fit_workflowset(\n      verbose   = TRUE,\n      allow_par = FALSE\n    )\n  )\n\n\nmodel_parallel_tbl %&gt;%\n  modeltime_calibrate(testing(splits)) %&gt;%\n  modeltime_accuracy() #%&gt;%\n\n# A tibble: 6 × 9\n  .model_id .model_desc      .type   mae  mape  mase smape  rmse           rsq\n      &lt;int&gt; &lt;chr&gt;            &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1         1 RECIPE_SVM_RBF_1 Test  49.3  28.0  10.9  32.6  50.1  0.594        \n2         2 RECIPE_SVM_RBF_2 Test  48.0  27.2  10.6  31.6  48.8  0.531        \n3         3 RECIPE_SVM_RBF_3 Test  35.5  20.1   7.81 22.4  36.3  0.451        \n4         4 RECIPE_SVM_RBF_4 Test   7.25  4.20  1.60  4.23  8.80 0.191        \n5         5 RECIPE_SVM_RBF_5 Test  12.4   7.36  2.73  7.02 15.8  0.00000000452\n6         6 RECIPE_SVM_RBF_6 Test  14.5   8.61  3.20  8.16 17.5  0.0883       \n\n # table_modeltime_accuracy(.interactive = FALSE)"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#forecast-assessment",
    "href": "posts/serie_ouro/svm/index.html#forecast-assessment",
    "title": "Cotação do ouro - Parte 4",
    "section": "Forecast Assessment",
    "text": "Forecast Assessment\n\nmodel_parallel_tbl %&gt;%\n  modeltime_calibrate(testing(splits)) %&gt;% \n  modeltime_forecast(\n    new_data    = testing(splits),\n    actual_data = gold_mean,\n    keep_data   = TRUE\n  ) %&gt;%\n  plot_modeltime_forecast()\n\n\n\n\n\nfazer o tuning dos dois parâmetros separados e contruir um modelo com eles para ver como fica"
  },
  {
    "objectID": "posts/serie_ouro/svm/index.html#forecast-future",
    "href": "posts/serie_ouro/svm/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 4",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  model_parallel_tbl %&gt;%\n  modeltime_calibrate(testing(splits)) %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()\n\n\n\n\n\n\nparallel_stop()"
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#modelo-stl-seasonal-trend-decomposition-using-loess",
    "href": "posts/serie_ouro/EDA/index.html#modelo-stl-seasonal-trend-decomposition-using-loess",
    "title": "Cotação do ouro - Parte 1",
    "section": "Modelo STL (Seasonal-Trend Decomposition Using LOESS)",
    "text": "Modelo STL (Seasonal-Trend Decomposition Using LOESS)\nO modelo STL desagrega uma série temporal em três componentes principais: tendência (Trend), sazonalidade (Seasonal), e resíduos (Remainder). Ele faz isso separadamente para cada componente, usando suavização LOESS (um método de suavização não paramétrica) para estimar cada parte. Sendo especialmente útil quando a sazonalidade em seus dados varia com o tempo. Isso o torna útil para séries temporais com sazonalidades irregulares ou não lineares. O STL é principalmente usado para entender a estrutura de uma série temporal e não para fazer previsões diretamente. Entretanto existem diferentes técnicas de modelagem voltadas para previsão que utilizam como parte do seu processo a decomposição dos dados através de modelos STL.\nModelo STL pode ser descrito da seguinte forma:\n\nO modelo STL desagrega uma série temporal \\(y_t\\) em três componentes principais: tendência (\\(T_t\\)), sazonalidade (\\(S_t\\)) e resíduos (\\(R_t\\)) usando o método LOESS.\nEquação de Decomposição:\n\\[y_t = T_t + S_t + R_t\\]\nEquação da Tendência:\n\\[T_t = \\mu_t + \\mu_{t-1} + \\ldots + \\mu_{t-d+1}\\]\nEquação da Sazonalidade:\n\\[S_t = \\gamma_{t-m} + \\gamma_{t-2m} + \\ldots + \\gamma_{t-km}\\]\nEquação dos Resíduos:\n\\[R_t = y_t - T_t - S_t\\]\nPeso LOESS (w):\n\\[w_i = \\begin{cases} (1 - (|i|/d)^3)^3 & \\text{para } |i| \\leq d \\\\ 0 & \\text{para } |i| &gt; d \\end{cases}\\]\nonde:\n\n\\(\\mu_t\\) refere-se à estimativa da tendência na época \\(t\\)\n\\(d\\) é o parâmetro que determina a largura da janela de suavização usada para estimar a tendência\n\\(\\gamma_{t-km}\\) refere-se à estimativa da sazonalidade na época \\(t\\) com um atraso de \\(m\\), onde \\(m\\) é a periodicidade sazonal\n\\(w_i\\) é o peso, que depende da distância entre a observação na época \\(t\\) e a época \\(i\\). Quando a distância é menor ou igual a \\(d\\) (a largura da janela de suavização), o peso é calculado usando a função de tri-cubo. Para distâncias maiores que \\(d\\), o peso é zero, o que significa que as observações muito distantes não influenciam a estimativa da tendência.\n\nApós a compreensão do funcionamento, vou processeguir com o ajuste do modelo aos dados e, em seguida, com a criação dos gráficos para visualização dos resultados.\n\n#Ajustando o modelo STL aos dados\nfit &lt;- \n  gold_data %&gt;%\n  fabletools::model(\n    feasts::STL(value)\n  ) %&gt;% \n  fabletools::components() \n\n\n#Gerando os gráficos do modelo STL\nfit %&gt;% fabletools::autoplot()\n\n\n\n\nOs resultados apontam para a presença de componentes de tendência, sazonalidade e ruído. No entanto, aparentemente, há estruturas dentro da componente de ruído. Portanto, antes de prosseguir com a investigação da componente sazonal gerada pelo modelo, vamos verificar se a componente de ruído pode ser genuinamente considerada como ruído branco. Para esta finalidade, farei uso do teste de Ljung-Box, que avalia a existência de correlações significativas nos dados.\n\nfit %&gt;% \n  fabletools::features(remainder, ljung_box, lag = 18)\n\n# A tibble: 1 × 3\n  .model             lb_stat lb_pvalue\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 feasts::STL(value)    142.         0\n\n\nO resultado do teste rejeita a hipótese nula de que não há autocorrelação nos resíduos. Assim, não podemos considerar que a componente não possui informações relevantes.\nO próximo passo nesta Análise Exploratória será a análise espectral, que tem como objetivo compreender o comportamento das potenciais componentes sazonais presentes nos dados. Dados os resultados obtidos até o momento, vou realizar análises espectrais nos dados originais, bem como nas componentes sazonais e de ruído identificadas pelo modelo STL. Embora esse processo possa parecer trabalhoso e, à primeira vista, redundante, essa avaliação detalhada pode fornecer informações valiosas sobre os dados."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#análise-preliminar",
    "href": "posts/serie_ouro/EDA/index.html#análise-preliminar",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análise Preliminar",
    "text": "Análise Preliminar\n\nUma vez que os dados foram carregados, podemos começar nossa análise.\nVou começar criando uma tabela para visualizar os dados e utilizando as funções summary e str para obter informações da estrutura dos dados dentro do R e infomrações estatísticas básicas.\n\n\ngold_data %&gt;% \n  slice_head(n=5) %&gt;% \n  set_tab(cap_tab = \"Dados Brutos\")\n\n\n\nDados Brutos\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\nGLD\n2013-01-02\n163.49\n164.14\n163.14\n163.17\n10431800\n163.17\n\n\nGLD\n2013-01-03\n162.48\n162.88\n160.83\n161.20\n16117500\n161.20\n\n\nGLD\n2013-01-04\n159.52\n160.63\n158.89\n160.44\n19179800\n160.44\n\n\nGLD\n2013-01-07\n159.36\n159.96\n159.15\n159.43\n9361800\n159.43\n\n\nGLD\n2013-01-08\n160.46\n160.99\n160.01\n160.56\n7694800\n160.56\n\n\n\n\n\n\n\n\n\n\n#base::summary(gold_data)\n#utils::str(gold_data)\n\n\nO aspecto mais significativo desses resultados é a irregularidade amostral. Apesar dos resultados da função summary não indicarem a presença de NA’s (valores ausentes), é importante observar que as datas na coluna “date” contêm registros para fins de semana e feriados, que são omitidos pela fonte dos dados. Isso pode resultar em efeitos indesejados e até mesmo tornar inviáveis algumas análises.\nDentre as diversas abordagens para lidar com essa situação, eu vou optar pelo método da reamostragem, onde converteremos nossos dados que possuem uma frequência amostral diária em mensal.\nApesar dessa técnica levar a uma perda de resolução, ela pode proporcionar uma simplificação do processamento e análise, além da diminuição do ruido nos dados. Pontos que serão muito positivos levando em conta o propósito demonstrativo deste post. É importante ressaltar que, dependendo do propósito da análise, a melhor opção seria utilizar o máximo de informação possível e optar pelo “descarte” de parte das informações apenas após uma avaliação mais criteriosa."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#decompondo-a-série-temporal",
    "href": "posts/serie_ouro/EDA/index.html#decompondo-a-série-temporal",
    "title": "Cotação do ouro - Parte 1",
    "section": "Decompondo a Série Temporal",
    "text": "Decompondo a Série Temporal\n\nA decomposição de séries temporais é uma técnica fundamental na análise de dados temporais, permitindo separar uma série temporal em seus componentes principais para melhor compreensão e modelagem. Existem várias abordagens para realizar essa decomposição, sendo as mais comuns as seguintes:\n\nDecomposição Aditiva: Nessa abordagem, a série temporal é dividida em três componentes: tendência, sazonalidade e ruído. A tendência representa a evolução de longo prazo da série, a sazonalidade corresponde a padrões repetitivos em intervalos fixos de tempo e o ruído representa as flutuações aleatórias. A decomposição aditiva é útil quando as variações em cada componente são independentes do nível da série.\nDecomposição Multiplicativa: Na decomposição multiplicativa, a série temporal é dividida em tendência, sazonalidade e ruído, mas em termos multiplicativos. Isso significa que os componentes são representados como proporções do nível da série. Essa abordagem é apropriada quando as variações são proporcionais ao nível da série, como em casos onde o crescimento é exponencial.\nDecomposição STL (Seasonal and Trend decomposition using LOESS): A decomposição STL é uma técnica avançada que utiliza alisamento local (LOESS) para decompor a série em tendência, sazonalidade e resíduos. Ela é especialmente útil para séries com sazonalidade não constante ao longo do tempo ou com tendências não lineares.\nDecomposição por Componentes Independentes (ICA): A ICA é uma técnica avançada que busca separar uma série temporal em componentes independentes, cada um representando uma fonte de variação única. Isso pode ser útil quando se deseja identificar e isolar padrões específicos em dados complexos.\n\nA escolha da abordagem de decomposição depende da natureza dos dados e dos objetivos da análise. Devido às características complexas apresentadas pelos dados, a técnica de decomposição escolhida foi o STL, comumente utilizada para identificar tendências de longo prazo e padrões sazonais, inclusive permitindo múltiplos períodos sazonais.\n\n\nModelo STL (Seasonal-Trend Decomposition Using LOESS)\n\nAntes de realizar a decomposição dos dados vou apresentar em carater didático o funcionamento do modelo STL.\nO modelo STL desagrega uma série temporal em três componentes principais: tendência (Trend), sazonalidade (Seasonal), e resíduos (Remainder). Ele faz isso separadamente para cada componente, usando suavização LOESS (um método de suavização não paramétrica) para estimar cada parte. Sendo especialmente útil quando a sazonalidade em seus dados varia com o tempo. Isso o torna útil para séries temporais com sazonalidades irregulares ou não lineares. O STL é principalmente usado para entender a estrutura de uma série temporal e não para fazer previsões diretamente. Entretanto existem diferentes técnicas de modelagem voltadas para previsão que utilizam como parte do seu processo a decomposição dos dados através de modelos STL.\nO modelo STL pode ser descrito da seguinte forma:\n\nO modelo STL desagrega uma série temporal \\(y_t\\) em três componentes principais: tendência (\\(T_t\\)), sazonalidade (\\(S_t\\)) e resíduos (\\(R_t\\)) usando o método LOESS.\nEquação de Decomposição:\n\\[y_t = T_t + S_t + R_t\\]\nEquação da Tendência:\n\\[T_t = \\mu_t + \\mu_{t-1} + \\ldots + \\mu_{t-d+1}\\]\nEquação da Sazonalidade:\n\\[S_t = \\gamma_{t-m} + \\gamma_{t-2m} + \\ldots + \\gamma_{t-km}\\]\nEquação dos Resíduos:\n\\[R_t = y_t - T_t - S_t\\]\nPeso LOESS (w):\n\\[w_i = \\begin{cases}\n  (1 - \\left(\\frac{h}{|t - x_i|}\\right)^3)^3 & \\text{se } |t - x_i| \\leq h \\\\\n  0 & \\text{se } |t - x_i| &gt; h\n\\end{cases}\\]\nonde:\n\n\\(\\mu_t\\) refere-se à estimativa da tendência na época \\(t\\).\n\\(d\\) é o parâmetro que determina a largura da janela de suavização usada para estimar a tendência.\n\\(\\gamma_{t-km}\\) refere-se à estimativa da sazonalidade na época \\(t\\) com um atraso de \\(m\\), onde \\(m\\) é a periodicidade sazonal.\n\\(w_i\\) é o peso calculado usando a função de tri-cubo. Esta função atribui pesos próximos a 1 para pontos de dados que estão próximos ao ponto de interesse \\(t\\) (dentro da janela de suavização \\(h\\)) e atribui pesos decrescentes rapidamente para pontos de dados mais distantes.\n\nApós a compreensão do funcionamento, vou processeguir com o ajuste do modelo aos dados e, em seguida, com a criação dos gráficos para visualização dos resultados.\n\n#Ajustando o modelo STL aos dados\nfit &lt;- \n  gold_data %&gt;%\n  fabletools::model(\n    feasts::STL(value)\n  ) %&gt;% \n  fabletools::components() \n\n\n#Gerando os gráficos do modelo STL\nfit %&gt;% fabletools::autoplot()\n\n\n\n\nOs resultados indicam a presença de componentes relacionados a tendência, sazonalidade e ruído. O segundo gráfico (trend) evidencia as alterações de tendência descritas no início da análise exploratória. Já o terceiro gráfico (season_year) apresenta uma aparente mudança no comportamento sazonal, que gradualmente modifica seu padrão. O último gráfico (remainder), que teoricamente deveria representar ruído branco, aparenta conter estruturas potencialmente relevantes para a compreensão dos dados, principalmente em sua parte final.\nO próximo passo nesta Análise Exploratória consistirá na análise espectral, cujo propósito é aprofundar a compreensão do comportamento das potenciais componentes periódicas identificadas nos dados. Contudo, antes de prosseguir com as análises mencionadas, quero verificar se a componente remainder pode ser realmente considerada como ruído branco. Para esta finalidade, farei uso do teste de Ljung-Box, que avalia a existência de correlações significativas nos dados.\n\nTeste Ljung-Box\nO teste de Ljung-Box é usado para verificar a presença de autocorrelação nos resíduos de um modelo de séries temporais. Matematicamente, o teste pode ser representado da seguinte forma:\nA estatística de teste Q para o teste de Ljung-Box é definida como:\n\\[Q = n(n+2) \\sum_{k=1}^{h} \\frac{\\hat{\\rho}_k^2}{n-k}\\]\nOnde:\n\n\\(Q\\) é a estatística de teste.\n\\(n\\) é o número de observações na série temporal.\n\\(h\\) é o número máximo de atrasos considerados.\n\\(k\\) é uma variável que varia de 1 até \\(h\\).\n\\(\\hat{\\rho}_k^2\\) representa as estimativas de autocorrelação nos atrasos \\(h\\) nos resíduos.\nA fórmula calcula a estatística Q como a soma ponderada dos quadrados das autocorrelações estimadas nos atrasos \\(k\\), onde os pesos são proporcionais a \\((n-k)^1\\).\n\nO valor de \\(Q\\) segue uma distribuição qui-quadrado com \\(h\\) graus de liberdade sob a hipótese nula de que não há autocorrelação nos resíduos.\nEste teste é amplamente utilizado na análise de séries temporais para garantir que os resíduos de um modelo se comportem como um ruído branco, o que é uma suposição importante para muitos métodos estatísticos e de previsão.\n\nfit %&gt;% \n  fabletools::features(remainder, ljung_box, lag = 48) %&gt;% \n  set_tab(cap_tab = \"Teste ljung-box\")\n\n\n\nTeste ljung-box\n\n\n.model\nlb_stat\nlb_pvalue\n\n\n\n\nfeasts::STL(value)\n248.7254\n0\n\n\n\n\n\n\n\n\nO resultado do teste rejeita a hipótese nula de que não há autocorrelação nos resíduos. Assim, não podemos considerar que a componente não possui informações relevantes.\nDados os resultados obtidos até o momento, vou realizar análises espectrais nos dados originais, bem como nas componentes sazonais e de ruído identificadas pelo modelo STL. Embora esse processo possa parecer trabalhoso e, à primeira vista, redundante, essa avaliação detalhada pode fornecer informações valiosas sobre os dados."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#análises-espectrais",
    "href": "posts/serie_ouro/EDA/index.html#análises-espectrais",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análises Espectrais",
    "text": "Análises Espectrais\n\nA análise espectral é uma técnica fundamental em processamento de sinais (série temporal) que envolve decompor os dados em suas componentes de frequência. Isso é alcançado por meio da transformada de Fourier ou outras técnicas similares, permitindo que o sinal seja representado no domínio da frequência e não somente no domínio do tempo. A análise espectral é valiosa para identificar padrões periódicos, identificar frequências dominantes, detectar sazonalidades e compreender a estrutura subjacente de um sinal. Em muitos casos, a análise espectral pode revelar informações ocultas nos dados que não são facilmente perceptíveis na forma temporal original.\nOs métodos de análise espectral utilizadas serão a análise de Fourier e de wavelets.\n\n\nAnálise de Fourier\n\nA análise de Fourier é uma técnica matemática poderosa usada para decompor sinais complexos em suas componentes de frequência, revelando os diversos padrões de oscilação que os compõem. Essa abordagem é fundamentada na transformada de Fourier, cujos resultados são geralmente representados pelo espectro de potência, que descreve a distribuição de energia em diferentes frequências. O resultado é um gráfico de amplitude em função da frequência, de interpretação direta e intuitiva.\nA transformada de Fourier contínua de uma função(t) é dada por:\n\\[X(f) = \\int_{-\\infty}^{\\infty} x(t) \\cdot e^{-j2\\pi ft} \\,dt\\]\nOnde:\n\n\\(X(f)\\) é a Transformada de Fourier do sinal \\(x(t)\\) no domínio da frequência \\(f\\).\n\\(x(t)\\) é o sinal no domínio do tempo.\n\\(j\\) é a unidade imaginária.\n\nCalculada a tranformada de Fourier \\(X(f)\\) do sinal \\(x(t)\\), o espectro de potência \\(P(f)\\) é dado como:\n\\[P(f) = |X(f)|^2\\]\n\n\n\nAnálise Wavelets\n\nConforme mencionado anteriormente um dos métodos escolhido para análise espctral foi a análise via wavelets. Baseada em funções wavelets, que são pequenas ondas (ou “wavelets”), essa técnica tem como principal caracteristica uma duração finita no tempo, garantindo sua localizadas no domínio do tempo e da frequência. Uma vantagem em relação a técnica tradicional da transformadas de Fourier, que representam sinais como uma combinação de senos e cossenos com duração infinita no tempo, indicando apenas a presença das componentes periódicas e não o seu comportamento ao decorrer do tempo de registro.\nSemelhante à técnica anterior, a análise via wavelets é baseada na transformada wavelet, cujos resultados são normalmente representados por meio de um “escalograma de wavelets”. Essa técnica é frequentemente empregada na análise wavelet para representar como a energia do sinal varia em diferentes escalas ao longo do tempo. O escalograma é uma representação bidimensional, em que o eixo horizontal denota o tempo, o eixo vertical denota a frequência e a intensidade do sinal é codificada por cores ou tons de cinza.\nA transformada de wavelet contínua pode ser definida como o produto interno entre o sinal original e uma função wavelet mãe que é transladada e escalonada. A fórmula geral para isso é a seguinte:\n\\[W(a, b) = \\int_{-\\infty}^{\\infty} x(t) \\cdot \\psi_{a,b}(t) \\, dt\\]\nonde:\n\n\\(W(a, b)\\) é o coeficiente da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\\(x(t)\\) é o sinal de entrada.\n\\(\\psi_{a,b}(t)\\) é a wavelet mãe escalonada e transladada de acordo com os parâmetros \\(a\\) e \\(b\\). Cada coeficiente do escalograma é calculado como o módulo ao quadrado dos coeficientes da Transformada Wavelet:\n\n\\[S(a, b) = |W(a, b)|^2\\]\nonde:\n\n\\(S(a, b)\\) é o escalograma na escala \\(a\\) e posição \\(b\\).\n\\(W(a, b)\\) são os coeficientes da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\n\n\n\n\nRemoção da Tendência e Normalização dos Dados\n\nAntes de avançarmos com a análise espectral, é imprescindível realizar duas etapas cruciais: a remoção da tendência, caso esta esteja presente, e a normalização dos dados. Para atingir esses objetivos, utilizaremos a componente ‘trend’, gerada pelo modelo STL, a qual será subtraída dos dados originais. Posteriormente, empregaremos a função ‘scale’ para centralizar e normalizar os dados, assegurando que estejam preparados de forma adequada para a análise subsequente.\nLembrando que esses procedimentos serão aplicados apenas para os dados originais. As componentes ‘season’ e ‘remainder’, provenientes do modelos STL, já estão livres de tendência e centrados.\n\ndetrend_ts &lt;- \n  fit %&gt;%\n  mutate(\n    value = value - trend,\n    value = scale(value)\n    ) %&gt;% \n  select(index,value) \n\ndetrend_ts %&gt;%\n  slice_head(n=5)%&gt;%\n  set_tab(cap_tab = \"Dados sem tendência e normalizados\")\n\n\n\nDados sem tendência e normalizados\n\n\nindex\nvalue\n\n\n\n\n2013 jan\n1.93360712\n\n\n2013 fev\n1.67668880\n\n\n2013 mar\n1.53406325\n\n\n2013 abr\n0.05350123\n\n\n2013 mai\n-0.72788956\n\n\n\n\n\n\n\n\nPronto, os dados estão prontos para as análises espectrais!\n\n\n\nResultados Análises Espectrais\n\nPara realizar as análises descritas será utilizado o pacote WaveletComp. Os códigos usados são os mesmos para todos os dados e serão apresentados apenas para os dados originais.\nPara obtenção do espectro de potência será utilizada a função wt.avg, que retorna as médias da potência de wavelet ao longo do tempo. Equivalente ao espectro de potência obtido através da Transformada de Fourier.\nOs resultados serão apresentados na seguinte ordem: dados originais, componente ‘season’, e componente ‘remainder’ do modelo STL.\n\n\nDados originais\n\nO código abaixo realiza o cálculo da transformada wavelet e salva seus resultados em um objéto, nomeado w.ouro.dt, que será utilizado na criação dos gráficos desejádos.\n\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    loess.span = 0,\n    verbose=FALSE,\n    date.format = \"%Y-%m-%d\"\n    )\n\n\nUma vez calculada a transformada wavelet, podemos criar os gráficos do espectro de potência e escalograma.\nPara a criação do gráfico do espectro de potência, o pacote WaveletComp possui a função wt.avg.\n\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\n\nO espectro de potência dos dados originais revela dois picos estatisticamente significativos, ocorrendo aproximadamente em 12 meses e 16 meses, enquanto um terceiro pico em torno de 6 meses não alcança significância estatística. No entanto, os resultados apresentam uma resolução limitada.\nVamos analisar o comportamento dessas componentes ao longo do tempo. Para criar o escalograma, o pacote ‘WaveletComp’ disponibiliza a função wt.image.\n\n\n# monthly.ticks &lt;- seq(as_date(\"2013-01-01\", format = \"%Y-%m-%d\"), \n#                      as_date(\"2023-09-22\", format = \"%Y-%m-%d\"), \n#                      by = \"month\")\n# monthly.labels &lt;- strftime(monthly.ticks, format = \"%b %Y\")\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  periodlab = \"periodo (meses)\", \n   show.date = FALSE, date.format = \"%Y-%m-%d\"#, \n   # spec.time.axis = list(at = monthly.ticks, labels = monthly.labels, \n   #                       las = 2)\n  )\n\n\n\n\n\nO resultado do escalograma apresenta aproximadamente as mesmas componentes espectrais presentes do espectro de potência. A componente de aproximadamente 6 meses aparece fora da faixa de confiabilidade estatística, de modo descontínuo e com baixos valores de escala. As componenetes de aproximadamente 11 meses e 16 meses aparecem dentro da margem de 95% de confiabilidade estatística, contudo os resultados não possuem uma boa resolução. Estas componentes exibem variação em suas faixas de periodicidades e variações nos seus valores de escala. Apresentando seus valores mais expressivos (&gt;0,5) apenas na parte final dos dados.\n\n\n\nComponente ‘season’\n\n\n\n\n\n\nEm contraste aos resultados anteriores, as componentes agora estão definidas com maior clareza e estão concentradas nos períodos de 6 meses e 12 meses. Notavelmente, o modelo STL não conseguiu capturar a componente que ocorre aproximadamente a cada 16 meses, presente nos dados originais.\n\n\n\n\n\n\n\nDevido à forma como a componente ‘season’ foi construída, ela não contém o ruído presente nos dados originais, tornando as componentes espectrais muito mais evidentes. A componente anual mantém um comportamento uniforme ao longo de todo o período de registro. Por outro lado, a componente de 6 meses exibe valores elevados até a metade do período de registro, após o qual desaparece do sinal.\n\n\n\nComponente ‘remainder’\n\n\n\n\n\n\nEmbora seja considerada como resíduo pelo modelo STL, os resultados do espectro de potência são notavelmente semelhantes aos dos dados originais. Eles mostram componentes espectrais em torno de 12 meses e 16 meses, dentro da margem de confiabilidade estatística, enquanto uma componente de aproximadamente 6 meses fica fora dessa margem. Ao contrário dos resultados dos dados originais, as componentes apresentam uma definição mais nítida entre os picos, e a componente de aproximadamente 16 meses exibe uma maior concentração de energia em comparação com as outras componentes.\n\n\n\n\n\n\n\nO escalograma da componente ‘remainder’ é surpreendentemente semelhante ao escalograma dos dados originais. Isso é inesperado, uma vez que teoricamente o modelo STL deveria ter removido não apenas a tendência, mas também a componente sazonal."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#análise-de-autocorrelação",
    "href": "posts/serie_ouro/EDA/index.html#análise-de-autocorrelação",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análise de Autocorrelação",
    "text": "Análise de Autocorrelação\nA autocorrelação é um conceito chave na análise de séries temporais e dados sequenciais, indicando a relação entre observações em diferentes pontos no tempo. Ela mede a similaridade entre os valores de uma série e seus próprios valores atrasados (defasados) em diferentes intervalos de tempo. Quando a autocorrelação é forte, significa que os valores passados influenciam fortemente os valores futuros da série. Isso pode ser usado para identificar padrões cíclicos, sazonalidades e tendências nos dados. A função de autocorrelação é uma ferramenta essencial para entender a estrutura temporal dos dados e para selecionar adequadamente modelos de previsão. A autocorrelação também está intimamente relacionada ao conceito de estacionariedade, já que séries temporais estacionárias frequentemente exibem autocorrelações consistentes ao longo do tempo, o que facilita a aplicação de técnicas de modelagem e previsão.\nUm método adicional que pode ser empregado para identificar a existência de componentes periódicas é a análise de autocorrelação. Essa análise não apenas auxilia na detecção dessas componentes, mas também proporciona informações valiosas durante o processo de modelagem, tais como os coeficientes de correlação para diversos intervalos de tempo, que podem sinalizar a estacionariedade da série.\nA seguir, adotaremos a análise de autocorrelação para investigar a presença de periodicidade nos dados e avaliar a estacionariedade da série.\nExistem diversas funções disponíveis no ambiente R e em pacotes especializados para calcular e visualizar a função de autocorrelação. Entretanto, para aprimorar a visualização dessa função, optaremos por criar uma função personalizada. Isso simplificará a análise e interpretação dos resultados.\n\ntidy_acf &lt;- function(data, value, lags = 0:20) {\n  \n  #value_expr &lt;- enquo(value)\n  \n  acf_values &lt;- data %&gt;%\n    acf(lag.max = tail(lags, 1), plot = FALSE) %&gt;%\n    .$acf %&gt;%\n    .[,,1]\n  \n  ret &lt;- tibble(acf = acf_values) %&gt;%\n    rowid_to_column(var = \"lag\") %&gt;%\n    mutate(lag = lag - 1) %&gt;%\n    filter(lag %in% lags)\n  \n  return(ret)\n}\n\nUma vez que a função tenha sido criada, podemos prosseguir com a verificação da autocorrelação dos dados. Nesse contexto, iremos definir um intervalo de atraso de 127, um valor que ultrapassa a componente de aproximadamente 16 meses.\n\nmax_lag &lt;- nrow(gold_data)-1\n\nconfidence &lt;- 1.96 / sqrt(nrow(gold_data))\n\n\ntidy_acf(gold_data$value, lags = 0:max_lag) %&gt;%\n  ggplot(aes(lag, acf)) +\n  geom_segment(aes(xend = lag, yend = 0), color = palette_light()[[1]]) +\n  geom_vline(xintercept = 18, linewidth = 1.8, color = palette_light()[[2]]) +\n  geom_hline(yintercept = confidence, color = \"red\",\n             linetype = \"dotted\",linewidth = 1.2) +\n  geom_hline(yintercept = -confidence, color = \"red\",\n             linetype = \"dotted\",linewidth = 1.2) +\n  annotate(\"text\", label = \"18 meses\", x = 19, y = 0.8, \n           color = palette_light()[[2]], size = 6, hjust = 0) +\n  theme_tq() +\n  labs(title = \"ACF: Valor do Ouro \")\n\n\n\n\nA análise de correlação reforça as conclusões das análises espectrais realizadas, apontando para uma correlação significativa com um atraso de 18 meses. Além disso, é perceptível a presença de vários atrasos com valores estatisticamente relevantes. Agora, vamos examinar mais detalhadamente esses resultados.\nPara isso, vamos utilizar a função que criamos a fim de ampliar nossa visão sobre os resultados, permitindo uma avaliação mais precisa.\n\ntidy_acf(gold_data$value, lags = 12:24) %&gt;%\n  ggplot(aes(lag, acf)) +\n  geom_vline(xintercept = 18, linewidth = 3, color = palette_light()[[2]]) +\n  geom_segment(aes(xend = lag, yend = 0), color = palette_light()[[1]]) +\n  geom_hline(yintercept = confidence, color = \"red\",\n             linetype = \"dotted\",linewidth = 1.2) +\n  geom_hline(yintercept = -confidence, color = \"red\",\n             linetype = \"dotted\",linewidth = 1.2) +\n  geom_point(color = palette_light()[[1]], size = 2) +\n  geom_label(aes(label = acf %&gt;% round(2)), vjust = -1,\n             color = palette_light()[[1]]) +\n  annotate(\"text\", label = \"18 meses\", x =18.5, y = 0.8, \n           color = palette_light()[[2]], size = 5, hjust = 0) +\n  theme_tq() +\n  labs(title = \"ACF: Valor do Ouro\",\n       subtitle = \"Zoom entre os Lags 12 a 24\")\n\n\n\n\nAtravés desse enfoque mais detalhado, podemos observar que o último atraso com um valor igual ou superior a 0.5, teoricamente fornecendo um sólido potencial de previsão, é, de fato, o atraso de 18 meses. Durante a fase de modelagem, pretende-se experimentar com intervalos de tempo superiores a 18 meses, contudo, é provável que isso não resulte em melhorias substanciais nos resultados."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#verificando-a-estacionariedade-dos-dados",
    "href": "posts/serie_ouro/EDA/index.html#verificando-a-estacionariedade-dos-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Verificando a Estacionariedade dos Dados",
    "text": "Verificando a Estacionariedade dos Dados\nA estacionariedade é um conceito fundamental na análise de séries temporais e processos estocásticos. Uma série temporal é considerada estacionária quando suas propriedades estatísticas, como a média e a variância, permanecem constantes ao longo do tempo. Isso implica que os padrões e as relações entre os dados não mudam com o tempo. A estacionariedade é crucial para muitas técnicas de modelagem e previsão, pois muitos métodos assumem que os dados exibem essa propriedade para produzir resultados precisos. Caso contrário, a falta de estacionariedade pode levar a resultados enganosos, uma vez que os padrões flutuantes nos dados podem obscurecer tendências reais e criar falsas correlações. Existem testes estatísticos e técnicas de transformação para avaliar e alcançar a estacionariedade em séries temporais, garantindo assim uma base sólida para análises e previsões precisas.\nObservando o gráfico dos dados da cotação do valor do ouro e seus resultados de autocorrelação já encontramos indicativos que a nosa série temporal não é estacionária.\nVamos utilizar o teste Aumentado Dickey-Fuller (adf.test()) e o teste de Kwiatkowski-Phillips-Schmidt-Shin (unitroot_kpss) para verificar, de forma menos subjetiva se a série temporal é estacionária. Talves você estejá se perguntanto o motivo de utilizar dois teste. Eu considero uma boa prática para tornar os resutados mais robustos.\n\ntseries::adf.test(ts(gold_data$value),alternative =\"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(gold_data$value)\nDickey-Fuller = -2.4131, Lag order = 5, p-value = 0.4048\nalternative hypothesis: stationary\n\n\nO teste adf não encontrou evidencia para rejeitar a hipótese nula de não estacionariedade. Vamos verificar o resultado do teste kpss.\n\ngold_data %&gt;% \n  fabletools::features(value, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      1.83        0.01\n\n\nO teste kpss, apresenta evidência para rejeitar a hipótese nula, que nesse caso é de estacionariedade.\nOs dois testes indicam a não estacionariedade dos dados, uma característica desfavorável e, em alguns casos, essencial para certos tipos de modelos. Em breve, examinaremos as principais abordagens para abordar essa situação e transformar os dados em um estado estacionário. Vale ressaltar desde já que a transformação dos dados para torná-los estacionários requer a aplicação de algum tipo de transformação, a qual precisa ser revertida durante as fases de previsão."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#transformando-os-dados",
    "href": "posts/serie_ouro/EDA/index.html#transformando-os-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Transformando os Dados",
    "text": "Transformando os Dados\nVamos agora avaliar os resultados de algumas dessas técnicas. Durante a análise da eficácia dessas abordagens em tornar a série estacionária, reaproveitaremos diversos testes já conduzidos, cujos códigos serão omitidos.\n\nBox Cox\nA transformação de Box-Cox é uma técnica estatística usada para estabilizar a variância e tornar uma distribuição mais próxima da normalidade. Ela é frequentemente aplicada em séries temporais ou outras análises estatísticas quando os dados exibem heteroscedasticidade (variação não constante) ou não seguem uma distribuição normal. A transformação é especialmente útil quando você deseja aplicar métodos estatísticos que assumem uma distribuição normal dos dados.\n\\[y(\\lambda) = \\begin{cases}\n\\frac{y^\\lambda - 1}{\\lambda}, & \\text{se } \\lambda \\neq 0 \\\\\n\\log(y), & \\text{se } \\lambda = 0\n\\end{cases}\\]\nNesta fórmula,\\(y\\) é o valor original da série temporal e \\(λ\\) é o parâmetro de transformação. O parâmetro \\(λ\\) pode assumir qualquer valor real, e diferentes valores de \\(λ\\) resultam em diferentes transformações. A escolha do valor ideal de \\(λ\\) geralmente é feita de maneira a maximizar a normalidade ou estabilizar a variância dos dados transformados.\nPara escolher o valor ideal de \\(λ\\), é comum testar vários valores em um intervalo, aplicar a transformação a cada valor da série e analisar a normalidade e a homogeneidade da variância dos dados transformados. Isso pode ser feito visualmente ou por meio de testes estatísticos. Para essa tarefa vamos utilizando a função forecast::BoxCox.lambda do pacote forecast.\n\nlambda &lt;- round(forecast::BoxCox.lambda(gold_data$value), digits = 2)\nlambda\n\n[1] 0.45\n\n\n\ngold_data %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   features(box_cox_close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      1.82        0.01\n\n\nMesmo após a transformação continuamos encontrando evidências para rejeitar a estacionariedade. Não será mostrado, mas outras transformações foram tentadas (log, sqrt) e também não foram eficazes.\n\n\nVisualizando os Dados Após a Tranformação\n\ngold_data %&gt;%\n   mutate(\n     box_cox_close = fabletools::box_cox(value, lambda=lambda)\n     ) %&gt;% \n   ggplot(aes(x=index, y=box_cox_close))+\n   geom_line()\n\n\n\n\nComo essa abordagem não se mostrou efetiva não vamos processeguir avaliando seus resultados.\n\n\nDiferenciação\nA diferenciação em séries temporais é uma técnica fundamental para transformar dados não estacionários em um formato mais adequado para análise e modelagem. Ela envolve a subtração de valores consecutivos da série, visando remover tendências e padrões de sazonalidade. Ao aplicar diferenciação, a série é transformada em uma nova série de diferenças, que é esperançosamente estacionária. Essa abordagem permite a utilização de modelos estatísticos, como o ARIMA (AutoRegressive Integrated Moving Average), que pressupõem a estacionariedade dos dados.\nPara se determinar o número de diferenças necessárias para tornar os dados estacionários usaremos a função unitroot_ndiffs. O termo “unit root” refere-se à raiz unitária, que é uma característica de uma série temporal não estacionária. A presença de uma raiz unitária indica que a série não reverte rapidamente a perturbações ou choques, o que pode tornar a análise e a modelagem mais desafiadoras.\n\ngold_data %&gt;% \n  fabletools::features(value,  unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\nO teste indica que uma diferenciação é necessária para tornar a série estacionária.\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.388      0.0825\n\n\nO tete KPSS apresenta evidência para aceitar a hipótese nula de estacionariedade. Vamos visualizar a Série temporal após a aplicação de uma diferença.\n\n\n\n\n\nApós a transformação os dados, aparentemente, não existe nenhuma estrutura remanecente. Vamos utilizar o teste Ljung-Box para verificar se os valores observados das autocorrelações são consistentes com o que seria esperado em uma série de dados aleatórios sem autocorrelação. Valores significativos no teste podem indicar que há autocorrelações nas defasagens testadas, o que sugere que um modelo de série temporal pode ser necessário para capturar essas correlações.\n\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    17.5     0.131\n\n\nVamos verificar a função de autocorrelção novamente\n\n\n\n\n\nAtravés do gráfico e do teste Ljung-Box encontramos evidências de que após esse procedimento os dados não apresentam autocorrelação entre os atrasos da série.\n\n\nRemoção da Tendência\nApós a decomposição dos dados podemos obter os dados sem o trend de modo bem simples, apenas subtraindo o trend, encontrado pelo modelo, dos dados originais.\nVamos utilizar novamente o test KPSS para verificar a estacionariedade dos dados.\n\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1    0.0283         0.1\n\n\nEncontramos evidencia de que a série é estacionária\nVisualizando os dados após a remoção do trend.\n\n\n\n\n\nVamos verificar a função de autocorrelção novamente\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    146.         0\n\n\nExistem evidências para rejeitar a hipótese nula de que não há autocorrelação. Assim como nos dados originais e na transformação box_cox, os dados apresentam correlação com seus atrasos de modo significativo. Porém a caracteristica da curva de correlção com a remoção do trend é bem diferente, sugerindo a existência de um padrão ciclico nos dados. Isso vai de encontro as componetes encontradas pelo modelo STL, que indica a existência de componetes sazonais nos dados. Contudo, os lags com valore de até 0.5 vão até o lag 35."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#análises-espectrais-via-wavelets",
    "href": "posts/serie_ouro/EDA/index.html#análises-espectrais-via-wavelets",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análises Espectrais via Wavelets",
    "text": "Análises Espectrais via Wavelets\nA análise espectral é uma técnica fundamental em processamento de sinais (série temporal) que envolve decompor os dados em suas componentes de frequência. Isso é alcançado por meio da transformada de Fourier ou outras técnicas similares, permitindo que o sinal seja representado no domínio da frequência em vez do domínio do tempo. A análise espectral é valiosa para identificar padrões periódicos, identificar frequências dominantes, detectar sazonalidades e compreender a estrutura subjacente de um sinal. Em muitos casos, a análise espectral pode revelar informações ocultas nos dados que não são facilmente perceptíveis na forma temporal original.\nConforme mencionado anteriormente o método escolhido para análise espctral foi a transformada wavelet. Baseada em funções wavelet, que são pequenas ondas (ou “wavelets”), essa técnica tem como principal caracteristica uma duração finita no tempo, garantindo sua localizadas no domínio do tempo e da frequência. Uma vantagem em relação a técnica tradicional da transformadas de Fourier, que representam sinais como uma combinação de senos e cossenos com duração infinita no tempo, indicando apenas a presença das componentes periódicas e não o seu comportamento ao decorrer do tempo de registro.\nAs técnicas que serão empregadas nesta etapa da EDA serão o periodograma, que fornesse um gráfico mais fácil de entender e garante uma maior acertividade em relação as componentes espctrais presentes nos dados. E o escalograma\nA análise através da transformada wavelet pode ser descrita da seguinte forma:\nTransformada Wavelet Discreta (DWT) - A DWT é usada para decompor um sinal \\(x(t)\\) em diferentes escalas e deslocamentos, usando funções wavelet \\(\\psi_{a,b}(t)\\). A representação da DWT é dada por:\n\\[DWT(a,b) = \\int_{-\\infty}^{\\infty} x(t) \\cdot \\psi_{a,b}(t) \\, dt\\]\nonde \\(a\\) é o fator de escala e \\(b\\) é o deslocamento no tempo.\nPeriodograma de Wavelets: O periodograma de wavelets é uma representação da distribuição de energia de um sinal em diferentes escalas de frequência. Ele é calculado através da convolução do sinal original \\(x(t)\\) com uma wavelet mãe \\(\\psi(t)\\) em diferentes escalas \\(a\\) e deslocamentos temporais \\(b\\). A fórmula para o periodograma de wavelets pode ser escrita como:\n\\[P_x(f) = \\lim_{T \\to \\infty} \\frac{1}{T} \\left| \\int_{0}^{T} x(t) \\cdot e^{-j2\\pi f t} \\, dt \\right|^2\\]\nOnde:\n\\(P(a, b)\\) é a intensidade ou energia do sinal na escala \\(a\\) e deslocamento \\(b\\). \\(x(t)\\) é o sinal de entrada. \\(psi^_{a,b}(t)\\) é a wavelet mãe deslocada e dilatada para a escala \\(a\\) e deslocamento \\(b\\), representada como \\(psi^_{a,b}(t) = \\frac{1}{\\sqrt{a}}\\psi\\left(\\frac{t-b}{a}\\right)\\). \\(|\\cdot|^2\\) representa o módulo ao quadrado.\nEspectrograma de Wavelets: O espectrograma de wavelets é uma representação da distribuição de energia de um sinal em diferentes escalas de frequência e ao longo do tempo. Ele é calculado ao calcular o periodograma de wavelets em diferentes janelas temporais do sinal. A fórmula para o espectrograma de wavelets pode ser escrita como:\n\\[S(a, t) = \\left|\\int x(t')\\psi^*_{a,t}(t') dt'\\right|^2\\]\nOnde:\n\\(S(a, t)\\) é a intensidade ou energia do sinal na escala \\(a\\) e tempo \\(t\\). \\(x(t)\\) é o sinal de entrada. \\(\\psi^_{a,t}(t')\\) é a wavelet mãe deslocada e dilatada para a escala \\(a\\) e tempo \\(t\\), representada como \\(\\psi^_{a,t}(t') = \\frac{1}{\\sqrt{a}}\\psi\\left(\\frac{t'-t}{a}\\right)\\). \\(|\\cdot|^2\\) representa o módulo ao quadrado.\n\nRemoção da Tendência e Normalização dos Dados\nUma etapa necessária antes de realizar a análise espectral de um conjunto de dados é a remoção da tendência, caso exista, e a normalização dos dados. Para isso, utilizaremos as componentes criadas pelo modelo STL, que foram salvas no objeto chamado “fit”, em conjunto com a função base do R chamada scale. Embora a maioria das funções especializadas em análise espectral possuam essas etapas embutidas em sua execução, realizaremos separadamente para ilustrar o processo.\n\ndetrend_ts &lt;- \n  fit %&gt;%\n  mutate(\n    value = value - trend,\n    value = scale(value)\n    ) %&gt;% \n  select(index,value) \n\nhead(detrend_ts)\n\n# A tsibble: 6 x 2 [1M]\n     index value[,1]\n     &lt;mth&gt;     &lt;dbl&gt;\n1 2013 jan    1.94  \n2 2013 fev    1.68  \n3 2013 mar    1.54  \n4 2013 abr    0.0542\n5 2013 mai   -0.728 \n6 2013 jun   -1.65  \n\n\nPronto, nossos dados estão prontos para as análises espectrais!\n\n\nCálculo da Transformada Wavelet\nPara realizar essa análise, vamos utilizar o pacote WaveletComp. O código usado para calcular os valores e criar os gráficos será o mesmo tanto para os dados originais como para as saídas do modelo. Os códigos serão apresentados apenas na primeira aplicação e omitidos nas demais para evitar repetições desnecessárias.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    date.format=\"%Y-%M\",\n    loess.span = 0,\n    verbose=FALSE\n    )\n\nUma vez calculada a transformada wavelet através da função analyze.wavelet, podemos criar o gráfico do espectro de potência dos dados sem tendência.\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\nO espectro de potência dos dados originais apresenta dois picos estatisticamente significativos, em aproximadamente 11 meses e 16 meses. Embora a presença desses dois picos seja evidente, os resultados não possuem uma boa resolução.\nVamos verificar o comportamento dessas componentes ao longo da variável tempo. O código abaixo cria o gráfico do escalograma de wavelets, utilizando também a saída da função analyze.wavelet.\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  date.format=\"%Y-%m-%d\",\n  show.date = TRUE\n  )\n\n\n\n\nAssim como nos resultados anteriores, não existe uma boa resolução das compoentes espectrais. Apesar de presentes ao longo de todo o registro, os valores de atribuidos aos períodos detectados são baixos, atingindo seus valores mais altos na parte final do registro. Outro ponto a ser destacado é que as componentes sofrem peqenas mudanças em suas faixas ao longo do registro, podendo ser efeito de interações complexas entre as componentes como modulação.\nVamos prosseguir com a análise e verificar os resultados para a componente season gerada pelo modelo STL.\n\n\n\n\n\nDiferente dos resultados anteriores, as componentes aqui presentes são bem definidas e estão centradas em 12 meses e 6 meses. Outros pontos relevantes a serem destacados são: a ausência da componente de aproximadamente 16 meses, que estava presente nos resultados anteriores. Além disso, a componente de 6 meses apresenta significância estatística, o que difere dos resultados anteriores.\n\n\n\n\n\nAs componentes detectadas nos resultados anteriores aparecem de forma contínua e bem definida ao longo de todo o registro. A componente anual apresenta comportamento semelhante ao longo de todo o registro. No entanto, a componente de 6 meses possui altos valores de amplitude atribuídos a ela apenas até a metade do tempo do registro, praticamente não apresentando energia na parte final.\nVamos agora verificar a componente “remainder” criada pelo modelo STL\n\n\n\n\n\nApesar de ser tratada como resíduo pelo modelo STL, os resultados do espectro de potência são bastante similares aos dos dados originais, apresentando picos espectrais mais definidos. As componentes detectadas estão em torno de ~16 meses e 10 meses. Há também uma componente de ~6 meses, mas esta está fora da margem de significância estatística.\nVamos verificar o escalograma de wavelets.\n\n\n\n\n\nÉ fácil notar a semelhança entre os resultados da componente “remainder” e os dados originais. Dessa forma, podemos concluir que o modelo STL não foi capaz de capturar a componente de ~16 meses ou a considerou como ruído vermelho. Isso é surpreendente, pois esse método é teoricamente capaz de identificar tanto padrões cíclicos como semicíclicos.\n\n\nPontos Mais Relevantes\nApós analisarmos os dados originais (sem tendência) e as componentes geradas pelo modelo STL, encontramos evidências que nos levam a crer na presença de componentes periódicas em nossos dados. Caso essa informação seja confirmada, teremos uma boa capacidade de predição, o que resultará em modelos mais eficientes e robustos."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#fourier",
    "href": "posts/serie_ouro/EDA/index.html#fourier",
    "title": "Cotação do ouro - Parte 1",
    "section": "Fourier",
    "text": "Fourier\nTransformada Fourier - A transformada de Fourier contínua de uma função(t) é dada por:\n\\[X(f) = \\int_{-\\infty}^{\\infty} x(t) \\cdot e^{-j2\\pi ft} \\,dt\\]\nOnde:\n\\(X(f)\\) é a Transformada de Fourier do sinal \\(x(t)\\) no domínio da frequência \\(f\\).\n\\(x(t)\\) é o sinal no domínio do tempo.\n\\(j\\) é a unidade imaginária.\nEspectro de Potência:\nO espectro de potência é uma medida da distribuição de energia em diferentes frequências, sem relação com o tempo. O resultado é um gráfico de amplitude versus frequência, fácilmente imterpretável.\nÉ calculado a partir da Transformada de Fourier como o módulo ao quadrado do espectro de frequência:\n\\[S(f) = |X(f)|^2\\]\n\nWavelets\nConforme mencionado anteriormente o método escolhido para análise espctral foi a transformada wavelet. Baseada em funções wavelet, que são pequenas ondas (ou “wavelets”), essa técnica tem como principal caracteristica uma duração finita no tempo, garantindo sua localizadas no domínio do tempo e da frequência. Uma vantagem em relação a técnica tradicional da transformadas de Fourier, que representam sinais como uma combinação de senos e cossenos com duração infinita no tempo, indicando apenas a presença das componentes periódicas e não o seu comportamento ao decorrer do tempo de registro.\nApós o cálculo da transformada wavelet dos dados serão calculados os espectro de potência e o espectrograma. A seguir uma breve descrição matemática e da aplicação destas técnicas.\nTransformada Wavelet - A transformada de wavelet contínua é definida como o produto interno do sinal original com uma função wavelet mãe que é transladada e escalonada. A fórmula geral é a seguinte:\n\\[W(a, b) = \\int_{-\\infty}^{\\infty} x(t) \\cdot \\psi_{a,b}(t) \\, dt\\]\nonde:\n\\(W(a, b)\\) é o coeficiente da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\\(x(t)\\) é o sinal de entrada.\n\\(\\psi_{a,b}(t)\\) é a wavelet mãe escalonada e transladada de acordo com os parâmetros \\(a\\) e \\(b\\).\nEscalograma:\nO escalograma é comumente usado na análise de wavelets para representar como a energia do sinal varia em diferentes escalas ao longo do tempo. É uma representação bidimensional em que o eixo horizontal representa o tempo, o eixo vertical representa a frequência e a intensidade do sinal é representada por cores. Cada coeficiente do escalograma é calculado como o módulo ao quadrado dos coeficientes da Transformada Wavelet:\n\\[S(a, b) = |W(a, b)|^2\\]\nonde:\n\\(S(a, b)\\) é o escalograma na escala \\(a\\) e posição \\(b\\).\n\\(W(a, b)\\) são os coeficientes da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\n\nRemoção da Tendência e Normalização dos Dados\nAntes de avançarmos com a análise espectral, é imprescindível realizar duas etapas cruciais: a remoção da tendência, caso esta esteja presente, e a normalização dos dados. Para atingir esses objetivos, utilizaremos a componente ‘trend’, gerada pelo modelo STL, a qual será subtraída dos dados originais. Posteriormente, empregaremos a função ‘scale’ para centralizar e normalizar os dados, assegurando que estejam preparados de forma adequada para a análise subsequente.\n\ndetrend_ts &lt;- \n  fit %&gt;%\n  mutate(\n    value = value - trend,\n    value = scale(value)\n    ) %&gt;% \n  select(index,value) \n\nhead(detrend_ts)\n\n# A tsibble: 6 x 2 [1M]\n     index value[,1]\n     &lt;mth&gt;     &lt;dbl&gt;\n1 2013 jan    1.94  \n2 2013 fev    1.68  \n3 2013 mar    1.54  \n4 2013 abr    0.0542\n5 2013 mai   -0.728 \n6 2013 jun   -1.65  \n\n\nPronto, os dados estão prontos para as análises espectrais!\n\n\nCálculo da Transformada Wavelet\nPara realizar essa análise, vamos utilizar o pacote WaveletComp. O código usado para calcular os valores e criar os gráficos será o mesmo tanto para os dados originais como para as saídas do modelo. Os códigos serão apresentados apenas na primeira aplicação e omitidos nas demais para evitar repetições desnecessárias.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    loess.span = 0,\n    verbose=FALSE\n    )\n\nUma vez calculada a transformada wavelet através da função analyze.wavelet, podemos criar o gráfico do espectro de potência dos dados sem tendência.\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\nO espectro de potência dos dados originais apresenta dois picos estatisticamente significativos, em aproximadamente 11 meses e 16 meses. Embora a presença desses dois picos seja evidente, os resultados não possuem uma boa resolução.\nVamos verificar o comportamento dessas componentes ao longo da variável tempo. O código abaixo cria o gráfico do escalograma de wavelets, utilizando também a saída da função analyze.wavelet.\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  label.time.axis = TRUE\n  )\n\n\n\n\nAssim como nos resultados anteriores, não existe uma boa resolução das compoentes espectrais. Apesar de presentes ao longo de todo o registro, os valores de atribuidos aos períodos detectados são baixos, atingindo seus valores mais altos na parte final do registro. Outro ponto a ser destacado é que as componentes sofrem peqenas mudanças em suas faixas ao longo do registro, podendo ser efeito de interações complexas entre as componentes como modulação.\nVamos prosseguir com a análise e verificar os resultados para a componente season gerada pelo modelo STL.\n\n\n\n\n\nDiferente dos resultados anteriores, as componentes aqui presentes são bem definidas e estão centradas em 12 meses e 6 meses. Outros pontos relevantes a serem destacados são: a ausência da componente de aproximadamente 16 meses, que estava presente nos resultados anteriores. Além disso, a componente de 6 meses apresenta significância estatística, o que difere dos resultados anteriores.\n\n\n\n\n\nAs componentes detectadas nos resultados anteriores aparecem de forma contínua e bem definida ao longo de todo o registro. A componente anual apresenta comportamento semelhante ao longo de todo o registro. No entanto, a componente de 6 meses possui altos valores de amplitude atribuídos a ela apenas até a metade do tempo do registro, praticamente não apresentando energia na parte final.\nVamos agora verificar a componente “remainder” criada pelo modelo STL\n\n\n\n\n\nApesar de ser tratada como resíduo pelo modelo STL, os resultados do espectro de potência são bastante similares aos dos dados originais, apresentando picos espectrais mais definidos. As componentes detectadas estão em torno de ~16 meses e 10 meses. Há também uma componente de ~6 meses, mas esta está fora da margem de significância estatística.\nVamos verificar o escalograma de wavelets.\n\n\n\n\n\nÉ fácil notar a semelhança entre os resultados da componente “remainder” e os dados originais. Dessa forma, podemos concluir que o modelo STL não foi capaz de capturar a componente de ~16 meses ou a considerou como ruído vermelho. Isso é surpreendente, pois esse método é teoricamente capaz de identificar tanto padrões cíclicos como semicíclicos.\n\n\nPontos Mais Relevantes\nApós analisarmos os dados originais (sem tendência) e as componentes geradas pelo modelo STL, encontramos evidências que nos levam a crer na presença de componentes periódicas em nossos dados. Caso essa informação seja confirmada, teremos uma boa capacidade de predição, o que resultará em modelos mais eficientes e robustos."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#análise-de-fourier",
    "href": "posts/serie_ouro/EDA/index.html#análise-de-fourier",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análise de Fourier",
    "text": "Análise de Fourier\nA análise de Fourier é uma técnica matemática poderosa usada para decompor sinais complexos em suas componentes de frequência, revelando os diversos padrões de oscilação que os compõem. Essa abordagem é fundamentada na transformada de Fourier, cujos resultados são geralmente representados pelo espectro de potência, que descreve a distribuição de energia em diferentes frequências. O resultado é um gráfico de amplitude em função da frequência, de interpretação direta e intuitiva.\nA transformada de Fourier contínua de uma função(t) é dada por:\n\\[X(f) = \\int_{-\\infty}^{\\infty} x(t) \\cdot e^{-j2\\pi ft} \\,dt\\]\nOnde:\n\\(X(f)\\) é a Transformada de Fourier do sinal \\(x(t)\\) no domínio da frequência \\(f\\).\n\\(x(t)\\) é o sinal no domínio do tempo.\n\\(j\\) é a unidade imaginária.\nCalculada a tranformada de Fourier \\(X(f)\\) do sinal \\(x(t)\\), o espectro de potência \\(P(f)\\) é dado como:\n\\[P(f) = |X(f)|^2\\]"
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#análise-wavelets",
    "href": "posts/serie_ouro/EDA/index.html#análise-wavelets",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análise Wavelets",
    "text": "Análise Wavelets\nConforme mencionado anteriormente um dos métodos escolhido para análise espctral foi a análise via wavelets. Baseada em funções wavelets, que são pequenas ondas (ou “wavelets”), essa técnica tem como principal caracteristica uma duração finita no tempo, garantindo sua localizadas no domínio do tempo e da frequência. Uma vantagem em relação a técnica tradicional da transformadas de Fourier, que representam sinais como uma combinação de senos e cossenos com duração infinita no tempo, indicando apenas a presença das componentes periódicas e não o seu comportamento ao decorrer do tempo de registro.\nSemelhante à técnica anterior, a análise via wavelets é baseada na transformada wavelet, cujos resultados são normalmente representados por meio de um “escalograma de wavelets”. Essa técnica é frequentemente empregada na análise wavelet para representar como a energia do sinal varia em diferentes escalas ao longo do tempo. O escalograma é uma representação bidimensional, em que o eixo horizontal denota o tempo, o eixo vertical denota a frequência e a intensidade do sinal é codificada por cores ou tons de cinza.\nA transformada de wavelet contínua pode ser definida como o produto interno entre o sinal original e uma função wavelet mãe que é transladada e escalonada. A fórmula geral para isso é a seguinte:\n\\[W(a, b) = \\int_{-\\infty}^{\\infty} x(t) \\cdot \\psi_{a,b}(t) \\, dt\\]\nonde:\n\\(W(a, b)\\) é o coeficiente da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\\(x(t)\\) é o sinal de entrada.\n\\(\\psi_{a,b}(t)\\) é a wavelet mãe escalonada e transladada de acordo com os parâmetros \\(a\\) e \\(b\\).\nCada coeficiente do escalograma é calculado como o módulo ao quadrado dos coeficientes da Transformada Wavelet:\n\\[S(a, b) = |W(a, b)|^2\\]\nonde:\n\\(S(a, b)\\) é o escalograma na escala \\(a\\) e posição \\(b\\).\n\\(W(a, b)\\) são os coeficientes da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\)."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#resultados-análises-espectrais",
    "href": "posts/serie_ouro/EDA/index.html#resultados-análises-espectrais",
    "title": "Cotação do ouro - Parte 1",
    "section": "Resultados Análises Espectrais",
    "text": "Resultados Análises Espectrais\nPara realizar as análises descritas será utilizado o pacote WaveletComp. Os códigos usados são os mesmos para todos os dados e serão apresentados apenas para os dados originais.\nPara obtenção do espectro de potência será utilizada a função wt.avg, que retorna as médias da potência de wavelet ao longo do tempo. Equivalente ao espectro de potência obtido através da Transformada de Fourier.\nOs resultados serão apresentados na seguinte ordem: dados originais, componente ‘season’, e componente ‘remainder’ do modelo STL.\n\nDados originais\nO código abaixo realiza o cálculo da transformada wavelet e salva seus resultados em um objéto, nomeado w.ouro.dt, que será utilizado na criação dos gráficos desejádos.\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    loess.span = 0,\n    verbose=FALSE,\n    date.format = \"%Y-%m-%d\"\n    )\n\nUma vez calculada a transformada wavelet, podemos criar os gráficos do espectro de potência e escalograma.\nPara a criação do gráfico do espectro de potência, o pacote WaveletComp possui a função wt.avg.\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\nO espectro de potência dos dados originais revela dois picos estatisticamente significativos, ocorrendo aproximadamente em 12 meses e 16 meses, enquanto um terceiro pico em torno de 6 meses não alcança significância estatística. No entanto, os resultados apresentam uma resolução limitada.\nVamos analisar o comportamento dessas componentes ao longo do tempo. Para criar o escalograma, o pacote ‘WaveletComp’ disponibiliza a função wt.image.\n\n# monthly.ticks &lt;- seq(as_date(\"2013-01-01\", format = \"%Y-%m-%d\"), \n#                      as_date(\"2023-09-22\", format = \"%Y-%m-%d\"), \n#                      by = \"month\")\n# monthly.labels &lt;- strftime(monthly.ticks, format = \"%b %Y\")\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  periodlab = \"periodo (meses)\", \n   show.date = FALSE, date.format = \"%Y-%m-%d\"#, \n   # spec.time.axis = list(at = monthly.ticks, labels = monthly.labels, \n   #                       las = 2)\n  )\n\n\n\n\nO resultado do escalograma apresenta aproximadamente as mesmas componentes espectrais presentes do espectro de potência. A componente de aproximadamente 6 meses aparece fora da faixa de confiabilidade estatística, de modo descontínuo e com baixos valores de escala. As componenetes de aproximadamente 11 meses e 16 meses aparecem dentro da margem de 95% de confiabilidade estatística, contudo os resultados não possuem uma boa resolução. Estas componentes exibem variação em suas faixas de periodicidades e variações nos seus valores de escala. Apresentando seus valores mais expressivos (&gt;0,5) apenas na parte final dos dados.\n\n\nComponente ‘season’\n\n\n\n\n\nEm contraste aos resultados anteriores, as componentes agora estão definidas com maior clareza e estão concentradas nos períodos de 6 meses e 12 meses. Notavelmente, o modelo STL não conseguiu capturar a componente que ocorre aproximadamente a cada 16 meses, presente nos dados originais.\n\n\n\n\n\nDevido à forma como a componente ‘season’ foi construída, ela não contém o ruído presente nos dados originais, tornando as componentes espectrais muito mais evidentes. A componente anual mantém um comportamento uniforme ao longo de todo o período de registro. Por outro lado, a componente de 6 meses exibe valores elevados até a metade do período de registro, após o qual desaparece do sinal.\n\n\nComponente ‘remainder’\n\n\n\n\n\nEmbora seja considerada como resíduo pelo modelo STL, os resultados do espectro de potência são notavelmente semelhantes aos dos dados originais. Eles mostram componentes espectrais em torno de 12 meses e 16 meses, dentro da margem de confiabilidade estatística, enquanto uma componente de aproximadamente 6 meses fica fora dessa margem. Ao contrário dos resultados dos dados originais, as componentes apresentam uma definição mais nítida entre os picos, e a componente de aproximadamente 16 meses exibe uma maior concentração de energia em comparação com as outras componentes.\n\n\n\n\n\nO escalograma da componente ‘remainder’ é surpreendentemente semelhante ao escalograma dos dados originais. Isso é inesperado, uma vez que teoricamente o modelo STL deveria ter removido não apenas a tendência, mas também a componente sazonal."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#definição-do-problema",
    "href": "posts/serie_ouro/EDA/index.html#definição-do-problema",
    "title": "Cotação do ouro - Parte 1",
    "section": "Definição do Problema",
    "text": "Definição do Problema\n\nConforme o título deste post sugere, esta série estará relacionada à cotação do ouro, mais especificamente na criação de um modelo preditivo para o SPDR Gold Shares (GLD)."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#aquisição-dos-dados",
    "href": "posts/serie_ouro/EDA/index.html#aquisição-dos-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Aquisição dos Dados",
    "text": "Aquisição dos Dados\n\nOs dados utilizados nesta série são provenientes do Yahoo Finanças. Para acessá-los, utilizaremos a função tq_get do pacote tidyquant. Por padrão, essa função utiliza a opção stock.prices, que retorna os valores de ‘open’, ‘high’, ‘low’, ‘close’, ‘volume’ e ‘adjusted’ do Yahoo Finanças. No entanto, é possível obter outras opções, como dividendos ou informações sobre ‘split’ de ações, além de dados de outras fontes.\nQuando utilizamos o padrão da função e informamos apenas o símbolo da ação desejada, todos os dados disponíveis para o período de tempo completo são retornados.\n\n\n# Obter dados da ação de ouro (código GLD)\ngold_data &lt;- tidyquant::tq_get(\"GLD\")"
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#limpeza-dos-dados",
    "href": "posts/serie_ouro/EDA/index.html#limpeza-dos-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Limpeza dos Dados",
    "text": "Limpeza dos Dados\n\nA etapa de limpeza de dados é um processo fundamental na análise de dados que envolve a identificação, correção e eliminação de erros, inconsistências e valores ausentes em um conjunto de dados. Isso inclui remover duplicatas, preencher lacunas, padronizar formatos e eliminar outliers. A limpeza de dados é crucial para garantir a qualidade e confiabilidade dos dados, pois dados sujos ou corrompidos podem levar a análises imprecisas e conclusões equivocadas. Portanto, essa etapa prepara os dados para análise posterior, assegurando que estejam prontos para serem explorados e modelados de forma eficaz.\nDevido a fonte dos dados, Yahoo Finanças, amplamente reconhecida e utilizada tanto na indústria quanto por investidores individuais, podemos antecipar que encontraremos poucos problemas.\nSem mais delongas vamos iniciar a análise dos dados."
  },
  {
    "objectID": "posts/serie_ouro/EDA/index.html#gráficos-da-série-temporal",
    "href": "posts/serie_ouro/EDA/index.html#gráficos-da-série-temporal",
    "title": "Cotação do ouro - Parte 1",
    "section": "Gráficos da Série Temporal",
    "text": "Gráficos da Série Temporal\nCom os dados preparados para análise, vou criar as primeiras visualizações gráfica.\n\nGráfico das médias mensais\n\ngold_data %&gt;%\n  ggplot2::ggplot(aes(x=index,y=value))+\n  geom_line()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')+\n  labs(title = \"Cotação do Ouro entre 2013-2023\", )+\n  xlab(\"\")+\n  ylab(\"USD por Ação\")+\n  theme_minimal()\n\n\n\n\n\nAo analisar o gráfico das médias mensais, é possível notar que os dados apresentam uma tendência negativa de 2013 a 2016, seguida por um período aparentemente estacionário que se estende até 2019. Após esse período, há uma tendência positiva observada até aproximadamente meados de 2020, seguida por um retorno a uma aparente estacionariedade nos dados. A parte final dos dados parece exibir uma maior volatilidade. Em termos gerais, quando se considera a série temporal como um todo, os dados parecem não ser estacionários. No entanto, esse é um ponto que será discutido e avaliado mais detalhadamente posteriormente.Ao analisar o gráfico das médias mensais, é possível notar que os dados apresentam uma tendência negativa de 2013 a 2016, seguida por um período aparentemente estacionário que se estende até 2019. Após esse período, há uma tendência positiva observada até aproximadamente meados de 2020, seguida por um retorno a uma aparente estacionariedade nos dados. A parte final dos dados parece exibir uma maior volatilidade. Nesta primeira visualização, os dados não apresentam nenhum padrão sazonal ou cíclico evidente. Em termos gerais, quando se considera a série temporal como um todo, os dados parecem não ser estacionários. No entanto, esse é um ponto que será discutido e avaliado mais detalhadamente posteriormente.\n\n\n\nGráficos sazonais\n\nUma representação sazonal é semelhante a um gráfico temporal, exceto que os dados são plotados em relação às “estações” individuais em que os dados foram observados.\n\nfeasts::gg_season(gold_data, labels = \"both\")\n\n\n\n\nUm gráfico sazonal torna o padrão sazonal subjacente mais visível e é especialmente útil para identificar anos em que esse padrão muda. Neste caso, não há um padrão claro de comportamento, já que diferentes anos apresentam comportamentos distintos. No entanto, parece que, em sua maioria, os valores tendem a cescer durante o início do ano e diminuir na parte final. Além disso, o gráfico destaca o rápido declínio dos valores durante o ano de 2013, o crescimento nos anos de 2019 e 2020, e uma relativa estabilidade nos demais anos.\nUma representação alternativa que enfatiza os padrões sazonais é aquela em que os dados de cada estação são agrupados em mini gráficos temporais separados.\n\nfeasts::gg_subseries(gold_data)\n\n\n\n\nPor meio deste gráfico, é possível visualizar um padrão sazonal potencial, com um pico observado em agosto, embora esse padrão não seja muito evidente.\nAté o momento, observou-se que os dados apresentam um comportamento complexo e não revelam padrões óbvios, o que torna a construção de modelos preditivos desafiadora e limita sua eficácia, especialmente em projeções de longo prazo.\nA seguir, vou aprofundar a análise na tentativa de compreender as estruturas subjacentes aos dados, buscando informações que possam aprimorar a capacidade preditiva dos modelos futuros."
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html",
    "href": "posts/serie_ouro/Decompose/index.html",
    "title": "Cotação do ouro - Parte 3",
    "section": "",
    "text": "Nesta terceira postagem sobre a cotação do ouro será empregada a mesma estratégia de validação cruzada comparação dos modelos, usadas na postagem anterior. Assim, os procedimentos e funções adotados nestas etapas não serão novamentes descritos."
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#modelos",
    "href": "posts/serie_ouro/Decompose/index.html#modelos",
    "title": "Cotação do ouro - Parte 3",
    "section": "Modelos",
    "text": "Modelos\n\nSTL + ETS (Error-Trend-Seasonality)\nAssim como o método STL, o modelo ETS decompõe uma série temporal em três componentes principais: erro (Error), tendência (Trend), e sazonalidade (Seasonal). Cada um desses componentes é modelado usando parâmetros paramétricos, o que significa que são usados modelos estatísticos específicos (Exponencial, Aditivo, Multiplicativo, etc.) para representar essas partes. O ETS assume que a sazonalidade é constante ao longo do tempo, o que significa que é mais apropriado para séries temporais com sazonalidades regulares e estáveis.O ETS é frequentemente usado para fazer previsões em séries temporais. Os modelos ETS estimam a evolução futura da série com base nas estimativas de tendência e sazonalidade.\nDependendo da natureza da série temporal, existem diferentes variantes do modelo ETS, como ETS(AAA), ETS(ADD), ETS(MMM), etc\nEm resumo, a principal diferença entre os modelos STL e ETS está na abordagem de decomposição e na ênfase em previsões. O STL é mais flexível em relação à sazonalidade variável, enquanto o ETS assume uma sazonalidade constante e é mais voltado para previsões.\nModelo ETS:\n\nO modelo ETS desagrega uma série temporal \\(y_t\\) em três componentes principais: erro (\\(E_t\\)), tendência (\\(T_t\\)) e sazonalidade (\\(S_t\\)).\nEquação de Decomposição:\n\\[y_t = T_t + S_t + E_t\\]\nEquação da Tendência:\n\\[T_t = \\alpha Y_t + (1 - \\alpha)(T_{t-1} + L_{t-1})\\]\nEquação da Sazonalidade:\n\\[S_t = \\beta(Y_t - T_t) + (1 - \\beta)S_{t-m}\\]\nEquação do Termo de Sazonalidade:\n\\[E_t = \\gamma(Y_t - T_t - S_t) + (1 - \\gamma)E_{t-1}\\]\nEquação do Erro:\n\\[E_t = y_t - T_t - S_t\\]\nOnde:\n\\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\) são os parâmetros de suavização\n\\(m\\) é a periodicidade sazonal\n\\(L_{t-1}\\) é uma correção de tendência sazonal\n\nets_mod &lt;- seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n  ) %&gt;% \n  set_engine(\"stlm_ets\") %&gt;% \n  fit(media ~ mes, training(cv$splits[[1]]))\n\n\n\nTBATS (Trigonometric Exponential Smoothing state space model)\nO método TBATS (Trigonometric Exponential Smoothing state space model) é uma técnica sofisticada para modelar séries temporais com sazonalidades complexas, tendências não lineares e resíduos irregulares. A abordagem envolve a combinação de componentes de suavização exponencial com componentes trigonométricas para capturar padrões sazonais não regulares.\nO método TBATS (Trigonometric Exponential Smoothing state space model) é uma técnica sofisticada para modelar séries temporais com sazonalidades complexas, tendências não lineares e resíduos irregulares. A abordagem envolve a combinação de componentes de suavização exponencial com componentes trigonométricas para capturar padrões sazonais não regulares. Aqui está uma descrição matemática detalhada do método TBATS, juntamente com as fórmulas em código LaTeX:\nObservação Original (Y):\n\\[Y_t\\]\nTendência (T):\nComponente Linear:\n\\[l_t = \\alpha_0 + \\alpha_1 t\\]\nComponente Não Linear:\n\\[b_t = \\sum_{i=1}^{p} \\left( \\beta_i \\sin \\left( \\frac{2\\pi i t}{m} \\right) + \\gamma_i \\cos \\left( \\frac{2\\pi i t}{m} \\right) \\right)\\]\nTendência Total:\n\\[T_t = l_t + b_t\\]\nSazonalidade (S): Componente Sazonal: \\[s_t = \\sum_{i=1}^{q} \\left( \\phi_i \\sin \\left( \\frac{2\\pi i t}{m} \\right) + \\theta_i \\cos \\left( \\frac{2\\pi i t}{m} \\right) \\right)\\]\nErro (E): Componente de Erro: \\[E_t = Y_t - T_t - s_t \\]\nFórmulas de Suavização Exponencial: Suavização Exponencial Simples (ETS): \\[l_t = \\alpha Y_t + (1 - \\alpha)(l_{t-1} + b_{t-1})\\] \\[b_t = \\beta (l_t - l_{t-1}) + (1 - \\beta) b_{t-1}\\] \\[s_t = \\gamma (Y_t - l_t) + (1 - \\gamma) s_{t-m}\\] \\[E_t = Y_t - l_t - s_t\\]\nEstimação de Parâmetros:\nOs parâmetros são estimados por meio de técnicas de otimização, como a minimização da função de verossimilhança negativa.\n\ntbats_mod &lt;- seasonal_reg(\n    seasonal_period_1 = 18,\n    seasonal_period_2 = 12\n    ) %&gt;% \n  set_engine(\"tbats\") %&gt;% \n  fit(media ~ mes, training(cv$splits[[1]]))\n\n\n\noutros\n\nexp_ets &lt;- \n exp_smoothing(\n   seasonal_period  = 12,\n   error            = \"multiplicative\",\n   trend            = \"additive\",\n   season           = \"multiplicative\"\n ) %&gt;% \n  set_engine(\"ets\") %&gt;% \n  fit(media ~ mes, training(cv$splits[[1]]))\n\n\nexp_croston &lt;- \n exp_smoothing(\n   smooth_level = 0.2\n ) %&gt;% \n  set_engine(\"croston\") %&gt;% \n  fit(media ~ mes, training(cv$splits[[1]]))\n\n\nexp_theta &lt;- \n exp_smoothing() %&gt;% \n  set_engine(\"theta\") %&gt;% \n  fit(media ~ mes, training(cv$splits[[1]]))\n\n\nexp_smooth &lt;- \n exp_smoothing(\n   seasonal_period  = 12,\n   error            = \"multiplicative\",\n   trend            = \"additive_damped\",\n   season           = \"additive\"\n ) %&gt;% \n  set_engine(\"smooth_es\") %&gt;% \n  fit(media ~ mes, training(cv$splits[[1]]))"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#comparando-os-modelos",
    "href": "posts/serie_ouro/Decompose/index.html#comparando-os-modelos",
    "title": "Cotação do ouro - Parte 3",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nmodel_table &lt;- \nmodeltime_table(\n  ets_mod,\n  tbats_mod,\n  exp_ets,\n  exp_croston,\n  exp_theta,\n  exp_smooth\n  \n  )"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#calibrando",
    "href": "posts/serie_ouro/Decompose/index.html#calibrando",
    "title": "Cotação do ouro - Parte 3",
    "section": "Calibrando",
    "text": "Calibrando\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(cv$splits[[1]]))"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#residual",
    "href": "posts/serie_ouro/Decompose/index.html#residual",
    "title": "Cotação do ouro - Parte 3",
    "section": "Residual",
    "text": "Residual\n\ncalib_table %&gt;% \n  modeltime_residuals() %&gt;% \n  plot_modeltime_residuals(\n     .type = \"acf\",\n     .interactive = FALSE\n     )"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#accuracy",
    "href": "posts/serie_ouro/Decompose/index.html#accuracy",
    "title": "Cotação do ouro - Parte 3",
    "section": "Accuracy",
    "text": "Accuracy\n\ncalib_table %&gt;% \n  modeltime_accuracy() \n\n# A tibble: 6 × 9\n  .model_id .model_desc             .type   mae  mape  mase smape  rmse      rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1         1 SEASONAL DECOMP: ETS(M… Test   19.4 10.9   4.21 11.6   21.4  0.0304 \n2         2 BATS(0.001, {0,0}, -, … Test   21.0 11.8   4.56 12.6   22.9 NA      \n3         3 ETS(M,AD,M)             Test   23.3 13.1   5.04 14.1   25.0  0.00224\n4         4 CROSTON METHOD          Test   11.2  6.30  2.42  6.49  12.3 NA      \n5         5 THETA METHOD            Test   16.0  8.95  3.47  9.46  17.6  0.595  \n6         6 ETS(MADA)               Test   28.5 16.0   6.16 17.5   30.5  0.700"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#test-set-visualization",
    "href": "posts/serie_ouro/Decompose/index.html#test-set-visualization",
    "title": "Cotação do ouro - Parte 3",
    "section": "Test set visualization",
    "text": "Test set visualization\n\ncalib_table %&gt;% \n  modeltime_forecast(\n    new_data = testing(cv$splits[[1]]),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#validação-cruzada",
    "href": "posts/serie_ouro/Decompose/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 3",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\nresamples_fitted\n\n# Modeltime Table\n# A tibble: 6 × 4\n  .model_id .model   .model_desc                 .resample_results\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                       &lt;list&gt;           \n1         1 &lt;fit[+]&gt; SEASONAL DECOMP: ETS(M,N,N) &lt;rsmp[+]&gt;        \n2         2 &lt;fit[+]&gt; BATS(0.001, {0,0}, -, -)    &lt;rsmp[+]&gt;        \n3         3 &lt;fit[+]&gt; ETS(M,AD,M)                 &lt;rsmp[+]&gt;        \n4         4 &lt;fit[+]&gt; CROSTON METHOD              &lt;rsmp[+]&gt;        \n5         5 &lt;fit[+]&gt; THETA METHOD                &lt;rsmp[+]&gt;        \n6         6 &lt;fit[+]&gt; ETS(MADA)                   &lt;rsmp[+]&gt;        \n\n\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = FALSE\n    )\n\n\n\n\n\nresamples_fitted %&gt;%\n    modeltime_resample_accuracy(summary_fns = mean) %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      n\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nSEASONAL DECOMP: ETS(M,N,N)\nResamples\n6\n15.49\n9.07\n3.65\n9.02\n17.80\n0.19\n    2\nBATS(0.001, {0,0}, -, -)\nResamples\n6\n16.69\n9.71\n3.82\n10.05\n18.32\nNA\n    3\nETS(M,AD,M)\nResamples\n6\n17.54\n10.23\n4.01\n10.33\n19.46\n0.15\n    4\nCROSTON METHOD\nResamples\n6\n10.43\n6.05\n2.31\n6.36\n11.75\nNA\n    5\nTHETA METHOD\nResamples\n6\n13.09\n7.63\n2.94\n7.74\n14.59\n0.23\n    6\nETS(MADA)\nResamples\n6\n19.06\n11.09\n4.44\n11.44\n20.83\n0.24"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#forecast-future",
    "href": "posts/serie_ouro/Decompose/index.html#forecast-future",
    "title": "Cotação do ouro - Parte 3",
    "section": "Forecast future",
    "text": "Forecast future\n\nfuture_forecast_tbl &lt;- \n  calib_table %&gt;% \n  modeltime_refit(gold_mean) %&gt;% \n  modeltime_forecast(\n    h  = \"1 year\",\n    actual_data = gold_mean\n  )\n\n\nfuture_forecast_tbl %&gt;% \n  plot_modeltime_forecast()"
  },
  {
    "objectID": "posts/serie_ouro/Decompose/index.html#referências",
    "href": "posts/serie_ouro/Decompose/index.html#referências",
    "title": "Cotação do ouro - Parte 3",
    "section": "Referências",
    "text": "Referências\nGeneral Interface for Exponential Smoothing State Space Models\nGeneral Interface for Multiple Seasonality Regression Models (TBATS, STLM)\nForecasting using stl objects\nAutomatic Time Series Forecasting: the forecast Package for R"
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#adf",
    "href": "posts/serie_ouro/EDA2/index.html#adf",
    "title": "Cotação do ouro - Parte 2",
    "section": "Dickey-Fuller (ADF)",
    "text": "Dickey-Fuller (ADF)\n\nO teste de Dickey-Fuller Aumentado (ADF, na sigla em inglês) é empregado para verificar a existência de raízes unitárias em séries temporais univariadas. A hipótese nula deste teste pressupõe a não estacionariedade; logo, se o valor-p do teste for inferior a um nível de significância especificado, temos base para rejeitar a hipótese nula e inferir que a série é estacionária.\nA equação do teste ADF é dada por:\n\\[\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\ldots + \\delta_p \\Delta y_{t-p} + \\varepsilon_t\\]\nonde:\n\n\\(\\Delta y_t \\text{ é a diferença entre } y_t \\text{ e } y_{t-1}\\),\n\\(\\alpha\\) é o termo constante,\n\\(\\beta\\) é o coeficiente da tendência temporal,\n\\(\\gamma\\) é o coeficiente de \\(y_{t-1}\\),\n\\(\\delta_1\\), \\(\\delta_2\\), \\(\\ldots\\), \\(\\delta_p\\) são os coeficientes das diferenças defasadas de \\(y_t\\),\n\\(\\varepsilon_t\\) é o termo de erro.\n\nPara realizar o teste ADF, utilizaremos a função adf.test do pacote tseries.\n\ntseries::adf.test(ts(fit$value),alternative =\"stationary\") \n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(fit$value)\nDickey-Fuller = -2.4719, Lag order = 5, p-value = 0.3803\nalternative hypothesis: stationary\n\n\nO teste ADF não encontrou evidências que justifiquem a rejeição da hipótese nula de não estacionariedade."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#kpss",
    "href": "posts/serie_ouro/EDA2/index.html#kpss",
    "title": "Cotação do ouro - Parte 2",
    "section": "Kwiatkowski-Phillips-Schmidt-Shin (KPSS)",
    "text": "Kwiatkowski-Phillips-Schmidt-Shin (KPSS)\n\nO teste KPSS visa determinar se uma série temporal é estacionária em relação a uma tendência determinística, em contraposição à estacionariedade em torno de um valor médio constante. A hipótese nula do teste KPSS pressupõe que a série é estacionária. Portanto, se o valor-p obtido for superior ao nível de significância, não temos justificativa para rejeitar a hipótese nula, o que indica que a série é estacionária em relação a uma tendência. No entanto, se o valor-p for inferior ao nível de significância, podemos rejeitar a hipótese nula e concluir que a série não é estacionária em relação a uma tendência, implicando que a tendência é estocástica.\nA estatística do teste KPSS é definida da seguinte forma:\n\\[KPSS = \\frac{T^2 \\cdot \\hat{\\sigma}^2}{\\sum_{t=1}^{T} S_t^2}\\]\nOnde:\n\n\\(T\\) é o número de observações na série temporal.\n\\(S_t\\) é a soma cumulativa das diferenças entre as observações e a estimativa da tendência local em cada ponto na série temporal. Calculado como: \\[S_t = \\sum_{i=1}^{t} (X_i - \\hat{m}_t)\\].\n\\(\\hat{\\sigma}^2\\) é uma estimativa da variância da série temporal.\n\nOnde \\(X_i\\) é a observação no tempo \\(i\\) e \\(\\hat{m}_t\\) é a estimativa da tendência local no tempo \\(t\\).\nPara o teste KPSS será usado a função unitroot_kpss do pacote fable.\n\nfit %&gt;% \n  fabletools::features(value, unitroot_kpss) %&gt;% \n  set_tab(\"Teste KPSS\")\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n1.8768\n0.01\n\n\n\n\n\n\n\n\nO teste kpss, apresenta evidência para rejeitar a hipótese nula, que nesse caso é de estacionariedade.\nOs dois testes indicam a não estacionariedade dos dados, uma característica desfavorável que, em alguns casos, é essencial para certos tipos de modelos. A seguir examinaremos algumas das principais abordagens para transformar os dados em um estado estacionário."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#decomposição",
    "href": "posts/serie_ouro/EDA2/index.html#decomposição",
    "title": "Cotação do ouro - Parte 2",
    "section": "Decomposição",
    "text": "Decomposição\n\nApós a decomposição dos dados, como discutido na primeira postagem desta série, obtemos várias representações dos dados originais que capturam as estruturas identificadas. Neste ponto, estou examinando a componente ‘remainder’ do modelo STL, que, teoricamente, teve as componentes de tendência e de sazonalidade retiradas. Como mencionei anteriormente, essa componente exibe um comportamento espectral muito semelhante ao dos dados brutos, incluindo elementos cíclicos ou quase cíclicos.\n\n\n\n\n\nVamos usar novamente o teste KPSS para verificar a estacionariedade dos dados.\n\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n0.0361\n0.1\n\n\n\n\n\n\n\n\nO teste não apresentou evidências que justificassem a rejeição da hipótese nula de estacionariedade, o que sugere que a abordagem foi eficaz em tornar os dados estacionários. No entanto, como indicado durante a análise espectral e teste Ljung-Box , esses dados não exibem um comportamento de ruído branco e podem conter estruturas significativas."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#box-cox",
    "href": "posts/serie_ouro/EDA2/index.html#box-cox",
    "title": "Cotação do ouro - Parte 2",
    "section": "Box Cox",
    "text": "Box Cox\n\nA transformação de Box-Cox é uma técnica estatística usada para estabilizar a variância e tornar uma distribuição mais próxima da normalidade. Ela é frequentemente aplicada em séries temporais ou outras análises estatísticas quando os dados exibem heteroscedasticidade (variação não constante) ou não seguem uma distribuição normal.\nSua forma geral pode ser definida matematicamente da seguinte maneira:\nPara um conjunto de dados \\((x_1, x_2, x_3, \\ldots, x_n)\\) a transformação de Box-Cox é definida como:\n\\[y_i =\n\\begin{cases}\n\\frac{x_i^\\lambda - 1}{\\lambda}, & \\text{se } \\lambda \\neq 0 \\\\\n\\ln(x_i), & \\text{se } \\lambda = 0\n\\end{cases}\\]\nOnde:\n\n\\(y_i\\) é o valor transformado do dado \\(x_i\\)\nO parâmetro \\(\\lambda\\) é estimado através da função de verossimilhança e avaliação da adequação do modelo.\n\nQuando \\(\\lambda\\) é igual a zero, a transformação se torna uma transformação logarítmica natural (\\(ln(x_i)\\)) Quando \\(\\lambda\\) não é igual a zero, a transformação é uma potência do dado original (\\(x_i^\\lambda\\)) com uma correção para garantir que a transformação seja bem definida para todos os valores de \\(x_i\\).\nA escolha do valor ideal para λ é geralmente feita visando a maximização da normalidade dos dados transformados ou a estabilização da variância. Uma prática comum envolve testar vários valores dentro de um intervalo específico e aplicar a transformação a cada valor da série. Nessa tarefa, estou utilizando a função forecast::BoxCox.lambda do pacote forecast, que possibilita a seleção automática desse parâmetro. Essa função determina o valor de λ de modo a maximizar o perfil da verossimilhança logarítmica de um modelo linear ajustado aos dados. Para dados não sazonais, é ajustada uma tendência temporal linear, enquanto para dados sazonais, é usado um modelo linear de tendência temporal com variáveis sazonais dummy.\n\nlambda &lt;- round(forecast::BoxCox.lambda(fit$value), digits = 2)\nlambda\n\n[1] 0.38\n\n\n\nfit %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   ggplot(aes(index,box_cox_close))+\n   geom_line()+\n   ggtitle(\"Transformação Box Cox\")+\n   xlab(\"\")+\n   ylab(\"\")\n\n\n\n\n\nfit %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   features(box_cox_close, unitroot_kpss) %&gt;% \n   set_tab(\"Teste KPSS\")\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n1.8652\n0.01\n\n\n\n\n\n\n\n\nA transformação não modificou a forma dos dados, apenas o range de valores do eixo y. Entretanto, como o teste KPSS indicou isso não foi o suficiente para tornar os dados estacionários."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#diferenciação",
    "href": "posts/serie_ouro/EDA2/index.html#diferenciação",
    "title": "Cotação do ouro - Parte 2",
    "section": "Diferenciação",
    "text": "Diferenciação\n\nA diferenciação em séries temporais é uma técnica amplamente empregada para converter dados não estacionários em um formato mais apropriado para análise e modelagem. Essa técnica consiste na subtração dos valores consecutivos na série temporal, com o propósito de eliminar estruturas de tendência e padrões sazonais. Ao aplicar a diferenciação, a série é transformada em uma nova série de diferenças, com a esperança de torná-la estacionária.\nA remoção das estruturas mencionadas é geralmente alcançada por meio das diferenciações de primeira ordem e sazonal, e ou pela combinação e repetição dos procedimentos. A diferenciação de primeira ordem é realizada subtraindo o valor atual pelo valor anterior na série temporal e é eficaz na eliminação de tendências lineares nos dados. Por outro lado, a diferenciação sazonal envolve o cálculo das diferenças entre os valores da série no mesmo período, mas em anos diferentes, o que é útil para eliminar efeitos sazonais. Isso é realizado com o uso do “lag sazonal”, que representa o número de períodos em uma temporada. Por exemplo, se estiver lidando com dados mensais e a sazonalidade for anual, o lag sazonal seria igual a 12.\nEstá técnica pode ser descrita da seguinte forma:\n\\[\\Delta y_t = y_t - y_{t-1}\\]\nOnde:\n\n\\(\\Delta\\) representa a diferença entre o valor atual \\(y_t\\) e o valor anterior \\(y_{t-1}\\) na série tempora.\n\nPara se determinar o número de diferenças necessárias para tornar os dados estacionários usaremos a função unitroot_ndiffs, que tem com base o teste ADF, anteriormente explicado. Lembrando que o termo “unit root” (raiz unitária) se refere a uma característica de séries temporais não estacionárias. A presença de uma raiz unitária indica que a série não reverte rapidamente perturbações, tornando-a menos previsíveis e mais suscetíveis a flutuações, o que, de fato, pode tornar a análise e a modelagem mais desafiadoras.\n\nfit %&gt;% \n  fabletools::features(value,  unitroot_ndiffs) %&gt;% \n  set_tab(\"Número de Diferenças\")\n\n\n\nNúmero de Diferenças\n\n\n.model\nndiffs\n\n\n\n\nfeasts::STL(value)\n1\n\n\n\n\n\n\n\n\nO teste sinaliza a necessidade de realizar uma diferenciação para tornar a série estacionária.\n\nfit %&gt;% \n  mutate(\n    diff_close = tsibble::difference(value)\n    ) %&gt;% \n  ggplot(aes(index,diff_close))+\n  geom_line()+\n  ggtitle(\"Dados diferenciados\")+\n  xlab(\"\")+\n  ylab(\"\")\n\n\n\n\nApós a transformação dos dados, aparentemente, não parece haver nenhuma estrutura remanescente.\n\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n0.3244\n0.1\n\n\n\n\n\n\n\n\nO teste KPSS apresenta evidências que sustentam a aceitação da hipótese nula de estacionariedade.\n\nfit %&gt;%\n   mutate(\n     diff_close = tsibble::difference(value)\n     ) %&gt;%\n  fabletools::features(value, ljung_box, lag = 78) %&gt;% \n  set_tab(\"Teste LJUNG-BOX\")\n\n\n\nTeste LJUNG-BOX\n\n\n.model\nlb_stat\nlb_pvalue\n\n\n\n\nfeasts::STL(value)\n2768.323\n0\n\n\n\n\n\n\n\n\nO teste Ljung-Box indica que a série se comporta como um ruído branco, ou seja, não apresenta autocorrelação significativa."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#função-de-autocorrelação-acf",
    "href": "posts/serie_ouro/EDA2/index.html#função-de-autocorrelação-acf",
    "title": "Cotação do ouro - Parte 2",
    "section": "Função de Autocorrelação (ACF)",
    "text": "Função de Autocorrelação (ACF)\n\nA ACF pode ser representada da seguinte forma:\nDada uma série temporal \\({X_t}\\), onde t representa o tempo, a autocorrelação para um atraso (lag) \\(k\\) é calculada da seguinte maneira:\n\\[ρ(k) = Cov(X_t, X_{t-k}) / (σ(X_t) * σ(X_{t-k}))\\]\nOnde:\n\n\\(ρ(k)\\) é a autocorrelação para o atraso \\(k\\).\n\\(Cov(X_t, X_{t-k})\\) é a covariância entre \\(X_t\\) e \\(X_{t-k}\\), ou seja, a medida de como as observações estão relacionadas em \\(t\\) e \\(t-k\\).\n\\(σ(X_t)\\) é o desvio padrão de \\(X_t\\), que mede a variabilidade dos valores em \\(t\\).\n\\(σ(X_{t-k})\\) é o desvio padrão de \\(X_{t-k}\\), que mede a variabilidade dos valores em \\(t-k\\).\n\nEm resumo, a autocorrelação para um determinado atraso \\(k\\) é a covariância normalizada pela variabilidade (desvio padrão) das observações separadas por esse atraso. Ela varia de -1 a 1, onde valores próximos de 1 indicam forte correlação positiva, valores próximos de -1 indicam forte correlação negativa e valores próximos de 0 indicam ausência de correlação."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#função-de-autocorrelação-parcial-pacf",
    "href": "posts/serie_ouro/EDA2/index.html#função-de-autocorrelação-parcial-pacf",
    "title": "Cotação do ouro - Parte 2",
    "section": "Função de Autocorrelação Parcial (PACF)",
    "text": "Função de Autocorrelação Parcial (PACF)\n\nA PACF é calculada da seguinte maneira:\nSuponha que temos uma série temporal \\(x_t\\) para \\(t = 1, 2, \\ldots, N\\). O PACF de atraso \\(k\\) (\\(PACF_k\\)) é definido como:\n\\[PACF_k = \\frac{\\gamma_{t,t-k} - \\sum_{i=1}^{k-1} \\phi_i \\gamma_{t-k,t-k+i}}{1 - \\sum_{i=1}^{k-1} \\phi_i PACF_i}\\]\nonde:\n\n\\(\\gamma_{t,t-k}\\) é a função de autocovariância entre \\(x_t\\) e \\(x_t-k\\).\n\\(\\phi_i\\) são os coeficientes estimados de um modelo AR de ordem \\(i\\), onde \\(i = 1, 2, \\ldots, k-1\\)\n\nA função PACF é calculada iterativamente, começando com \\(k\\) = 1 e progredindo até o atraso máximo desejado. Ela mede a correlação entre uma observação em um determinado momento e observações em intervalos de tempo anteriores, removendo o efeito de observações em lags intermediários. Isso ajuda a identificar a estrutura de dependência direta, indicando o possível número de termos autorregressivos a serem incluídos em um modelo AR."
  },
  {
    "objectID": "posts/serie_ouro/EDA2/index.html#algumas-aplicações",
    "href": "posts/serie_ouro/EDA2/index.html#algumas-aplicações",
    "title": "Cotação do ouro - Parte 2",
    "section": "Algumas Aplicações",
    "text": "Algumas Aplicações\n\n\nModelagem de Séries Temporais: A ACF e a PACF são usadas para identificar a ordem adequada (p, d, q) de modelos da fámilia ARIMA, onde “p” é a ordem do componente AR, “d” é a ordem da diferenciação e “q” é a ordem do componente MA. Lembrando que ACF é usada para evidência de MA(q) e suas sazonalidades e a PACF é usada para identificação de AR(p) e suas sazonalidades.\nIdentificação de Padrões: Além de sua aplicação em modelagem, a ACF e a PACF também são úteis para identificar padrões temporais nas séries temporais, como sazonalidades, e tendências.\n\nA presença de autocorrelação significativa em lags variados nas funções ACF pode sugerir a presença de tendência, e que a série não é estacionária.\nSe houver sazonalidade, você geralmente verá picos significativos na ACF em múltiplos lags (intervalos de tempo) que correspondem aos períodos sazonais. Por exemplo, em uma série mensal com sazonalidade anual, você esperaria ver picos nas defasagens 12, 24, 36, etc.\nO comportamento dos picos significativos pode ajudar a indentificar o tipo de sazonalidade (aditiva, multiplicativa) presente nos dados.\n\nDiagnóstico de Resíduos: Após ajustar um modelo a uma série temporal, a ACF e a PACF dos resíduos (diferença entre os valores observados e os valores previstos pelo modelo) podem ser usadas para verificar se há estrutura remanescente nos resíduos.\n\nEm resumo, ACF e PACF são ferramentas essenciais na análise de séries temporais, desempenhando um papel fundamental na identificação da estrutura de dependência temporal e na seleção dos parâmetros apropriados para a modelagem de modelos estatísticos.\n\n\nDados Originais\n\nAs funções ACF e PACF não são geralmente a primeira abordagem quando se suspeita que os dados não sejam estacionários, como neste caso. No entanto, para fins ilustrativos, vou usá-las nos dados brutos.\n\nmax_lag &lt;- nrow(fit)-1\n\nacf_plot &lt;- \nfit %&gt;% \n  feasts::ACF(value, lag_max = max_lag) %&gt;% \n  feasts::autoplot()\n\n\npacf_plot &lt;- \nfit %&gt;% \n  feasts::PACF(value, lag_max = max_lag) %&gt;% \n  feasts::autoplot()\n\n\nggpubr::ggarrange(acf_plot,pacf_plot, ncol=2)\n\n\n\n\nO Gráfico de ACF revela correlações significativas em lags próximos, com uma diminuição gradual que não se aproxima rapidamente de zero. Esse padrão sugere a presença de tendência nos dados, uma característica geralmente associada à não estacionariedade. A transição das correlações de positivas para negativas parece estar relacionada às mudanças no comportamento da tendência ao longo do tempo.\nPor outro lado, os resultados da função PACF mostram valores significativos apenas nos lags 1 e 2, indicando que os efeitos observados no Gráfico de ACF podem ser atribuídos principalmente à presença de tendência nos dados.\nEm uma série estacionária, é comum que os gráficos de ACF e PACF apresentem uma rápida diminuição após um pequeno número de defasagens, sem exibir padrões significativos. A ocorrência de padrões irregulares nesses gráficos sugere a possibilidade de ruído nos dados ou outros elementos não estacionários, corroborando assim os demais resultados das análises realizadas até o momento.\n\n\n\nComponente ‘remainder’\n\nConforme já verificado, a componente ‘remainder’ demonstra estacionariedade de acordo com o teste KPSS, mas não exibe um comportamento de ruído branco conforme indicado pelo teste Ljung-Box. Isso pode ser atribuído, provavelmente, à presença de componentes cíclicas ou quase cíclicas.\nNormalmente, as funções ACF e PACF são utilizadas em dados após a aplicação do operador de diferença. No entanto, a análise dos dados brutos, sem a componente de tendência, pode ser útil para ilustrar várias situações. Por exemplo, pode ajudar a melhorar a compreensão das estruturas cíclicas ou quase cíclicas nos dados. Em casos em que os dados brutos não exibem uma tendência intrínseca e já demonstram estacionariedade. Ou quando mesmo após a aplicação de diferenciação para torná-los estacionários, ainda podem persistir indícios de sazonalidade nos dados, e a estacionariedade pode não ser completamente alcançada.\n\n\n\n\n\nOs picos positivos nos dois primeiros lags no gráfico de ACF sugerem uma forte correlação entre os valores no tempo atual e os valores no tempo imediatamente anterior. Essa observação pode ser indicativo de um componente \\(MA(q)\\) em um modelo da família ARIMA.\nAlém disso, o gráfico mostra um comportamento senoidal, com picos significativos ou quase significativos em multiplos de cerca de 12 meses. Isso sugere a presença de um componente sazonal nas séries temporais. No entanto, é importante notar que o período não é bem definido, e os picos apresentam rápido decaimento e picos adicionais ao redor do período principal. Os valores dos picos na parte periódica são baixos (menores ou iguais a 0,5), o que pode indicar que a capacidade de previsão do componente sazonal pode ser limitada. A presença de picos negativos entre os picos positivos pode indicar algum tipo de efeito de amortecimento ou compensação nas séries temporais.\nO decaimento e a quase significância de outros picos na ACF podem resultar em raízes do polinômio característico que estão muito próximas do ciclo unitário. indício que os processos autorregressivos ou de média móvel têm dificuldade em convergir, o que pode resultar em previsões imprecisas e modelos instáveis.\nPara lidar com raízes próximas ao ciclo unitário em modelos ARIMA, é fundamental considerar alternativas, como a adoção de um modelo SARIMA (Seasonal ARIMA), a fim de eficazmente incorporar a sazonalidade e aprimorar a estabilidade das previsões. Além disso, é apropriado realizar uma diferenciação sazonal, com o período identificado, para tornar a série temporal mais estacionária. Recomenda-se, também, conduzir uma análise minuciosa dos resultados do modelo para assegurar sua adequação aos dados específicos em questão. Isso envolve a validação dos resíduos do modelo, que por sua vez requer o uso da ACF e PACF.\nA PACF indica que há significância até a segunda repetição do ciclo de 12 meses. No entanto, a natureza exata desse comportamento não é clara, e os valores, com exceção do primeiro pico, são baixos. Em resumo, os dados apresentam um comportamento complexo e ruidoso, como já evidenciado em resultados anteriores.\nCom base nas análises da ACF e PACF, é razoável suspeitar que um modelo SARIMA com termos \\(MA(q)(Q)\\), onde \\(q=1, 2\\) e \\(Q=1\\), com uma componente sazonal de 12 meses, e \\(AR(p)(P)\\) onde \\(p=1, 2, 4, 5\\) e \\(P=1\\) com a mesma componente sazonal de 12 meses, podem ser apropriados para modelar essa série temporal. No entanto, a escolha final do modelo SARIMA dependerá de testes de diagnóstico adicionais e validação cruzada para garantir a precisão das previsões e a adequação do modelo aos dados.\n\n\n\nDados sazonais\n\n\n\n\n\n\nA componente ‘sazonal’ criada pelo modelo STL é livres de ruído, tornando mais simples a visualização do padrão cíclico presente neles. Comparando os resultados dos dados brutos com os atuais, observamos que tanto a ACF quanto a PACF são semelhantes, aparentemente o modelo STL emulou o comportamento periódico dos dados brutos, mas “corrigindo” os picos para os lags múltiplos de 12.\nEm resumo, a análise dessa componente sazonal pode contribuir para uma identificação mais nítida do comportamento dos dados brutos através das caracteristicas da sazonalidade identificada pelo modelo STL ao construir essa componente.\nLembrando que existem dois tipos de sazonalidade, a aditiva e a multiplicativa. A sazonalidade aditiva inclui componentes \\(AR(.)\\) e \\(MA(.)\\) degenerados, o que resulta em picos de significância estatística isolados na ACF e PACF. Por outro lado, a sazonalidade multiplicativa causa significância não apenas no “coeficiente principal”, mas também em períodos próximos a ele, como ilustrado no exemplo atual.\nA PACF mostra significância até a primeira repetição do ciclo de 12 meses. É bastante provável que esse \\(AR(p)(1)_{12}\\) seja invertível e esteja causando o \\(MA(q)(Q)_{12}\\) com alta persistência nos lags múltiplos de 12, como indicado na ACF.\n\n\n\nDados Diferenciados\n\nComo mencionado anteriormente, uma diferenciação foi suficiente para tornar os dados estacionários, e agora eles exibem um comportamento semelhante ao de um ruído branco, como indicado pelo teste de Box-Cox. Em um contexto de modelagem ARIMA, uma vez que a estacionariedade é alcançada, o próximo passo é aplicar as funções de autocorrelação e autocorrelação parcial para determinar os parâmetros do modelo.\nAlém disso, esse comportamento é um indicativo de que a tendência pode ser de natureza aditiva.\n\n\n\n\n\nQuando os gráficos ACF e PACF apresentam resultados significativos apenas no lag 1, isso geralmente indica uma estrutura de série temporal simples e sugere que os valores da série temporal estão fortemente correlacionados apenas com seus valores imediatamente anteriores, um indicativo comum de que um modelo autorregressivo de primeira ordem, AR(1), pode ser apropriado para modelar os dados.\nPortanto, neste cenário podemos começar considerando um modelo ARIMA com \\(p=1\\) (ordem autoregressiva) e d=1 (uma diferenciação).\nNo entanto, é importante notar que essa é apenas uma primeira aproximação na escolha dos parâmetros do modelo ARIMA. A seleção final dos parâmetros geralmente envolve tentativa e erro, além de critérios de seleção de modelos, como AIC (Akaike Information Criterion) ou BIC (Bayesian Information Criterion), para escolher o modelo que melhor se ajusta aos dados."
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#suavização-exponencial-simples",
    "href": "posts/serie_ouro/exp_esmooth/index.html#suavização-exponencial-simples",
    "title": "Cotação do ouro - Parte 3",
    "section": "Suavização Exponencial Simples",
    "text": "Suavização Exponencial Simples\nPodendo ser entendida como um “meio termo” entre o método de Naïve, onde só a medida mais próxima teria inportância \\((\\hat{y}_{T+h|T} = y_T)\\), e uma média simples, onde todas as medidas tem a mesma importância \\((\\hat{y}_{T+h|T} = \\frac{1}{T} \\sum_{t=1}^{T} y_t)\\). A suavização exponencial simples tem os seus valores calculados usando médias ponderadas, onde os pesos diminuem exponencialmente à medida que as observações “envelhecem”. Assim o valor suavizado da série é dado por:\n\\[y_{T+1|T} = \\alpha y_T + \\alpha (1 - \\alpha) y_{T-1} + \\alpha (1 - \\alpha)^2 y_{T-2} + \\ldots,\\]\nUm modo de representar este método é através de suas componentes. Para a suavização exponencial simples, o único componente incluído é o nível, \\(\\ell_t\\).\nEquação de Previsão\n\\[\\hat{y}_{t+h|t} = \\ell_t,\\]\nEquação de Suavização\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha) \\ell_{t-1},\\]\nonde \\(0 \\leq \\alpha \\leq 1\\) é o parâmetro de suavização. A previsão de um passo à frente para o tempo T+1 é uma média ponderada de todas as observações na série \\(y_1, \\ldots, y_T\\). A taxa pela qual os pesos diminuem é controlada pelo parâmetro \\(\\alpha\\). E \\(\\ell_t\\) representa o nível (ou o valor suavizado) da série no tempo \\(t\\)."
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-tendência",
    "href": "posts/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-tendência",
    "title": "Cotação do ouro - Parte 3",
    "section": "Suavização Exponencial com Tendência",
    "text": "Suavização Exponencial com Tendência\nO próximo passo envolve a inclusão dos casos em que há presença de tendência nos dados. Nesse cenário, as equações são atualizadas para:\nEquação de previsão\n\\[y_{t+h|t} = \\ell_t + h b_t\\]\nEquação de suavização (nível)\n\\[ℓ_t = αy_t + (1 - α)(ℓ_{t-1} + b_{t-1})\\]\nEquação da tendência\n\\[b_t = \\beta^* (\\ell_t - \\ell_{t-1}) + (1 - \\beta^*) b_{t-1}\\]\nOnde \\(b_t\\) denota uma estimativa da tendência (inclinação) da série no tempo \\(t\\), e \\(\\beta^*\\) é o parâmetro de suavização para a tendência, \\((0 \\leq \\beta^* \\leq 1)\\).\nAssim como na suavização exponencial simples, a equação de \\(\\ell_t\\) aqui é uma média ponderada da observação \\(y_t\\) e da previsão de treinamento de um passo à frente para o tempo \\(t\\), dada por \\(\\ell_{t-1} + b_{t-1}\\). A equação de \\(b_t\\) é uma média ponderada da tendência estimada no tempo \\(t\\) com base em \\(\\ell_t - \\ell_{t-1}\\) e \\(b_{t-1}\\) é a estimativa anterior da tendência. Assim, a função de previsão não é mais plana, mas apresenta tendência. A \\(h-\\texttt{previsão}\\) à frente é igual ao último nível estimado mais \\(h\\) vezes o último valor estimado da tendência. Portanto, as previsões são uma função linear de \\(h\\)."
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-sazonalidade",
    "href": "posts/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-sazonalidade",
    "title": "Cotação do ouro - Parte 3",
    "section": "Suavização Exponencial com Sazonalidade",
    "text": "Suavização Exponencial com Sazonalidade\nA versão mais completa desta família de modelos engloba uma equação para capturar a componente sazonal. Existem duas variações deste método, aditivo e multiplicativo, que diferem na natureza do componente sazonal. O método aditivo é preferível quando as variações sazonais são aproximadamente constantes ao longo da série, enquanto o método multiplicativo é preferível quando as variações sazonais estão mudando proporcionalmente ao nível da série.\nA versão aditiva é descrita da seguinte forma:\n\\[\\begin{align*}\n\\hat{y}_{t+h|t} &= \\ell_t + h b_t + s_{t-m(k+1)} \\\\\n\\ell_t &= \\alpha (y_t - s_t - m) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\\nb_t &= \\beta^* (\\ell_t - \\ell_{t-1}) + (1 - \\beta^*) b_{t-1} \\\\\ns_t &= \\gamma (y_t - \\ell_{t-1} - b_{t-1}) + (1 - \\gamma) s_{t-m} \\\\\n\\end{align*}\\]\nOnde \\(k\\) é a parte inteira de \\(\\frac{{h - 1}}{m}\\), o que garante que as estimativas dos índices sazonais usados para previsões provenham do último ano da amostra. A equação de nível mostra uma média ponderada entre a observação ajustada sazonalmente \\(y_t - s_t - m\\) e a previsão não sazonal \\(\\ell_{t-1} + b_{t-1}\\) para o tempo \\(t\\). A equação de tendência é idêntica ao método linear de Holt (tópico anterior). A equação sazonal mostra uma média ponderada entre o índice sazonal atual \\(y_t - \\ell_{t-1} - b_{t-1}\\) e o índice sazonal do mesmo período do ano anterior (ou seja, \\(m\\) períodos atrás).\nA equação para o componente sazonal é frequentemente expressa como:\n\\[\ns_t = \\gamma^* (y_t - \\ell_t) + (1 - \\gamma^*) s_{t-m}\n\\]\nSe substituirmos \\(\\ell_t\\) a partir da equação de suavização para o nível, apresentada acima, obtemos:\n\\[\ns_t = \\gamma^* (1 - \\alpha) (y_t - \\ell_{t-1} - b_{t-1}) + [1 - \\gamma^* (1 - \\alpha)] s_{t-m}\n\\]\no que é idêntico à equação de suavização para o componente sazonal com \\((\\gamma = \\gamma^* (1 - \\alpha))\\). Assim, a restrição usual para o parâmetro é \\((0 \\leq \\gamma^* \\leq 1)\\), o que se traduz em \\((0 \\leq \\gamma \\leq 1 - \\alpha)\\).\nLembrando que esta é apenas uma simplificação da aplicação do método aditivo de Holt-Winters. Para uma discussão mais detalhada você pode consultar o capítulo 8 do livro “Forecasting: Principles and Practice” e as referências ali mencionadas."
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#visualizando-o-plano-de-validação-cruzada",
    "href": "posts/serie_ouro/exp_esmooth/index.html#visualizando-o-plano-de-validação-cruzada",
    "title": "Cotação do ouro - Parte 3",
    "section": "Visualizando o Plano de Validação Cruzada",
    "text": "Visualizando o Plano de Validação Cruzada\n\ncv %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#calibrando",
    "href": "posts/serie_ouro/exp_esmooth/index.html#calibrando",
    "title": "Cotação do ouro - Parte 3",
    "section": "Calibrando",
    "text": "Calibrando\n\nO processo de calibração estabelece as bases para atingir precisão e definir os intervalos de confiança das previsões. Isso é feito através do cálculo das previsões e da análise dos resíduos com base nos dados de teste.\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(cv$splits[[1]]))"
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#conjunto-de-teste-e-acurácia",
    "href": "posts/serie_ouro/exp_esmooth/index.html#conjunto-de-teste-e-acurácia",
    "title": "Cotação do ouro - Parte 3",
    "section": "Conjunto de Teste e Acurácia",
    "text": "Conjunto de Teste e Acurácia\n\nUtilizando os dados que foram calibrados, o próximo passo é realizar previsões com os modelos e compará-las ao conjunto de teste. Isso será realizado utilizando as funções modeltime_forecast e plot_modeltime_forecast, idealizadas para simplificar o processo de previsão e criação de gráficos dos dados originais e dos resultados dos modelos.\n\ncalib_table %&gt;% \nmodeltime::modeltime_forecast(\n    new_data = testing(cv$splits[[1]]),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast(.interactive = TRUE)\n\n\n\n\n\nAs métricas de cada modelo são obtidas através da função modeltime_accuracy, que simplifica o cálculos das métricas de precisão.\n\ncalib_table %&gt;% \n  modeltime::modeltime_accuracy() %&gt;% \n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nETS(M,N,N)\nTest\n21.60\n12.09\n5.36\n12.94\n22.64\nNA\n    2\nSEASONAL DECOMP: ETS(M,N,N)\nTest\n14.91\n8.38\n3.70\n8.77\n15.53\n0.60\n    3\nETS(M,A,A)\nTest\n13.09\n7.35\n3.25\n7.66\n13.84\n0.56\n    4\nETS(M,A,A)\nTest\n7.52\n4.18\n1.87\n4.30\n8.64\n0.61\n    5\nETS(M,AD,A)\nTest\n11.21\n6.25\n2.78\n6.50\n12.27\n0.51\n    6\nETS(M,A,A)\nTest\n7.26\n4.06\n1.80\n4.18\n8.50\n0.57\n  \n  \n  \n\n\n\n\nO modelo ajustado automaticamente (\\(\\texttt{ETS()}\\)) claramente não capturou o comportamento dos dados, retornando como previsão o último valor do conjunto de treino (naïve forecast). Por outro lado, os demais modelos, apesar de apresentarem diferentes níveis de suavidade, parecem ter desempenhado bem na reprodução dos dados de teste. Os modelos: com múltiplas sazonalidades, \\(\\texttt{ETS(M,A,A)}\\) com parâmetros \\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\) ajustados automaticamente e com o parâmetro \\(\\gamma\\) igual a 0.5 (repectivamente modelos 2, 3 e 5), mostram formatos e desempenhos semelhantes. Enquanto os modelos \\(\\texttt{ETS(M,A,A)}\\) com parâmetro \\(\\gamma\\) igual a 0.1 e 0.7 (modelos 4 e 6) apresentam os melhores desempenhos, apesar de serem as curvas com maior e menor nível de suavidade."
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#validação-cruzada",
    "href": "posts/serie_ouro/exp_esmooth/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 3",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nPara uma melhor compreenção do desempenho e estabilidade dos modelos, serão utilizados os folders criados no início deste post, onde os modelos serão novamente ajustados e testados em diferentes janelas temporais de ‘treino’ e previsão.\nPara realizar esta etatapa sera utilizada a função modeltime_fit_resamples, idealizada para ajustar os modelos e criar previsões iterativamente a partir das especificações contidas em uma objeto modeltime_table e de conjuntos de reamostragem (CV-folders).\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\n#resamples_fitted\n\nPara avaliar os modelos será utilizada a função plot_modeltime_resamples. Essa função plota as metricas de cada um dos modelos em relação a cada um dos conjuntos de reamostragem. A opção iterativa do gráfico é uma maneira conveniente de avaliar o desempenho dos modelos de forma mais detalhada, permitindo uma avaliação individual e comparativa.\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = TRUE\n    )\n\n\n\n\n\nEm linhas gerais todos os modelos tiveram um comportamento muito parecido em relação aos diferentes folder, apresentando bom desempenho nos quatro primeiros folder e uma piora gradativa nos demais conjuntos. Esse comporatamento indica que os modelos tem uma boa generalização para a parte dos dados dominada pelas componentes periódicas. É possível observar que o desempenho dos modelos começa a diminuir quando os conjuntos de treino passam a ter uma menor quantidade de dados dentro do período dominado pelas componentes periódicas e diminui mais ainda quando os conjuntos de teste passa a estar na parte dos dados onde a presença de tendência é mais significativa.\nO modelo com o melhor desempenho foi \\(\\texttt{ETS(A,M,M)}\\) com os parâmetros \\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\) ajustados de modo automático (modelo 3). Entretanto, com excessão do primeiro folder o desempenho dos modelos não difere de modo substâcial, inclusive para o modelo ajustado totalmente de forma automática."
  },
  {
    "objectID": "posts/serie_ouro/exp_esmooth/index.html#análise-dos-resíduos",
    "href": "posts/serie_ouro/exp_esmooth/index.html#análise-dos-resíduos",
    "title": "Cotação do ouro - Parte 3",
    "section": "Análise dos Resíduos",
    "text": "Análise dos Resíduos\n\nA análise de resíduos é uma etapa importante na avaliação de modelos. Ela ajuda a determinar se o modelo está capturando adequadamente as estruturas dos dados. Os resíduos que serão avaliados são em relação aos resultados dos modelos ajustados nos dados contidos no primeiro folder.\nA função plot_modeltime_residuals oferece um modo prático de avaliar os resíduos dos modelos que foram criados.\n\ncalib_table %&gt;% \n  modeltime_residuals() %&gt;% \n  plot_modeltime_residuals(\n     .type = \"timeplot\",\n     .interactive = FALSE\n     )\n\n\n\n\n\ncalib_table %&gt;% \n  modeltime_residuals() %&gt;% \n  plot_modeltime_residuals(\n     .type = \"acf\",\n     .interactive = FALSE\n     )\n\n\n\n\nEspera-se que os resíduos de um modelo exibam um comportamento de ruído branco, com média zero e variância constante. Qualquer padrão observado nos resíduos pode indicar que o modelo não está capturando de forma adequada a estrutura subjacente da série temporal.\nOs resíduos dos modelos criados não correspondem exatamente ao comportamento esperado. O gráfico de resíduos não exibe um padrão completamente aleatório, seguindo um padrão próximo aos dados de teste, e os gráficos de ACF e PACF mostram uma autocorrelação residual significativa para o primeiro lag. Apesar disso, é possível considerar que os resíduos estão próximos do comportamento esperado, sendo a maioria dos modelos capaz de capturar as principais estruturas dos dados de forma satisfatória."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#adf",
    "href": "posts/R/serie_ouro/EDA2/index.html#adf",
    "title": "Cotação do ouro - Parte 2",
    "section": "Dickey-Fuller (ADF)",
    "text": "Dickey-Fuller (ADF)\n\nO teste de Dickey-Fuller Aumentado (ADF, na sigla em inglês) é empregado para verificar a existência de raízes unitárias em séries temporais univariadas. A hipótese nula deste teste pressupõe a não estacionariedade; logo, se o valor-p do teste for inferior a um nível de significância especificado, temos base para rejeitar a hipótese nula e inferir que a série é estacionária.\nA equação do teste ADF é dada por:\n\\[\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\ldots + \\delta_p \\Delta y_{t-p} + \\varepsilon_t\\]\nonde:\n\n\\(\\Delta y_t \\text{ é a diferença entre } y_t \\text{ e } y_{t-1}\\),\n\\(\\alpha\\) é o termo constante,\n\\(\\beta\\) é o coeficiente da tendência temporal,\n\\(\\gamma\\) é o coeficiente de \\(y_{t-1}\\),\n\\(\\delta_1\\), \\(\\delta_2\\), \\(\\ldots\\), \\(\\delta_p\\) são os coeficientes das diferenças defasadas de \\(y_t\\),\n\\(\\varepsilon_t\\) é o termo de erro.\n\nPara realizar o teste ADF, utilizaremos a função adf.test do pacote tseries.\n\ntseries::adf.test(ts(fit$value),alternative =\"stationary\") \n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts(fit$value)\nDickey-Fuller = -2.4719, Lag order = 5, p-value = 0.3803\nalternative hypothesis: stationary\n\n\nO teste ADF não encontrou evidências que justifiquem a rejeição da hipótese nula de não estacionariedade."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#kpss",
    "href": "posts/R/serie_ouro/EDA2/index.html#kpss",
    "title": "Cotação do ouro - Parte 2",
    "section": "Kwiatkowski-Phillips-Schmidt-Shin (KPSS)",
    "text": "Kwiatkowski-Phillips-Schmidt-Shin (KPSS)\n\nO teste KPSS visa determinar se uma série temporal é estacionária em relação a uma tendência determinística, em contraposição à estacionariedade em torno de um valor médio constante. A hipótese nula do teste KPSS pressupõe que a série é estacionária. Portanto, se o valor-p obtido for superior ao nível de significância, não temos justificativa para rejeitar a hipótese nula, o que indica que a série é estacionária em relação a uma tendência. No entanto, se o valor-p for inferior ao nível de significância, podemos rejeitar a hipótese nula e concluir que a série não é estacionária em relação a uma tendência, implicando que a tendência é estocástica.\nA estatística do teste KPSS é definida da seguinte forma:\n\\[KPSS = \\frac{T^2 \\cdot \\hat{\\sigma}^2}{\\sum_{t=1}^{T} S_t^2}\\]\nOnde:\n\n\\(T\\) é o número de observações na série temporal.\n\\(S_t\\) é a soma cumulativa das diferenças entre as observações e a estimativa da tendência local em cada ponto na série temporal. Calculado como: \\[S_t = \\sum_{i=1}^{t} (X_i - \\hat{m}_t)\\].\n\\(\\hat{\\sigma}^2\\) é uma estimativa da variância da série temporal.\n\nOnde \\(X_i\\) é a observação no tempo \\(i\\) e \\(\\hat{m}_t\\) é a estimativa da tendência local no tempo \\(t\\).\nPara o teste KPSS será usado a função unitroot_kpss do pacote fable.\n\nfit %&gt;% \n  fabletools::features(value, unitroot_kpss) %&gt;% \n  set_tab(\"Teste KPSS\")\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n1.8768\n0.01\n\n\n\n\n\n\n\n\nO teste kpss, apresenta evidência para rejeitar a hipótese nula, que nesse caso é de estacionariedade.\nOs dois testes indicam a não estacionariedade dos dados, uma característica desfavorável que, em alguns casos, é essencial para certos tipos de modelos. A seguir examinaremos algumas das principais abordagens para transformar os dados em um estado estacionário."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#decomposição",
    "href": "posts/R/serie_ouro/EDA2/index.html#decomposição",
    "title": "Cotação do ouro - Parte 2",
    "section": "Decomposição",
    "text": "Decomposição\n\nApós a decomposição dos dados, como discutido na primeira postagem desta série, obtemos várias representações dos dados originais que capturam as estruturas identificadas. Neste ponto, estou examinando a componente ‘remainder’ do modelo STL, que, teoricamente, teve as componentes de tendência e de sazonalidade retiradas. Como mencionei anteriormente, essa componente exibe um comportamento espectral muito semelhante ao dos dados brutos, incluindo elementos cíclicos ou quase cíclicos.\n\n\n\n\n\nVamos usar novamente o teste KPSS para verificar a estacionariedade dos dados.\n\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n0.0361\n0.1\n\n\n\n\n\n\n\n\nO teste não apresentou evidências que justificassem a rejeição da hipótese nula de estacionariedade, o que sugere que a abordagem foi eficaz em tornar os dados estacionários. No entanto, como indicado durante a análise espectral e teste Ljung-Box , esses dados não exibem um comportamento de ruído branco e podem conter estruturas significativas."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#box-cox",
    "href": "posts/R/serie_ouro/EDA2/index.html#box-cox",
    "title": "Cotação do ouro - Parte 2",
    "section": "Box Cox",
    "text": "Box Cox\n\nA transformação de Box-Cox é uma técnica estatística usada para estabilizar a variância e tornar uma distribuição mais próxima da normalidade. Ela é frequentemente aplicada em séries temporais ou outras análises estatísticas quando os dados exibem heteroscedasticidade (variação não constante) ou não seguem uma distribuição normal.\nSua forma geral pode ser definida matematicamente da seguinte maneira:\nPara um conjunto de dados \\((x_1, x_2, x_3, \\ldots, x_n)\\) a transformação de Box-Cox é definida como:\n\\[y_i =\n\\begin{cases}\n\\frac{x_i^\\lambda - 1}{\\lambda}, & \\text{se } \\lambda \\neq 0 \\\\\n\\ln(x_i), & \\text{se } \\lambda = 0\n\\end{cases}\\]\nOnde:\n\n\\(y_i\\) é o valor transformado do dado \\(x_i\\)\nO parâmetro \\(\\lambda\\) é estimado através da função de verossimilhança e avaliação da adequação do modelo.\n\nQuando \\(\\lambda\\) é igual a zero, a transformação se torna uma transformação logarítmica natural (\\(ln(x_i)\\)) Quando \\(\\lambda\\) não é igual a zero, a transformação é uma potência do dado original (\\(x_i^\\lambda\\)) com uma correção para garantir que a transformação seja bem definida para todos os valores de \\(x_i\\).\nA escolha do valor ideal para λ é geralmente feita visando a maximização da normalidade dos dados transformados ou a estabilização da variância. Uma prática comum envolve testar vários valores dentro de um intervalo específico e aplicar a transformação a cada valor da série. Nessa tarefa, estou utilizando a função forecast::BoxCox.lambda do pacote forecast, que possibilita a seleção automática desse parâmetro. Essa função determina o valor de λ de modo a maximizar o perfil da verossimilhança logarítmica de um modelo linear ajustado aos dados. Para dados não sazonais, é ajustada uma tendência temporal linear, enquanto para dados sazonais, é usado um modelo linear de tendência temporal com variáveis sazonais dummy.\n\nlambda &lt;- round(forecast::BoxCox.lambda(fit$value), digits = 2)\nlambda\n\n[1] 0.38\n\n\n\nfit %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   ggplot(aes(index,box_cox_close))+\n   geom_line()+\n   ggtitle(\"Transformação Box Cox\")+\n   xlab(\"\")+\n   ylab(\"\")\n\n\n\n\n\nfit %&gt;%\n   mutate(box_cox_close = fabletools::box_cox(value, lambda=lambda)) %&gt;% \n   features(box_cox_close, unitroot_kpss) %&gt;% \n   set_tab(\"Teste KPSS\")\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n1.8652\n0.01\n\n\n\n\n\n\n\n\nA transformação não modificou a forma dos dados, apenas o range de valores do eixo y. Entretanto, como o teste KPSS indicou isso não foi o suficiente para tornar os dados estacionários."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#diferenciação",
    "href": "posts/R/serie_ouro/EDA2/index.html#diferenciação",
    "title": "Cotação do ouro - Parte 2",
    "section": "Diferenciação",
    "text": "Diferenciação\n\nA diferenciação em séries temporais é uma técnica amplamente empregada para converter dados não estacionários em um formato mais apropriado para análise e modelagem. Essa técnica consiste na subtração dos valores consecutivos na série temporal, com o propósito de eliminar estruturas de tendência e padrões sazonais. Ao aplicar a diferenciação, a série é transformada em uma nova série de diferenças, com a esperança de torná-la estacionária.\nA remoção das estruturas mencionadas é geralmente alcançada por meio das diferenciações de primeira ordem e sazonal, e ou pela combinação e repetição dos procedimentos. A diferenciação de primeira ordem é realizada subtraindo o valor atual pelo valor anterior na série temporal e é eficaz na eliminação de tendências lineares nos dados. Por outro lado, a diferenciação sazonal envolve o cálculo das diferenças entre os valores da série no mesmo período, mas em anos diferentes, o que é útil para eliminar efeitos sazonais. Isso é realizado com o uso do “lag sazonal”, que representa o número de períodos em uma temporada. Por exemplo, se estiver lidando com dados mensais e a sazonalidade for anual, o lag sazonal seria igual a 12.\nEstá técnica pode ser descrita da seguinte forma:\n\\[\\Delta y_t = y_t - y_{t-1}\\]\nOnde:\n\n\\(\\Delta\\) representa a diferença entre o valor atual \\(y_t\\) e o valor anterior \\(y_{t-1}\\) na série tempora.\n\nPara se determinar o número de diferenças necessárias para tornar os dados estacionários usaremos a função unitroot_ndiffs, que tem com base o teste ADF, anteriormente explicado. Lembrando que o termo “unit root” (raiz unitária) se refere a uma característica de séries temporais não estacionárias. A presença de uma raiz unitária indica que a série não reverte rapidamente perturbações, tornando-a menos previsíveis e mais suscetíveis a flutuações, o que, de fato, pode tornar a análise e a modelagem mais desafiadoras.\n\nfit %&gt;% \n  fabletools::features(value,  unitroot_ndiffs) %&gt;% \n  set_tab(\"Número de Diferenças\")\n\n\n\nNúmero de Diferenças\n\n\n.model\nndiffs\n\n\n\n\nfeasts::STL(value)\n1\n\n\n\n\n\n\n\n\nO teste sinaliza a necessidade de realizar uma diferenciação para tornar a série estacionária.\n\nfit %&gt;% \n  mutate(\n    diff_close = tsibble::difference(value)\n    ) %&gt;% \n  ggplot(aes(index,diff_close))+\n  geom_line()+\n  ggtitle(\"Dados diferenciados\")+\n  xlab(\"\")+\n  ylab(\"\")\n\n\n\n\nApós a transformação dos dados, aparentemente, não parece haver nenhuma estrutura remanescente.\n\n\n\n\nTeste KPSS\n\n\n.model\nkpss_stat\nkpss_pvalue\n\n\n\n\nfeasts::STL(value)\n0.3244\n0.1\n\n\n\n\n\n\n\n\nO teste KPSS apresenta evidências que sustentam a aceitação da hipótese nula de estacionariedade.\n\nfit %&gt;%\n   mutate(\n     diff_close = tsibble::difference(value)\n     ) %&gt;%\n  fabletools::features(value, ljung_box, lag = 78) %&gt;% \n  set_tab(\"Teste LJUNG-BOX\")\n\n\n\nTeste LJUNG-BOX\n\n\n.model\nlb_stat\nlb_pvalue\n\n\n\n\nfeasts::STL(value)\n2768.323\n0\n\n\n\n\n\n\n\n\nO teste Ljung-Box indica que a série se comporta como um ruído branco, ou seja, não apresenta autocorrelação significativa."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#função-de-autocorrelação-acf",
    "href": "posts/R/serie_ouro/EDA2/index.html#função-de-autocorrelação-acf",
    "title": "Cotação do ouro - Parte 2",
    "section": "Função de Autocorrelação (ACF)",
    "text": "Função de Autocorrelação (ACF)\n\nA ACF pode ser representada da seguinte forma:\nDada uma série temporal \\({X_t}\\), onde t representa o tempo, a autocorrelação para um atraso (lag) \\(k\\) é calculada da seguinte maneira:\n\\[ρ(k) = Cov(X_t, X_{t-k}) / (σ(X_t) * σ(X_{t-k}))\\]\nOnde:\n\n\\(ρ(k)\\) é a autocorrelação para o atraso \\(k\\).\n\\(Cov(X_t, X_{t-k})\\) é a covariância entre \\(X_t\\) e \\(X_{t-k}\\), ou seja, a medida de como as observações estão relacionadas em \\(t\\) e \\(t-k\\).\n\\(σ(X_t)\\) é o desvio padrão de \\(X_t\\), que mede a variabilidade dos valores em \\(t\\).\n\\(σ(X_{t-k})\\) é o desvio padrão de \\(X_{t-k}\\), que mede a variabilidade dos valores em \\(t-k\\).\n\nEm resumo, a autocorrelação para um determinado atraso \\(k\\) é a covariância normalizada pela variabilidade (desvio padrão) das observações separadas por esse atraso. Ela varia de -1 a 1, onde valores próximos de 1 indicam forte correlação positiva, valores próximos de -1 indicam forte correlação negativa e valores próximos de 0 indicam ausência de correlação."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#função-de-autocorrelação-parcial-pacf",
    "href": "posts/R/serie_ouro/EDA2/index.html#função-de-autocorrelação-parcial-pacf",
    "title": "Cotação do ouro - Parte 2",
    "section": "Função de Autocorrelação Parcial (PACF)",
    "text": "Função de Autocorrelação Parcial (PACF)\n\nA PACF é calculada da seguinte maneira:\nSuponha que temos uma série temporal \\(x_t\\) para \\(t = 1, 2, \\ldots, N\\). O PACF de atraso \\(k\\) (\\(PACF_k\\)) é definido como:\n\\[PACF_k = \\frac{\\gamma_{t,t-k} - \\sum_{i=1}^{k-1} \\phi_i \\gamma_{t-k,t-k+i}}{1 - \\sum_{i=1}^{k-1} \\phi_i PACF_i}\\]\nonde:\n\n\\(\\gamma_{t,t-k}\\) é a função de autocovariância entre \\(x_t\\) e \\(x_t-k\\).\n\\(\\phi_i\\) são os coeficientes estimados de um modelo AR de ordem \\(i\\), onde \\(i = 1, 2, \\ldots, k-1\\)\n\nA função PACF é calculada iterativamente, começando com \\(k\\) = 1 e progredindo até o atraso máximo desejado. Ela mede a correlação entre uma observação em um determinado momento e observações em intervalos de tempo anteriores, removendo o efeito de observações em lags intermediários. Isso ajuda a identificar a estrutura de dependência direta, indicando o possível número de termos autorregressivos a serem incluídos em um modelo AR."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA2/index.html#algumas-aplicações",
    "href": "posts/R/serie_ouro/EDA2/index.html#algumas-aplicações",
    "title": "Cotação do ouro - Parte 2",
    "section": "Algumas Aplicações",
    "text": "Algumas Aplicações\n\n\nModelagem de Séries Temporais: A ACF e a PACF são usadas para identificar a ordem adequada (p, d, q) de modelos da fámilia ARIMA, onde “p” é a ordem do componente AR, “d” é a ordem da diferenciação e “q” é a ordem do componente MA. Lembrando que ACF é usada para evidência de MA(q) e suas sazonalidades e a PACF é usada para identificação de AR(p) e suas sazonalidades.\nIdentificação de Padrões: Além de sua aplicação em modelagem, a ACF e a PACF também são úteis para identificar padrões temporais nas séries temporais, como sazonalidades, e tendências.\n\nA presença de autocorrelação significativa em lags variados nas funções ACF pode sugerir a presença de tendência, e que a série não é estacionária.\nSe houver sazonalidade, você geralmente verá picos significativos na ACF em múltiplos lags (intervalos de tempo) que correspondem aos períodos sazonais. Por exemplo, em uma série mensal com sazonalidade anual, você esperaria ver picos nas defasagens 12, 24, 36, etc.\nO comportamento dos picos significativos pode ajudar a indentificar o tipo de sazonalidade (aditiva, multiplicativa) presente nos dados.\n\nDiagnóstico de Resíduos: Após ajustar um modelo a uma série temporal, a ACF e a PACF dos resíduos (diferença entre os valores observados e os valores previstos pelo modelo) podem ser usadas para verificar se há estrutura remanescente nos resíduos.\n\nEm resumo, ACF e PACF são ferramentas essenciais na análise de séries temporais, desempenhando um papel fundamental na identificação da estrutura de dependência temporal e na seleção dos parâmetros apropriados para a modelagem de modelos estatísticos.\n\n\nDados Originais\n\nAs funções ACF e PACF não são geralmente a primeira abordagem quando se suspeita que os dados não sejam estacionários, como neste caso. No entanto, para fins ilustrativos, vou usá-las nos dados brutos.\n\nmax_lag &lt;- nrow(fit)-1\n\nacf_plot &lt;- \nfit %&gt;% \n  feasts::ACF(value, lag_max = max_lag) %&gt;% \n  feasts::autoplot()\n\n\npacf_plot &lt;- \nfit %&gt;% \n  feasts::PACF(value, lag_max = max_lag) %&gt;% \n  feasts::autoplot()\n\n\nggpubr::ggarrange(acf_plot,pacf_plot, ncol=2)\n\n\n\n\nO Gráfico de ACF revela correlações significativas em lags próximos, com uma diminuição gradual que não se aproxima rapidamente de zero. Esse padrão sugere a presença de tendência nos dados, uma característica geralmente associada à não estacionariedade. A transição das correlações de positivas para negativas parece estar relacionada às mudanças no comportamento da tendência ao longo do tempo.\nPor outro lado, os resultados da função PACF mostram valores significativos apenas nos lags 1 e 2, indicando que os efeitos observados no Gráfico de ACF podem ser atribuídos principalmente à presença de tendência nos dados.\nEm uma série estacionária, é comum que os gráficos de ACF e PACF apresentem uma rápida diminuição após um pequeno número de defasagens, sem exibir padrões significativos. A ocorrência de padrões irregulares nesses gráficos sugere a possibilidade de ruído nos dados ou outros elementos não estacionários, corroborando assim os demais resultados das análises realizadas até o momento.\n\n\n\nComponente ‘remainder’\n\nConforme já verificado, a componente ‘remainder’ demonstra estacionariedade de acordo com o teste KPSS, mas não exibe um comportamento de ruído branco conforme indicado pelo teste Ljung-Box. Isso pode ser atribuído, provavelmente, à presença de componentes cíclicas ou quase cíclicas.\nNormalmente, as funções ACF e PACF são utilizadas em dados após a aplicação do operador de diferença. No entanto, a análise dos dados brutos, sem a componente de tendência, pode ser útil para ilustrar várias situações. Por exemplo, pode ajudar a melhorar a compreensão das estruturas cíclicas ou quase cíclicas nos dados. Em casos em que os dados brutos não exibem uma tendência intrínseca e já demonstram estacionariedade. Ou quando mesmo após a aplicação de diferenciação para torná-los estacionários, ainda podem persistir indícios de sazonalidade nos dados, e a estacionariedade pode não ser completamente alcançada.\n\n\n\n\n\nOs picos positivos nos dois primeiros lags no gráfico de ACF sugerem uma forte correlação entre os valores no tempo atual e os valores no tempo imediatamente anterior. Essa observação pode ser indicativo de um componente \\(MA(q)\\) em um modelo da família ARIMA.\nAlém disso, o gráfico mostra um comportamento senoidal, com picos significativos ou quase significativos em multiplos de cerca de 12 meses. Isso sugere a presença de um componente sazonal nas séries temporais. No entanto, é importante notar que o período não é bem definido, e os picos apresentam rápido decaimento e picos adicionais ao redor do período principal. Os valores dos picos na parte periódica são baixos (menores ou iguais a 0,5), o que pode indicar que a capacidade de previsão do componente sazonal pode ser limitada. A presença de picos negativos entre os picos positivos pode indicar algum tipo de efeito de amortecimento ou compensação nas séries temporais.\nO decaimento e a quase significância de outros picos na ACF podem resultar em raízes do polinômio característico que estão muito próximas do ciclo unitário. indício que os processos autorregressivos ou de média móvel têm dificuldade em convergir, o que pode resultar em previsões imprecisas e modelos instáveis.\nPara lidar com raízes próximas ao ciclo unitário em modelos ARIMA, é fundamental considerar alternativas, como a adoção de um modelo SARIMA (Seasonal ARIMA), a fim de eficazmente incorporar a sazonalidade e aprimorar a estabilidade das previsões. Além disso, é apropriado realizar uma diferenciação sazonal, com o período identificado, para tornar a série temporal mais estacionária. Recomenda-se, também, conduzir uma análise minuciosa dos resultados do modelo para assegurar sua adequação aos dados específicos em questão. Isso envolve a validação dos resíduos do modelo, que por sua vez requer o uso da ACF e PACF.\nA PACF indica que há significância até a segunda repetição do ciclo de 12 meses. No entanto, a natureza exata desse comportamento não é clara, e os valores, com exceção do primeiro pico, são baixos. Em resumo, os dados apresentam um comportamento complexo e ruidoso, como já evidenciado em resultados anteriores.\nCom base nas análises da ACF e PACF, é razoável suspeitar que um modelo SARIMA com termos \\(MA(q)(Q)\\), onde \\(q=1, 2\\) e \\(Q=1\\), com uma componente sazonal de 12 meses, e \\(AR(p)(P)\\) onde \\(p=1, 2, 4, 5\\) e \\(P=1\\) com a mesma componente sazonal de 12 meses, podem ser apropriados para modelar essa série temporal. No entanto, a escolha final do modelo SARIMA dependerá de testes de diagnóstico adicionais e validação cruzada para garantir a precisão das previsões e a adequação do modelo aos dados.\n\n\n\nDados sazonais\n\n\n\n\n\n\nA componente ‘sazonal’ criada pelo modelo STL é livres de ruído, tornando mais simples a visualização do padrão cíclico presente neles. Comparando os resultados dos dados brutos com os atuais, observamos que tanto a ACF quanto a PACF são semelhantes, aparentemente o modelo STL emulou o comportamento periódico dos dados brutos, mas “corrigindo” os picos para os lags múltiplos de 12.\nEm resumo, a análise dessa componente sazonal pode contribuir para uma identificação mais nítida do comportamento dos dados brutos através das caracteristicas da sazonalidade identificada pelo modelo STL ao construir essa componente.\nLembrando que existem dois tipos de sazonalidade, a aditiva e a multiplicativa. A sazonalidade aditiva inclui componentes \\(AR(.)\\) e \\(MA(.)\\) degenerados, o que resulta em picos de significância estatística isolados na ACF e PACF. Por outro lado, a sazonalidade multiplicativa causa significância não apenas no “coeficiente principal”, mas também em períodos próximos a ele, como ilustrado no exemplo atual.\nA PACF mostra significância até a primeira repetição do ciclo de 12 meses. É bastante provável que esse \\(AR(p)(1)_{12}\\) seja invertível e esteja causando o \\(MA(q)(Q)_{12}\\) com alta persistência nos lags múltiplos de 12, como indicado na ACF.\n\n\n\nDados Diferenciados\n\nComo mencionado anteriormente, uma diferenciação foi suficiente para tornar os dados estacionários, e agora eles exibem um comportamento semelhante ao de um ruído branco, como indicado pelo teste de Box-Cox. Em um contexto de modelagem ARIMA, uma vez que a estacionariedade é alcançada, o próximo passo é aplicar as funções de autocorrelação e autocorrelação parcial para determinar os parâmetros do modelo.\nAlém disso, esse comportamento é um indicativo de que a tendência pode ser de natureza aditiva.\n\n\n\n\n\nQuando os gráficos ACF e PACF apresentam resultados significativos apenas no lag 1, isso geralmente indica uma estrutura de série temporal simples e sugere que os valores da série temporal estão fortemente correlacionados apenas com seus valores imediatamente anteriores, um indicativo comum de que um modelo autorregressivo de primeira ordem, AR(1), pode ser apropriado para modelar os dados.\nPortanto, neste cenário podemos começar considerando um modelo ARIMA com \\(p=1\\) (ordem autoregressiva) e d=1 (uma diferenciação).\nNo entanto, é importante notar que essa é apenas uma primeira aproximação na escolha dos parâmetros do modelo ARIMA. A seleção final dos parâmetros geralmente envolve tentativa e erro, além de critérios de seleção de modelos, como AIC (Akaike Information Criterion) ou BIC (Bayesian Information Criterion), para escolher o modelo que melhor se ajusta aos dados."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#definição-do-problema",
    "href": "posts/R/serie_ouro/EDA/index.html#definição-do-problema",
    "title": "Cotação do ouro - Parte 1",
    "section": "Definição do Problema",
    "text": "Definição do Problema\n\nConforme o título deste post sugere, esta série estará relacionada à cotação do ouro, mais especificamente na criação de um modelo preditivo para o SPDR Gold Shares (GLD)."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#carregando-os-pacotes-utilizados",
    "href": "posts/R/serie_ouro/EDA/index.html#carregando-os-pacotes-utilizados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Carregando os Pacotes Utilizados",
    "text": "Carregando os Pacotes Utilizados\n\nNeste projeto, farei uso de uma ampla variedade de pacotes e, consequentemente, suas respectivas funções para realizar as tarefas necessárias. No contexto do R, é comum carregar os pacotes próximos às linhas onde suas funções serão utilizadas. No entanto, isso nem sempre é viável, o que pode tornar a identificação da origem das funções confusa. Isso não apenas dificulta a leitura do código por terceiros, mas também pode complicar a revisão do próprio autor.\n\n\nPara tornar mais fácil identificar a origem das funções em uso, o R oferece uma opção útil: nome_do_pacote::nome_da_função(). Esse padrão é considerado uma boa prática no R para indicar a origem das funções e será adotado em todas as próximas postagens, sempre que necessário.\nPara simplificar o processo de carregamento dos pacotes, utilizaremos a função p_load(), que faz parte do pacote pacman. Essa função permite carregar as bibliotecas de forma centralizada e eficiente.\n\n\npacman::p_load(tidyquant, tsibble,fabletools,fabletools,timetk,fpp3,\n               tibbletime,feasts, tidyverse, tseries, WaveletComp,\n               tsoutliers, DT, plotly, kableExtra)\n\nAntes de continuar, vou criar uma função personalizada usando o pacote kableExtra para formatar as tabelas que serão apresentadas neste e nos próximos posts. Os argumentos desta função são os dados a serem tabulados e o título da tabela.\n\nset_tab &lt;- function(dados_tab, cap_tab){\n    kableExtra::kable( x = dados_tab,\n        booktabs = TRUE,\n          escape   = FALSE,\n          digits   = 4,\n          caption  = cap_tab) |&gt; \n  kableExtra::kable_styling(latex_options =\n                c(\"striped\", \"hold_position\"),\n                position      = \"center\",\n                full_width    = F,\n                bootstrap_options =\n                   c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n}"
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#aquisição-dos-dados",
    "href": "posts/R/serie_ouro/EDA/index.html#aquisição-dos-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Aquisição dos Dados",
    "text": "Aquisição dos Dados\n\nOs dados utilizados nesta série são provenientes do Yahoo Finanças. Para acessá-los, utilizaremos a função tq_get do pacote tidyquant. Por padrão, essa função utiliza a opção stock.prices, que retorna os valores de ‘open’, ‘high’, ‘low’, ‘close’, ‘volume’ e ‘adjusted’ do Yahoo Finanças. No entanto, é possível obter outras opções, como dividendos ou informações sobre ‘split’ de ações, além de dados de outras fontes.\nQuando utilizamos o padrão da função e informamos apenas o símbolo da ação desejada, todos os dados disponíveis para o período de tempo completo são retornados.\n\n\n# Obter dados da ação de ouro (código GLD)\ngold_data &lt;- tidyquant::tq_get(\"GLD\")"
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#limpeza-dos-dados",
    "href": "posts/R/serie_ouro/EDA/index.html#limpeza-dos-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Limpeza dos Dados",
    "text": "Limpeza dos Dados\n\nA etapa de limpeza de dados é um processo fundamental na análise de dados que envolve a identificação, correção e eliminação de erros, inconsistências e valores ausentes em um conjunto de dados. Isso inclui remover duplicatas, preencher lacunas, padronizar formatos e eliminar outliers. A limpeza de dados é crucial para garantir a qualidade e confiabilidade dos dados, pois dados sujos ou corrompidos podem levar a análises imprecisas e conclusões equivocadas. Portanto, essa etapa prepara os dados para análise posterior, assegurando que estejam prontos para serem explorados e modelados de forma eficaz.\nDevido a fonte dos dados, Yahoo Finanças, amplamente reconhecida e utilizada tanto na indústria quanto por investidores individuais, podemos antecipar que encontraremos poucos problemas.\nSem mais delongas vamos iniciar a análise dos dados."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#análise-preliminar",
    "href": "posts/R/serie_ouro/EDA/index.html#análise-preliminar",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análise Preliminar",
    "text": "Análise Preliminar\n\nUma vez que os dados foram carregados, podemos começar nossa análise.\nVou começar criando uma tabela para visualizar os dados e utilizando as funções summary e str para obter informações da estrutura dos dados dentro do R e infomrações estatísticas básicas.\n\n\ngold_data %&gt;% \n  slice_head(n=5) %&gt;% \n  set_tab(cap_tab = \"Dados Brutos\")\n\n\n\nDados Brutos\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\nGLD\n2013-01-02\n163.49\n164.14\n163.14\n163.17\n10431800\n163.17\n\n\nGLD\n2013-01-03\n162.48\n162.88\n160.83\n161.20\n16117500\n161.20\n\n\nGLD\n2013-01-04\n159.52\n160.63\n158.89\n160.44\n19179800\n160.44\n\n\nGLD\n2013-01-07\n159.36\n159.96\n159.15\n159.43\n9361800\n159.43\n\n\nGLD\n2013-01-08\n160.46\n160.99\n160.01\n160.56\n7694800\n160.56\n\n\n\n\n\n\n\n\n\n\n#base::summary(gold_data)\n#utils::str(gold_data)\n\n\nO aspecto mais significativo desses resultados é a irregularidade amostral. Apesar dos resultados da função summary não indicarem a presença de NA’s (valores ausentes), é importante observar que as datas na coluna “date” contêm registros para fins de semana e feriados, que são omitidos pela fonte dos dados. Isso pode resultar em efeitos indesejados e até mesmo tornar inviáveis algumas análises.\nDentre as diversas abordagens para lidar com essa situação, eu vou optar pelo método da reamostragem, onde converteremos nossos dados que possuem uma frequência amostral diária em mensal.\nApesar dessa técnica levar a uma perda de resolução, ela pode proporcionar uma simplificação do processamento e análise, além da diminuição do ruido nos dados. Pontos que serão muito positivos levando em conta o propósito demonstrativo deste post. É importante ressaltar que, dependendo do propósito da análise, a melhor opção seria utilizar o máximo de informação possível e optar pelo “descarte” de parte das informações apenas após uma avaliação mais criteriosa."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#reamostrando-os-dados",
    "href": "posts/R/serie_ouro/EDA/index.html#reamostrando-os-dados",
    "title": "Cotação do ouro - Parte 1",
    "section": "Reamostrando os Dados",
    "text": "Reamostrando os Dados\n\nA realização da reamostragem mencionada implica na criação de médias mensais, o que transformará o conjunto de dados no formato desejado. Os passos necessários para a execução desse procedimento estão explicados nos comentários do código a seguir. A maioria das funções utilizadas a seguir faz parte do pacote dplyr.\n\n\ngold_data &lt;-\n  gold_data %&gt;% \n  # criando variáveis ano e mês para cálculo das médias mensais\n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n    )%&gt;% \n  #agrupando os dados pelas variáveis ano e mês\n  group_by(year,month) %&gt;% \n  #calculando as médias mensais\n  summarise(\n    month_mean = mean(close)\n    ) %&gt;% \n  #desagrupando os dados\n  ungroup() %&gt;% \n  #criando a variável que será usada como index temporal\n   mutate(\n     index = tsibble::make_yearmonth(year,month)\n     ) %&gt;% \n  #renomeando a coluna dos valores\n  rename(\n    value = month_mean\n    ) %&gt;%\n  #selecionando apenas as colunas de interesse\n  select(index,value) %&gt;% \n  # transformando os dados em tsibble, formato adequado para os pacotes utilizados\n  tsibble::tsibble() \n\n\n#head(gold_data)\n#class(gold_data)\n\n\ngold_data %&gt;% \n  slice_head(n=5) %&gt;% \n  set_tab(cap_tab = \"Médias mensais\")\n\n\n\nMédias mensais\n\n\nindex\nvalue\n\n\n\n\n2013 jan\n161.6733\n\n\n2013 fev\n157.6026\n\n\n2013 mar\n154.1335\n\n\n2013 abr\n143.6223\n\n\n2013 mai\n136.9909\n\n\n\n\n\n\n\n\nPronto, agora os dados estão no formato desejado."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#identificando-outliers",
    "href": "posts/R/serie_ouro/EDA/index.html#identificando-outliers",
    "title": "Cotação do ouro - Parte 1",
    "section": "Identificando Outliers",
    "text": "Identificando Outliers\n\nOutliers, ou valores atípicos, são observações em uma série temporal que se desviam significativamente do padrão ou comportamento esperado dos dados em um determinado período de tempo. Eles podem ser causados por uma variedade de razões, incluindo erros de medição, eventos incomuns, mudanças no processo subjacente ou simplesmente flutuações aleatórias extremas. Identificar e lidar com outliers é importante na análise de séries temporais, pois podem afetar negativamente a precisão dos modelos e análises. Considerando que esses dados provêm de plataformas que disponibilizam valores oficiais do mercado de ações, a eventual presença de outliers nesses dados provavelmente estará ligada a eventos notáveis, e não a qualquer tipo de erro.\nHá várias abordagens para identificar e tratar outliers em séries temporais. Optei por utilizar o pacote tsoutliers, que oferece uma opção automatizada para identificação desses valores atípicos.\n\n\ntsoutliers::tso(ts(gold_data$value)) \n\nSeries:  \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.2729\ns.e.  0.0826\n\nsigma^2 = 18.77:  log likelihood = -368.83\nAIC=741.66   AICc=741.76   BIC=747.36\n\nNo outliers were detected.\n\n\n\nBem, não foi identificado nenhum outlier, o que era esperado, considerando a fonte dos dados e o fato de que eles representam médias mensais, o que implica um certo grau de suavização."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#gráficos-da-série-temporal",
    "href": "posts/R/serie_ouro/EDA/index.html#gráficos-da-série-temporal",
    "title": "Cotação do ouro - Parte 1",
    "section": "Gráficos da Série Temporal",
    "text": "Gráficos da Série Temporal\nCom os dados preparados para análise, vou criar as primeiras visualizações gráfica.\n\nGráfico das médias mensais\n\ngold_data %&gt;%\n  ggplot2::ggplot(aes(x=index,y=value))+\n  geom_line()+\n  geom_smooth(formula = y ~ s(x, bs = \"cs\"), method = 'gam')+\n  labs(title = \"Cotação do Ouro entre 2013-2023\", )+\n  xlab(\"\")+\n  ylab(\"USD por Ação\")+\n  theme_minimal()\n\n\n\n\n\nAo analisar o gráfico das médias mensais, é possível notar que os dados apresentam uma tendência negativa de 2013 a 2016, seguida por um período aparentemente estacionário que se estende até 2019. Após esse período, há uma tendência positiva observada até aproximadamente meados de 2020, seguida por um retorno a uma aparente estacionariedade nos dados. A parte final dos dados parece exibir uma maior volatilidade. Em termos gerais, quando se considera a série temporal como um todo, os dados parecem não ser estacionários. No entanto, esse é um ponto que será discutido e avaliado mais detalhadamente posteriormente.Ao analisar o gráfico das médias mensais, é possível notar que os dados apresentam uma tendência negativa de 2013 a 2016, seguida por um período aparentemente estacionário que se estende até 2019. Após esse período, há uma tendência positiva observada até aproximadamente meados de 2020, seguida por um retorno a uma aparente estacionariedade nos dados. A parte final dos dados parece exibir uma maior volatilidade. Nesta primeira visualização, os dados não apresentam nenhum padrão sazonal ou cíclico evidente. Em termos gerais, quando se considera a série temporal como um todo, os dados parecem não ser estacionários. No entanto, esse é um ponto que será discutido e avaliado mais detalhadamente posteriormente.\n\n\n\nGráficos sazonais\n\nUma representação sazonal é semelhante a um gráfico temporal, exceto que os dados são plotados em relação às “estações” individuais em que os dados foram observados.\n\nfeasts::gg_season(gold_data, labels = \"both\")\n\n\n\n\nUm gráfico sazonal torna o padrão sazonal subjacente mais visível e é especialmente útil para identificar anos em que esse padrão muda. Neste caso, não há um padrão claro de comportamento, já que diferentes anos apresentam comportamentos distintos. No entanto, parece que, em sua maioria, os valores tendem a cescer durante o início do ano e diminuir na parte final. Além disso, o gráfico destaca o rápido declínio dos valores durante o ano de 2013, o crescimento nos anos de 2019 e 2020, e uma relativa estabilidade nos demais anos.\nUma representação alternativa que enfatiza os padrões sazonais é aquela em que os dados de cada estação são agrupados em mini gráficos temporais separados.\n\nfeasts::gg_subseries(gold_data)\n\n\n\n\nPor meio deste gráfico, é possível visualizar um padrão sazonal potencial, com um pico observado em agosto, embora esse padrão não seja muito evidente.\nAté o momento, observou-se que os dados apresentam um comportamento complexo e não revelam padrões óbvios, o que torna a construção de modelos preditivos desafiadora e limita sua eficácia, especialmente em projeções de longo prazo.\nA seguir, vou aprofundar a análise na tentativa de compreender as estruturas subjacentes aos dados, buscando informações que possam aprimorar a capacidade preditiva dos modelos futuros."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#decompondo-a-série-temporal",
    "href": "posts/R/serie_ouro/EDA/index.html#decompondo-a-série-temporal",
    "title": "Cotação do ouro - Parte 1",
    "section": "Decompondo a Série Temporal",
    "text": "Decompondo a Série Temporal\n\nA decomposição de séries temporais é uma técnica fundamental na análise de dados temporais, permitindo separar uma série temporal em seus componentes principais para melhor compreensão e modelagem. Existem várias abordagens para realizar essa decomposição, sendo as mais comuns as seguintes:\n\nDecomposição Aditiva: Nessa abordagem, a série temporal é dividida em três componentes: tendência, sazonalidade e ruído. A tendência representa a evolução de longo prazo da série, a sazonalidade corresponde a padrões repetitivos em intervalos fixos de tempo e o ruído representa as flutuações aleatórias. A decomposição aditiva é útil quando as variações em cada componente são independentes do nível da série.\nDecomposição Multiplicativa: Na decomposição multiplicativa, a série temporal é dividida em tendência, sazonalidade e ruído, mas em termos multiplicativos. Isso significa que os componentes são representados como proporções do nível da série. Essa abordagem é apropriada quando as variações são proporcionais ao nível da série, como em casos onde o crescimento é exponencial.\nDecomposição STL (Seasonal and Trend decomposition using LOESS): A decomposição STL é uma técnica avançada que utiliza alisamento local (LOESS) para decompor a série em tendência, sazonalidade e resíduos. Ela é especialmente útil para séries com sazonalidade não constante ao longo do tempo ou com tendências não lineares.\nDecomposição por Componentes Independentes (ICA): A ICA é uma técnica avançada que busca separar uma série temporal em componentes independentes, cada um representando uma fonte de variação única. Isso pode ser útil quando se deseja identificar e isolar padrões específicos em dados complexos.\n\nA escolha da abordagem de decomposição depende da natureza dos dados e dos objetivos da análise. Devido às características complexas apresentadas pelos dados, a técnica de decomposição escolhida foi o STL, comumente utilizada para identificar tendências de longo prazo e padrões sazonais, inclusive permitindo múltiplos períodos sazonais.\n\n\nModelo STL (Seasonal-Trend Decomposition Using LOESS)\n\nAntes de realizar a decomposição dos dados vou apresentar em carater didático o funcionamento do modelo STL.\nO modelo STL desagrega uma série temporal em três componentes principais: tendência (Trend), sazonalidade (Seasonal), e resíduos (Remainder). Ele faz isso separadamente para cada componente, usando suavização LOESS (um método de suavização não paramétrica) para estimar cada parte. Sendo especialmente útil quando a sazonalidade em seus dados varia com o tempo. Isso o torna útil para séries temporais com sazonalidades irregulares ou não lineares. O STL é principalmente usado para entender a estrutura de uma série temporal e não para fazer previsões diretamente. Entretanto existem diferentes técnicas de modelagem voltadas para previsão que utilizam como parte do seu processo a decomposição dos dados através de modelos STL.\nO modelo STL pode ser descrito da seguinte forma:\n\nO modelo STL desagrega uma série temporal \\(y_t\\) em três componentes principais: tendência (\\(T_t\\)), sazonalidade (\\(S_t\\)) e resíduos (\\(R_t\\)) usando o método LOESS.\nEquação de Decomposição:\n\\[y_t = T_t + S_t + R_t\\]\nEquação da Tendência:\n\\[T_t = \\mu_t + \\mu_{t-1} + \\ldots + \\mu_{t-d+1}\\]\nEquação da Sazonalidade:\n\\[S_t = \\gamma_{t-m} + \\gamma_{t-2m} + \\ldots + \\gamma_{t-km}\\]\nEquação dos Resíduos:\n\\[R_t = y_t - T_t - S_t\\]\nPeso LOESS (w):\n\\[w_i = \\begin{cases}\n  (1 - \\left(\\frac{h}{|t - x_i|}\\right)^3)^3 & \\text{se } |t - x_i| \\leq h \\\\\n  0 & \\text{se } |t - x_i| &gt; h\n\\end{cases}\\]\nonde:\n\n\\(\\mu_t\\) refere-se à estimativa da tendência na época \\(t\\).\n\\(d\\) é o parâmetro que determina a largura da janela de suavização usada para estimar a tendência.\n\\(\\gamma_{t-km}\\) refere-se à estimativa da sazonalidade na época \\(t\\) com um atraso de \\(m\\), onde \\(m\\) é a periodicidade sazonal.\n\\(w_i\\) é o peso calculado usando a função de tri-cubo. Esta função atribui pesos próximos a 1 para pontos de dados que estão próximos ao ponto de interesse \\(t\\) (dentro da janela de suavização \\(h\\)) e atribui pesos decrescentes rapidamente para pontos de dados mais distantes.\n\nApós a compreensão do funcionamento, vou processeguir com o ajuste do modelo aos dados e, em seguida, com a criação dos gráficos para visualização dos resultados.\n\n#Ajustando o modelo STL aos dados\nfit &lt;- \n  gold_data %&gt;%\n  fabletools::model(\n    feasts::STL(value)\n  ) %&gt;% \n  fabletools::components() \n\n\n#Gerando os gráficos do modelo STL\nfit %&gt;% fabletools::autoplot()\n\n\n\n\nOs resultados indicam a presença de componentes relacionados a tendência, sazonalidade e ruído. O segundo gráfico (trend) evidencia as alterações de tendência descritas no início da análise exploratória. Já o terceiro gráfico (season_year) apresenta uma aparente mudança no comportamento sazonal, que gradualmente modifica seu padrão. O último gráfico (remainder), que teoricamente deveria representar ruído branco, aparenta conter estruturas potencialmente relevantes para a compreensão dos dados, principalmente em sua parte final.\nO próximo passo nesta Análise Exploratória consistirá na análise espectral, cujo propósito é aprofundar a compreensão do comportamento das potenciais componentes periódicas identificadas nos dados. Contudo, antes de prosseguir com as análises mencionadas, quero verificar se a componente remainder pode ser realmente considerada como ruído branco. Para esta finalidade, farei uso do teste de Ljung-Box, que avalia a existência de correlações significativas nos dados.\n\nTeste Ljung-Box\nO teste de Ljung-Box é usado para verificar a presença de autocorrelação nos resíduos de um modelo de séries temporais. Matematicamente, o teste pode ser representado da seguinte forma:\nA estatística de teste Q para o teste de Ljung-Box é definida como:\n\\[Q = n(n+2) \\sum_{k=1}^{h} \\frac{\\hat{\\rho}_k^2}{n-k}\\]\nOnde:\n\n\\(Q\\) é a estatística de teste.\n\\(n\\) é o número de observações na série temporal.\n\\(h\\) é o número máximo de atrasos considerados.\n\\(k\\) é uma variável que varia de 1 até \\(h\\).\n\\(\\hat{\\rho}_k^2\\) representa as estimativas de autocorrelação nos atrasos \\(h\\) nos resíduos.\nA fórmula calcula a estatística Q como a soma ponderada dos quadrados das autocorrelações estimadas nos atrasos \\(k\\), onde os pesos são proporcionais a \\((n-k)^1\\).\n\nO valor de \\(Q\\) segue uma distribuição qui-quadrado com \\(h\\) graus de liberdade sob a hipótese nula de que não há autocorrelação nos resíduos.\nEste teste é amplamente utilizado na análise de séries temporais para garantir que os resíduos de um modelo se comportem como um ruído branco, o que é uma suposição importante para muitos métodos estatísticos e de previsão.\n\nfit %&gt;% \n  fabletools::features(remainder, ljung_box, lag = 48) %&gt;% \n  set_tab(cap_tab = \"Teste ljung-box\")\n\n\n\nTeste ljung-box\n\n\n.model\nlb_stat\nlb_pvalue\n\n\n\n\nfeasts::STL(value)\n248.7254\n0\n\n\n\n\n\n\n\n\nO resultado do teste rejeita a hipótese nula de que não há autocorrelação nos resíduos. Assim, não podemos considerar que a componente não possui informações relevantes.\nDados os resultados obtidos até o momento, vou realizar análises espectrais nos dados originais, bem como nas componentes sazonais e de ruído identificadas pelo modelo STL. Embora esse processo possa parecer trabalhoso e, à primeira vista, redundante, essa avaliação detalhada pode fornecer informações valiosas sobre os dados."
  },
  {
    "objectID": "posts/R/serie_ouro/EDA/index.html#análises-espectrais",
    "href": "posts/R/serie_ouro/EDA/index.html#análises-espectrais",
    "title": "Cotação do ouro - Parte 1",
    "section": "Análises Espectrais",
    "text": "Análises Espectrais\n\nA análise espectral é uma técnica fundamental em processamento de sinais (série temporal) que envolve decompor os dados em suas componentes de frequência. Isso é alcançado por meio da transformada de Fourier ou outras técnicas similares, permitindo que o sinal seja representado no domínio da frequência e não somente no domínio do tempo. A análise espectral é valiosa para identificar padrões periódicos, identificar frequências dominantes, detectar sazonalidades e compreender a estrutura subjacente de um sinal. Em muitos casos, a análise espectral pode revelar informações ocultas nos dados que não são facilmente perceptíveis na forma temporal original.\nOs métodos de análise espectral utilizadas serão a análise de Fourier e de wavelets.\n\n\nAnálise de Fourier\n\nA análise de Fourier é uma técnica matemática poderosa usada para decompor sinais complexos em suas componentes de frequência, revelando os diversos padrões de oscilação que os compõem. Essa abordagem é fundamentada na transformada de Fourier, cujos resultados são geralmente representados pelo espectro de potência, que descreve a distribuição de energia em diferentes frequências. O resultado é um gráfico de amplitude em função da frequência, de interpretação direta e intuitiva.\nA transformada de Fourier contínua de uma função(t) é dada por:\n\\[X(f) = \\int_{-\\infty}^{\\infty} x(t) \\cdot e^{-j2\\pi ft} \\,dt\\]\nOnde:\n\n\\(X(f)\\) é a Transformada de Fourier do sinal \\(x(t)\\) no domínio da frequência \\(f\\).\n\\(x(t)\\) é o sinal no domínio do tempo.\n\\(j\\) é a unidade imaginária.\n\nCalculada a tranformada de Fourier \\(X(f)\\) do sinal \\(x(t)\\), o espectro de potência \\(P(f)\\) é dado como:\n\\[P(f) = |X(f)|^2\\]\n\n\n\nAnálise Wavelets\n\nConforme mencionado anteriormente um dos métodos escolhido para análise espctral foi a análise via wavelets. Baseada em funções wavelets, que são pequenas ondas (ou “wavelets”), essa técnica tem como principal caracteristica uma duração finita no tempo, garantindo sua localizadas no domínio do tempo e da frequência. Uma vantagem em relação a técnica tradicional da transformadas de Fourier, que representam sinais como uma combinação de senos e cossenos com duração infinita no tempo, indicando apenas a presença das componentes periódicas e não o seu comportamento ao decorrer do tempo de registro.\nSemelhante à técnica anterior, a análise via wavelets é baseada na transformada wavelet, cujos resultados são normalmente representados por meio de um “escalograma de wavelets”. Essa técnica é frequentemente empregada na análise wavelet para representar como a energia do sinal varia em diferentes escalas ao longo do tempo. O escalograma é uma representação bidimensional, em que o eixo horizontal denota o tempo, o eixo vertical denota a frequência e a intensidade do sinal é codificada por cores ou tons de cinza.\nA transformada de wavelet contínua pode ser definida como o produto interno entre o sinal original e uma função wavelet mãe que é transladada e escalonada. A fórmula geral para isso é a seguinte:\n\\[W(a, b) = \\int_{-\\infty}^{\\infty} x(t) \\cdot \\psi_{a,b}(t) \\, dt\\]\nonde:\n\n\\(W(a, b)\\) é o coeficiente da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\\(x(t)\\) é o sinal de entrada.\n\\(\\psi_{a,b}(t)\\) é a wavelet mãe escalonada e transladada de acordo com os parâmetros \\(a\\) e \\(b\\). Cada coeficiente do escalograma é calculado como o módulo ao quadrado dos coeficientes da Transformada Wavelet:\n\n\\[S(a, b) = |W(a, b)|^2\\]\nonde:\n\n\\(S(a, b)\\) é o escalograma na escala \\(a\\) e posição \\(b\\).\n\\(W(a, b)\\) são os coeficientes da transformada de wavelet para os parâmetros de escala \\(a\\) e deslocamento \\(b\\).\n\n\n\n\n\nRemoção da Tendência e Normalização dos Dados\n\nAntes de avançarmos com a análise espectral, é imprescindível realizar duas etapas cruciais: a remoção da tendência, caso esta esteja presente, e a normalização dos dados. Para atingir esses objetivos, utilizaremos a componente ‘trend’, gerada pelo modelo STL, a qual será subtraída dos dados originais. Posteriormente, empregaremos a função ‘scale’ para centralizar e normalizar os dados, assegurando que estejam preparados de forma adequada para a análise subsequente.\nLembrando que esses procedimentos serão aplicados apenas para os dados originais. As componentes ‘season’ e ‘remainder’, provenientes do modelos STL, já estão livres de tendência e centrados.\n\ndetrend_ts &lt;- \n  fit %&gt;%\n  mutate(\n    value = value - trend,\n    value = scale(value)\n    ) %&gt;% \n  select(index,value) \n\ndetrend_ts %&gt;%\n  slice_head(n=5)%&gt;%\n  set_tab(cap_tab = \"Dados sem tendência e normalizados\")\n\n\n\nDados sem tendência e normalizados\n\n\nindex\nvalue\n\n\n\n\n2013 jan\n1.93360712\n\n\n2013 fev\n1.67668880\n\n\n2013 mar\n1.53406325\n\n\n2013 abr\n0.05350123\n\n\n2013 mai\n-0.72788956\n\n\n\n\n\n\n\n\nPronto, os dados estão prontos para as análises espectrais!\n\n\n\nResultados Análises Espectrais\n\nPara realizar as análises descritas será utilizado o pacote WaveletComp. Os códigos usados são os mesmos para todos os dados e serão apresentados apenas para os dados originais.\nPara obtenção do espectro de potência será utilizada a função wt.avg, que retorna as médias da potência de wavelet ao longo do tempo. Equivalente ao espectro de potência obtido através da Transformada de Fourier.\nOs resultados serão apresentados na seguinte ordem: dados originais, componente ‘season’, e componente ‘remainder’ do modelo STL.\n\n\nDados originais\n\nO código abaixo realiza o cálculo da transformada wavelet e salva seus resultados em um objéto, nomeado w.ouro.dt, que será utilizado na criação dos gráficos desejádos.\n\n\n# Calculando a transformada wavelet dos dados sem o trend\nw.ouro.dt &lt;- \n  WaveletComp::analyze.wavelet(\n    detrend_ts,\n    \"value\",\n    loess.span = 0,\n    verbose=FALSE,\n    date.format = \"%Y-%m-%d\"\n    )\n\n\nUma vez calculada a transformada wavelet, podemos criar os gráficos do espectro de potência e escalograma.\nPara a criação do gráfico do espectro de potência, o pacote WaveletComp possui a função wt.avg.\n\n\n#Plot do gráfico de espectro de potência ad\nWaveletComp::wt.avg(w.ouro.dt, show.legend = FALSE)\n\n\n\n\n\nO espectro de potência dos dados originais revela dois picos estatisticamente significativos, ocorrendo aproximadamente em 12 meses e 16 meses, enquanto um terceiro pico em torno de 6 meses não alcança significância estatística. No entanto, os resultados apresentam uma resolução limitada.\nVamos analisar o comportamento dessas componentes ao longo do tempo. Para criar o escalograma, o pacote ‘WaveletComp’ disponibiliza a função wt.image.\n\n\n# monthly.ticks &lt;- seq(as_date(\"2013-01-01\", format = \"%Y-%m-%d\"), \n#                      as_date(\"2023-09-22\", format = \"%Y-%m-%d\"), \n#                      by = \"month\")\n# monthly.labels &lt;- strftime(monthly.ticks, format = \"%b %Y\")\n\n#Plot do escalograma de wavelet\nWaveletComp::wt.image(\n  w.ouro.dt, color.key = \"interval\",\n  legend.params = list(lab = \"wavelet power levels\"),\n  periodlab = \"periodo (meses)\", \n   show.date = FALSE, date.format = \"%Y-%m-%d\"#, \n   # spec.time.axis = list(at = monthly.ticks, labels = monthly.labels, \n   #                       las = 2)\n  )\n\n\n\n\n\nO resultado do escalograma apresenta aproximadamente as mesmas componentes espectrais presentes do espectro de potência. A componente de aproximadamente 6 meses aparece fora da faixa de confiabilidade estatística, de modo descontínuo e com baixos valores de escala. As componenetes de aproximadamente 11 meses e 16 meses aparecem dentro da margem de 95% de confiabilidade estatística, contudo os resultados não possuem uma boa resolução. Estas componentes exibem variação em suas faixas de periodicidades e variações nos seus valores de escala. Apresentando seus valores mais expressivos (&gt;0,5) apenas na parte final dos dados.\n\n\n\nComponente ‘season’\n\n\n\n\n\n\nEm contraste aos resultados anteriores, as componentes agora estão definidas com maior clareza e estão concentradas nos períodos de 6 meses e 12 meses. Notavelmente, o modelo STL não conseguiu capturar a componente que ocorre aproximadamente a cada 16 meses, presente nos dados originais.\n\n\n\n\n\n\n\nDevido à forma como a componente ‘season’ foi construída, ela não contém o ruído presente nos dados originais, tornando as componentes espectrais muito mais evidentes. A componente anual mantém um comportamento uniforme ao longo de todo o período de registro. Por outro lado, a componente de 6 meses exibe valores elevados até a metade do período de registro, após o qual desaparece do sinal.\n\n\n\nComponente ‘remainder’\n\n\n\n\n\n\nEmbora seja considerada como resíduo pelo modelo STL, os resultados do espectro de potência são notavelmente semelhantes aos dos dados originais. Eles mostram componentes espectrais em torno de 12 meses e 16 meses, dentro da margem de confiabilidade estatística, enquanto uma componente de aproximadamente 6 meses fica fora dessa margem. Ao contrário dos resultados dos dados originais, as componentes apresentam uma definição mais nítida entre os picos, e a componente de aproximadamente 16 meses exibe uma maior concentração de energia em comparação com as outras componentes.\n\n\n\n\n\n\n\nO escalograma da componente ‘remainder’ é surpreendentemente semelhante ao escalograma dos dados originais. Isso é inesperado, uma vez que teoricamente o modelo STL deveria ter removido não apenas a tendência, mas também a componente sazonal."
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#suavização-exponencial-simples",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#suavização-exponencial-simples",
    "title": "Cotação do ouro - Parte 3",
    "section": "Suavização Exponencial Simples",
    "text": "Suavização Exponencial Simples\nPodendo ser entendida como um “meio termo” entre o método de Naïve, onde só a medida mais próxima teria inportância \\((\\hat{y}_{T+h|T} = y_T)\\), e uma média simples, onde todas as medidas tem a mesma importância \\((\\hat{y}_{T+h|T} = \\frac{1}{T} \\sum_{t=1}^{T} y_t)\\). A suavização exponencial simples tem os seus valores calculados usando médias ponderadas, onde os pesos diminuem exponencialmente à medida que as observações “envelhecem”. Assim o valor suavizado da série é dado por:\n\\[y_{T+1|T} = \\alpha y_T + \\alpha (1 - \\alpha) y_{T-1} + \\alpha (1 - \\alpha)^2 y_{T-2} + \\ldots,\\]\nUm modo de representar este método é através de suas componentes. Para a suavização exponencial simples, o único componente incluído é o nível, \\(\\ell_t\\).\nEquação de Previsão\n\\[\\hat{y}_{t+h|t} = \\ell_t,\\]\nEquação de Suavização\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha) \\ell_{t-1},\\]\nonde \\(0 \\leq \\alpha \\leq 1\\) é o parâmetro de suavização. A previsão de um passo à frente para o tempo T+1 é uma média ponderada de todas as observações na série \\(y_1, \\ldots, y_T\\). A taxa pela qual os pesos diminuem é controlada pelo parâmetro \\(\\alpha\\). E \\(\\ell_t\\) representa o nível (ou o valor suavizado) da série no tempo \\(t\\)."
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-tendência",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-tendência",
    "title": "Cotação do ouro - Parte 3",
    "section": "Suavização Exponencial com Tendência",
    "text": "Suavização Exponencial com Tendência\nO próximo passo envolve a inclusão dos casos em que há presença de tendência nos dados. Nesse cenário, as equações são atualizadas para:\nEquação de previsão\n\\[y_{t+h|t} = \\ell_t + h b_t\\]\nEquação de suavização (nível)\n\\[ℓ_t = αy_t + (1 - α)(ℓ_{t-1} + b_{t-1})\\]\nEquação da tendência\n\\[b_t = \\beta^* (\\ell_t - \\ell_{t-1}) + (1 - \\beta^*) b_{t-1}\\]\nOnde \\(b_t\\) denota uma estimativa da tendência (inclinação) da série no tempo \\(t\\), e \\(\\beta^*\\) é o parâmetro de suavização para a tendência, \\((0 \\leq \\beta^* \\leq 1)\\).\nAssim como na suavização exponencial simples, a equação de \\(\\ell_t\\) aqui é uma média ponderada da observação \\(y_t\\) e da previsão de treinamento de um passo à frente para o tempo \\(t\\), dada por \\(\\ell_{t-1} + b_{t-1}\\). A equação de \\(b_t\\) é uma média ponderada da tendência estimada no tempo \\(t\\) com base em \\(\\ell_t - \\ell_{t-1}\\) e \\(b_{t-1}\\) é a estimativa anterior da tendência. Assim, a função de previsão não é mais plana, mas apresenta tendência. A \\(h-\\texttt{previsão}\\) à frente é igual ao último nível estimado mais \\(h\\) vezes o último valor estimado da tendência. Portanto, as previsões são uma função linear de \\(h\\)."
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-sazonalidade",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#suavização-exponencial-com-sazonalidade",
    "title": "Cotação do ouro - Parte 3",
    "section": "Suavização Exponencial com Sazonalidade",
    "text": "Suavização Exponencial com Sazonalidade\nA versão mais completa desta família de modelos engloba uma equação para capturar a componente sazonal. Existem duas variações deste método, aditivo e multiplicativo, que diferem na natureza do componente sazonal. O método aditivo é preferível quando as variações sazonais são aproximadamente constantes ao longo da série, enquanto o método multiplicativo é preferível quando as variações sazonais estão mudando proporcionalmente ao nível da série.\nA versão aditiva é descrita da seguinte forma:\n\\[\\begin{align*}\n\\hat{y}_{t+h|t} &= \\ell_t + h b_t + s_{t-m(k+1)} \\\\\n\\ell_t &= \\alpha (y_t - s_t - m) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\\nb_t &= \\beta^* (\\ell_t - \\ell_{t-1}) + (1 - \\beta^*) b_{t-1} \\\\\ns_t &= \\gamma (y_t - \\ell_{t-1} - b_{t-1}) + (1 - \\gamma) s_{t-m} \\\\\n\\end{align*}\\]\nOnde \\(k\\) é a parte inteira de \\(\\frac{{h - 1}}{m}\\), o que garante que as estimativas dos índices sazonais usados para previsões provenham do último ano da amostra. A equação de nível mostra uma média ponderada entre a observação ajustada sazonalmente \\(y_t - s_t - m\\) e a previsão não sazonal \\(\\ell_{t-1} + b_{t-1}\\) para o tempo \\(t\\). A equação de tendência é idêntica ao método linear de Holt (tópico anterior). A equação sazonal mostra uma média ponderada entre o índice sazonal atual \\(y_t - \\ell_{t-1} - b_{t-1}\\) e o índice sazonal do mesmo período do ano anterior (ou seja, \\(m\\) períodos atrás).\nA equação para o componente sazonal é frequentemente expressa como:\n\\[\ns_t = \\gamma^* (y_t - \\ell_t) + (1 - \\gamma^*) s_{t-m}\n\\]\nSe substituirmos \\(\\ell_t\\) a partir da equação de suavização para o nível, apresentada acima, obtemos:\n\\[\ns_t = \\gamma^* (1 - \\alpha) (y_t - \\ell_{t-1} - b_{t-1}) + [1 - \\gamma^* (1 - \\alpha)] s_{t-m}\n\\]\no que é idêntico à equação de suavização para o componente sazonal com \\((\\gamma = \\gamma^* (1 - \\alpha))\\). Assim, a restrição usual para o parâmetro é \\((0 \\leq \\gamma^* \\leq 1)\\), o que se traduz em \\((0 \\leq \\gamma \\leq 1 - \\alpha)\\).\nLembrando que esta é apenas uma simplificação da aplicação do método aditivo de Holt-Winters. Para uma discussão mais detalhada você pode consultar o capítulo 8 do livro “Forecasting: Principles and Practice” e as referências ali mencionadas."
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#visualizando-o-plano-de-validação-cruzada",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#visualizando-o-plano-de-validação-cruzada",
    "title": "Cotação do ouro - Parte 3",
    "section": "Visualizando o Plano de Validação Cruzada",
    "text": "Visualizando o Plano de Validação Cruzada\n\ncv %&gt;% \n  tk_time_series_cv_plan() %&gt;% \n  plot_time_series_cv_plan(mes,media, .facet_ncol = 2, .interactive = FALSE)"
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#calibrando",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#calibrando",
    "title": "Cotação do ouro - Parte 3",
    "section": "Calibrando",
    "text": "Calibrando\n\nO processo de calibração estabelece as bases para atingir precisão e definir os intervalos de confiança das previsões. Isso é feito através do cálculo das previsões e da análise dos resíduos com base nos dados de teste.\n\ncalib_table &lt;- \n  model_table %&gt;% \n  modeltime_calibrate(testing(cv$splits[[1]]))"
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#conjunto-de-teste-e-acurácia",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#conjunto-de-teste-e-acurácia",
    "title": "Cotação do ouro - Parte 3",
    "section": "Conjunto de Teste e Acurácia",
    "text": "Conjunto de Teste e Acurácia\n\nUtilizando os dados que foram calibrados, o próximo passo é realizar previsões com os modelos e compará-las ao conjunto de teste. Isso será realizado utilizando as funções modeltime_forecast e plot_modeltime_forecast, idealizadas para simplificar o processo de previsão e criação de gráficos dos dados originais e dos resultados dos modelos.\n\ncalib_table %&gt;% \nmodeltime::modeltime_forecast(\n    new_data = testing(cv$splits[[1]]),\n    actual_data = gold_mean\n  ) %&gt;% \n  plot_modeltime_forecast(.interactive = TRUE)\n\n\n\n\n\nAs métricas de cada modelo são obtidas através da função modeltime_accuracy, que simplifica o cálculos das métricas de precisão.\n\ncalib_table %&gt;% \n  modeltime::modeltime_accuracy() %&gt;% \n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n  \n    \n      Accuracy Table\n    \n    \n    \n      .model_id\n      .model_desc\n      .type\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nETS(M,N,N)\nTest\n21.60\n12.09\n5.36\n12.94\n22.64\nNA\n    2\nSEASONAL DECOMP: ETS(M,N,N)\nTest\n14.91\n8.38\n3.70\n8.77\n15.53\n0.60\n    3\nETS(M,A,A)\nTest\n13.09\n7.35\n3.25\n7.66\n13.84\n0.56\n    4\nETS(M,A,A)\nTest\n7.52\n4.18\n1.87\n4.30\n8.64\n0.61\n    5\nETS(M,AD,A)\nTest\n11.21\n6.25\n2.78\n6.50\n12.27\n0.51\n    6\nETS(M,A,A)\nTest\n7.26\n4.06\n1.80\n4.18\n8.50\n0.57\n  \n  \n  \n\n\n\n\nO modelo ajustado automaticamente (\\(\\texttt{ETS()}\\)) claramente não capturou o comportamento dos dados, retornando como previsão o último valor do conjunto de treino (naïve forecast). Por outro lado, os demais modelos, apesar de apresentarem diferentes níveis de suavidade, parecem ter desempenhado bem na reprodução dos dados de teste. Os modelos: com múltiplas sazonalidades, \\(\\texttt{ETS(M,A,A)}\\) com parâmetros \\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\) ajustados automaticamente e com o parâmetro \\(\\gamma\\) igual a 0.5 (repectivamente modelos 2, 3 e 5), mostram formatos e desempenhos semelhantes. Enquanto os modelos \\(\\texttt{ETS(M,A,A)}\\) com parâmetro \\(\\gamma\\) igual a 0.1 e 0.7 (modelos 4 e 6) apresentam os melhores desempenhos, apesar de serem as curvas com maior e menor nível de suavidade."
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#validação-cruzada",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#validação-cruzada",
    "title": "Cotação do ouro - Parte 3",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nPara uma melhor compreenção do desempenho e estabilidade dos modelos serão utilizados os folders criados no início deste post, onde os modelos serão novamente ajustados e testados em diferentes janelas temporais de ‘treino’ e previsão.\nPara realizar esta etatapa sera utilizada a função modeltime_fit_resamples, idealizada para ajustar os modelos e criar previsões iterativamente a partir das especificações contidas em um objeto modeltime_table e de conjuntos de reamostragem (CV-folders).\n\nresamples_fitted &lt;- model_table %&gt;%\n    modeltime_fit_resamples(\n        resamples = cv,\n        control   = control_resamples(verbose = FALSE)\n    )\n\n#resamples_fitted\n\nPara avaliar os modelos será utilizada a função plot_modeltime_resamples. Essa função plota as metricas de cada um dos modelos em relação a cada um dos conjuntos de reamostragem. A opção iterativa do gráfico é uma maneira conveniente de avaliar o desempenho dos modelos de forma mais detalhada, permitindo uma avaliação individual e comparativa.\n\nresamples_fitted %&gt;%\n    plot_modeltime_resamples(\n      .point_size  = 3, \n      .point_alpha = 0.8,\n      .interactive = TRUE\n    )\n\n\n\n\n\nO modelo com o melhor desempenho médio foi \\(\\texttt{ETS(A, M, M)}\\) com os parâmetros \\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\) ajustados automaticamente (modelo 3). Contudo, os resultados médios dos modelos não diferem de modo substâncial.\nOs resultados dos modelos nos diferentes folders mostram cenários diversos. No folder 1, existe uma diferença considerável no desempenho dos modelos. Nos folders 2, 3 e 4, os resultados são bem próximos. Nos folders 5 e 6, a diferença entre os modelos volta a ser bem diversa. Já nos folders 7 e 8, os modelos têm um desempenho próximo, mas inferior aos demais folders.\nEm linhas gerais, os modelos tiveram um comportamento semelhante em relação aos diferentes folders, apresentando um desempenho melhor nos quatro primeiros conjuntos em relação aos demais. É possível observar que o desempenho dos modelos começa a diminuir quando os conjuntos de treinamento possuem uma menor quantidade de dados, dentro do período de tempo dominado pelas componentes sazonais, e diminui ainda mais quando os conjuntos de teste estão na parte dos dados onde a presença de tendência é mais significativa. Essa piora gradual nos conjuntos subsequentes pode indicar que os modelos têm uma boa capacidade de generalização para a parte dos dados dominada pelas componentes periódicas, e que o comportamento dos dados vem sofrendo mudanças significativas ao longo dos anos."
  },
  {
    "objectID": "posts/R/serie_ouro/exp_esmooth/index.html#análise-dos-resíduos",
    "href": "posts/R/serie_ouro/exp_esmooth/index.html#análise-dos-resíduos",
    "title": "Cotação do ouro - Parte 3",
    "section": "Análise dos Resíduos",
    "text": "Análise dos Resíduos\n\nA análise de resíduos é uma etapa importante na avaliação de modelos. Ela ajuda a determinar se o modelo está capturando adequadamente as estruturas dos dados. Os resíduos que serão avaliados são em relação aos resultados dos modelos ajustados nos dados contidos no primeiro folder.\nA função plot_modeltime_residuals oferece um modo prático de avaliar os resíduos dos modelos que foram criados.\n\ncalib_table %&gt;% \n  modeltime_residuals() %&gt;% \n  plot_modeltime_residuals(\n     .type = \"timeplot\",\n     .interactive = FALSE\n     )\n\n\n\n\n\ncalib_table %&gt;% \n  modeltime_residuals() %&gt;% \n  plot_modeltime_residuals(\n     .type = \"acf\",\n     .interactive = FALSE\n     )\n\n\n\n\nEspera-se que os resíduos de um modelo exibam um comportamento de ruído branco, com média zero e variância constante. Qualquer padrão observado nos resíduos pode indicar que o modelo não está capturando de forma adequada a estrutura subjacente da série temporal. É possível notar que os resíduos dos modelos criados não correspondem exatamente ao comportamento esperado. O gráfico de resíduos não exibe um padrão completamente aleatório, seguindo um padrão próximo aos dados de teste, e os gráficos de ACF e PACF mostram uma autocorrelação residual significativa para o primeiro lag. Apesar disso, é possível considerar que os resíduos estão próximos do comportamento esperado, sendo a maioria dos modelos capaz de capturar as principais estruturas dos dados de forma satisfatória."
  },
  {
    "objectID": "posts/Python/price_elasticity/index.html",
    "href": "posts/Python/price_elasticity/index.html",
    "title": "Modelagem Elasticidade Preço-Demanda",
    "section": "",
    "text": "Em um mercado dinâmico, compreender a relação entre o preço de um produto e sua demanda é essencial para estratégias eficazes de precificação. Neste artigo, embarcaremos em uma exploração da aplicação de Modelos Aditivos Generalizados (GAMs) para modelar a variação da demanda em relação ao preço.\nDado meu conhecimento inicial limitado sobre o tema, optarei por começar minha incursão por meio de um exemplo simples. Utilizarei o conceito de quantil de expectativa para criar modelos que representem diferentes cenários da relação entre demanda e preço. Essa abordagem inicial permitirá não apenas uma compreensão mais acessível do método, mas também a construção de modelos que capturam tanto o comportamento médio quanto cenários extremos."
  },
  {
    "objectID": "posts/Python/price_elasticity/index.html#expectile",
    "href": "posts/Python/price_elasticity/index.html#expectile",
    "title": "Modelagem de Cenários de Precificação com Modelos Aditivos Generalizados (GAMs)",
    "section": "Expectile",
    "text": "Expectile\n\n“Expectile” é uma abreviação de “expectation quantile” (quantil de expectativa). Em estatísticas e modelagem estatística, um expectil representa um valor condicional associado a um determinado quantil da distribuição de uma variável aleatória. Em outras palavras, é uma medida de tendência central que foca em um quantil específico da distribuição, proporcionando uma visão mais detalhada das características dessa distribuição em comparação com a média condicional. O uso de expectis é particularmente útil quando se deseja entender o comportamento de uma variável em diferentes percentis da sua distribuição."
  },
  {
    "objectID": "posts/Python/price_elasticity/index.html#preparando-os-dados",
    "href": "posts/Python/price_elasticity/index.html#preparando-os-dados",
    "title": "Modelagem Elasticidade Preço-Demanda",
    "section": "Preparando os dados",
    "text": "Preparando os dados\nRemovendo outliers.\n\ndf = df[df['Price'] &gt;= 5]\n\nCriando duas variáveis, x e y, a partir de um DataFrame df.\n\nx = df[['Price']]\ny = df['Quantity']\n\npara a criação de x esta sendo aplicadoo o uso de colchetes duplos ([[‘Price’]]) que cria um DataFrame em vez de uma Série. Portanto, x será um DataFrame com uma única coluna ‘Price’. Já y está sendo atribuído com a coluna ‘Quantity’ do DataFrame original df, resultando em uma Série."
  },
  {
    "objectID": "posts/Python/price_elasticity/index.html#quantil-gams",
    "href": "posts/Python/price_elasticity/index.html#quantil-gams",
    "title": "Modelagem Elasticidade Preço-Demanda",
    "section": "Quantil GAMs",
    "text": "Quantil GAMs\n\nNesse trecho de código, vamos criar uma lista chamada quantiles com os valores de [0.025, 0.5, 0.975], nossos quantis de interesse. Também está sendo criado um dicionário vazio chamado gam_results. Os dicionários em Python são estruturas de dados que mapeiam chaves a valores. Eles são úteis para armazenar e organizar dados.\n\nquantiles = [0.025, 0.5, 0.975]\ngam_results = {}\n\nNeste trecho de código, vamos iterar sobre os valores da lista quantiles e, para cada valor q na lista, está criando e ajustando um modelo GAM através da função ExpectileGAM, específica para estimar expectativas condicionais de quantis (percentis) de uma variável de resposta. Em outras palavras, ela é projetada para modelar a relação entre variáveis preditoras e a expectativa condicional de determinados quantis da variável resposta, em vez de estimar a média condicional.\nO s(0) indica uma spline univariada aplicada à variável.\n\nfor q in quantiles:\n  gam = ExpectileGAM(s(0),expectile=q)\n  gam.fit(x,y)\n  gam_results[q] = gam\n  \n#gam_results\n\nAo final desse loop, gam_results conterá modelos GAM ajustados para cada quantil especificado na lista quantiles."
  },
  {
    "objectID": "posts/Python/price_elasticity/index.html#quantil-de-expectativa-expectile",
    "href": "posts/Python/price_elasticity/index.html#quantil-de-expectativa-expectile",
    "title": "Modelagem Elasticidade Preço-Demanda",
    "section": "Quantil de Expectativa (Expectile)",
    "text": "Quantil de Expectativa (Expectile)\n\n“Expectile” é uma abreviação de “expectation quantile” (quantil de expectativa). Em estatísticas e modelagem estatística, um expectil representa um valor condicional associado a um determinado quantil da distribuição de uma variável aleatória. Em outras palavras, é uma medida de tendência central que foca em um quantil específico da distribuição, proporcionando uma visão mais detalhada das características dessa distribuição em comparação com a média condicional. O uso de expectis é particularmente útil quando se deseja entender o comportamento de uma variável em diferentes percentis da sua distribuição."
  }
]